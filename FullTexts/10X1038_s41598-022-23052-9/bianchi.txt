CLIP è una rete neurale multi-modale visione-linguaggio addestrata tramite CL per associare concetti visivi con il testo. Il modello comprende un codificatore di visione e uno di testo, ciascuno seguito da uno strato lineare per proiettare le rappresentazioni di immagini e testo nello stesso spazio latente. CLIP è addestrato aposizionare immagini e descrizioni corrispondenti (ad esempio, un'immagine di una maglietta rossa e la sua descrizione "una maglietta rossa") vicine nello spazio vettoriale (vedi Fig. [1] per un esempio). Quando addestrato su 400 milioni di coppie <immagine, testo> raccolte da internet,  CLIP ha dimostrato un trasferimento competitivo zero-shot o few-shot a compiti a valle come OCR e classificazione fine-grained degli oggetti.

CLIP è una multi-modal neural network visione-linguaggio addestrata tramite CL per associare concetti visivi con il testo. Il modello comprende un encoder per la visione e uno per il testo, ciascuno seguito da un linear layer per proiettare le rappresentazioni delle immagini e del testo nello stesso latent space. CLIP è addestrato per posizionare immagini e descrizioni corrispondenti (ad esempio, un'immagine di una maglietta rossa e la sua descrizione "una maglietta rossa") vicine nello vector space (vedi Fig. [1] per un esempio). Quando addestrato su 400 milioni di coppie <image, text> raccolte da internet, CLIP ha dimostrato di poter trasferire in modo competitivo zero-shot o few-shot ai downstream tasks come OCR e classificazione fine-grained degli oggetti.

