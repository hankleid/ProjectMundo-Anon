Apprendimento contrastivo di linguaggio e visione di concetti generali di moda

Abstract: La crescita costante dello shopping online va di pari passo con lo sviluppo di modelli di ML e NLP sempre più complessi. Mentre la maggior parte dei casi d'uso sono considerati come problemi di apprendimento supervisionato specializzato, sosteniamo che i professionisti trarrebbero grande beneficio da rappresentazioni generali e trasferibili dei prodotti. Inquesto lavoro, ci basiamo sugli sviluppi recenti nell'apprendimento contrastivo per addestrareFashionCLIP , un modello simile aCLIP adattato per l'industria della moda. Dimostriamo l'efficacia delle rappresentazioni apprese daFashionCLIP con test estensivi su una varietà di compiti, dataset e prove di generalizzazione. Sosteniamo che le adattazioni di grandi modelli pre-addestrati come CLIP offrono nuove prospettive in termini di scalabilità e sostenibilità per certi tipi di attori nell'industria. Infine, dettagliamo i costi e l'impatto ambientale dell'addestramento e rilasciamo i pesi del modello e il codice come contributo open source alla comunità.

La straordinaria crescita del commercio al dettaglio online—nel 2020, 4 trilioni di dollari all'anno—ha profondamente impattato l'industria della moda, con 1 transazione su 4 che ora avviene online. La combinazione di grandi quantità di dati e una varietà di casi d'uso ha reso l'e-commerce fertile per modelli di apprendimento automatico (ML) all'avanguardia, con il Natural Language Processing (NLP) coinvolto in raccomandazioni, recupero di informazioni (IR), classificazione dei prodotti e molti altri casi d'uso, , , ,.

Tuttavia, mentre la comunità inizia ad affrontare i grandi costi operativi di addestramento e sviluppo dei modelli, sta diventando chiaro che il valore delle innovazioni ML è stato catturato principalmente da pochi attori. Mentre il resto dell'industria del retail sta facendo sforzi concreti per adattarsi prontamente, le aziende che offrono prodotti ML come servizio hanno recentemente guadagnato terreno, creando un nuovo mercato da miliardi di dollari, , ,. La necessità di capacità ML che possano essere applicate a intere industrie e verticali aumenta la posta in gioco per una domanda antica nell'ML:possiamo costruire modelli che possano essere riutilizzati su diversi compiti e dataset ?

Mentre la generalizzazione è una virtù teorica, i modelli nel mondo reale spesso hanno successo adattandosi (eccessivamente) a un dataset e un compito specifico,. In pratica, la generalizzazione è stata considerata sia difficile da raggiungereche economicamente indesiderabile per casi d'uso su larga scala. In questo contesto, l'avvento di modelli auto-supervisionati su larga scala come ilContrastive Language-Image Pre-training (CLIP) è particolarmente interessante sia da un punto di vista teorico che pratico. Costruire su modelli pre-addestrati di grandi dimensioni per apprendere concettigenerali in verticali/industrie specifiche (ad esempio, Moda, Elettronica, Fai-da-te, ecc.) può fornire un modo nuovo e sostenibile per portare i benefici delle capacità ML a un insieme più ampio di professionisti, specialmente al di fuori delle grandi aziende tecnologiche. L'idea sarebbe di perfezionare modelli fondamentali generali per apprendere concetti specifici di un dominio (ad esempio, la moda), ma abbastanza generali da essere applicabili a tutti i casi d'uso all'interno di quel dominio.

Inquesto lavoro, mostriamo attraverso test estensivi e codice open-source che l'addestramento multi-modale può essere utilizzato con successo per apprendere concetti generali in un dominio specifico, ovvero la moda. Infatti, sosteniamo che non solo è tecnicamente possibile, ma anche economicamente vantaggioso e praticamente utile, poiché allontanarsi dall'impostazione tradizionale in cui i singoli modelli supervisionati sono addestrati specificamente per caso d'uso riduce i costi di annotazione e manutenzione fornendo soluzioni trasferibili tra compiti.

L'apprendimento contrastivo è recentemente diventato un approccio predominante per apprendere rappresentazioni significative di concetti in ML. Il framework di apprendimento si basa sull'idea che concetti semanticamente correlati (ad esempio, due immagini dello stesso oggetto da punti di vista diversi) dovrebbero avere rappresentazionisimili , mentre quelli non correlati dovrebbero esseredissimili. Inizialmente concepito per l'apprendimento della rappresentazione delle immagini auto-supervisionato, l'apprendimento contrastivo è stato recentemente applicato anche al linguaggio, ,. Lavori recenti hanno utilizzato l'addestramento contrastivo per collegare diverse modalità, ad esempio, visione e linguaggio, audio e linguaggio, o una combinazione delle tre, , ,. Questi modelli apprendono rappresentazioni di concetti da diverse modalità (ad esempio, un estratto testuale come "un cane che corre su un campo" e un'immagine che rappresenta la scena) e le ottimizzano per essere vicine in uno spazio latente condiviso. Fondamentalmente, la pipeline tipica è auto-supervisionata: poiché non è coinvolta alcuna annotazione manuale (ad esempio, nell'esempio precedente, si possono raccogliere coppie immagine-testo dal web), l'intervento umano è limitato a decidere quale compito di pre-addestramento debba essere utilizzato.

CLIP è una rete neurale multi-modale visione-linguaggio addestrata tramite CL per associare concetti visivi con il testo. Il modello comprende un codificatore di visione e uno di testo, ciascuno seguito da uno strato lineare per proiettare le rappresentazioni di immagini e testo nello stesso spazio latente.CLIP è addestrato aposizionare immagini e descrizioni corrispondenti (ad esempio, un'immagine di una maglietta rossa e la sua descrizione "una maglietta rossa") vicine nello spazio vettoriale (vedi Fig. [1] per un esempio). Quando addestrato su 400 milioni di coppie, testo> raccolte da internet,  CLIP ha dimostrato un trasferimento competitivo zero-shot o few-shot a compiti a valle come OCR e classificazione fine-grained degli oggetti.

Più formalmente,CLIP è un modello multi-modale che utilizza un codificatore di immagini ( Iθ I ) e un codificatore di testo ( Tθ T ). Entrambi i codificatori sono reti neurali profonde che mappano rappresentazioni grezze (cioè, un'immagine e un testo) a un vettore denso di 512 dimensioni (ad esempio, data un'immaginei ,Iθ I( i )∈ R512 ). Durante l'addestramento,N coppie di immagini e testi corrispondenti < i , t >sono selezionate (ad esempio, come in Fig. [1] , l'immagine di una maglietta rossa e la descrizione "una maglietta rossa"), codificate usando Iθ I e Tθ T , L 2-normalizzate, e confrontate a coppie.CLIP minimizza la perdita di entropia incrociata in modo cheI¯  θ I(i j)·T¯  θ T(t k) per j , k = 1 ,.. , Nè massimo quando la didascalia è abbinata all'immagine corretta ( j = k), e basso altrimenti ( j ≠ k), doveI¯  θ I( · ) /T¯  θ I( · ) sono gli output L 2-normalizzati dei codificatori di immagini e testo. Riassumiamo l'obiettivo di ottimizzazione perCLIP nelle Eq. ( [1] ) e ( [2] ). 1L ( θI , θT ) =- 1 2N∑j loge I ¯θ I(i j)·T ¯θ T(t j)∑k′ e I ¯θ I(i j)·T ¯θ T(tk′) + ∑k loge I ¯θ I(i k)·T ¯θ T(t k)∑j′ e I ¯θ I(ij′)·T ¯θ T(t k) 2 θ I∗, θ T∗=argmin θI , θT L ( θI , θT ) Qui, θ Ie θ Tsono i parametri apprendibili delle reti neurali codificatrici di immagini e testo, e l'operatore ( · )rappresenta il prodotto scalare. Il primo operando di addizione dell'Equazione [1] è l'entropia incrociata sull'asse delle immagini, mentre il secondo operando di addizione è sull'asse del testo.

Recentemente, i professionisti del settore hanno iniziato a riconoscere l'importanza e l'utilità del pre-addestramento contrastivo per il loro dominio di riferimento, con diversi lavori che presentano applicazioni a valle di successo a partire dal modelloCLIP. Nella moda, la natura multi-modale di CLIP è stata trovata utile nei modelli discriminativi recenti, che sono stati sviluppati sotto il paradigma standard di modelli supervisionati specifici per compito. Nel contesto generativo, CLIP spesso completa un quadro più ampio: ad esempio, CLIP è utilizzato per apprendere codebook linguisticamente fondati in Variational Auto Encoders o per guidare la sintesi e la manipolazione delle immagini nei modelli generativi di diffusione. Sebbene interessante per il grounding (vedi sotto Fig. [8] ), il caso d'uso target (generazione di immagini) e il focus più ristretto (singolo compito, singolo dataset) non sono facilmente comparabili aFashionCLIP , ma suggeriscono invece una possibile applicazione complementare nei casi d'uso generativi. Tuttavia, nessuna applicazione recente di CLIP è stata sviluppata per produrre rappresentazioni a livello industriale attraverso molteplici casi d'uso e dataset. In altre parole, CLIP è stato utilizzato solo come modello pre-addestrato, senza alcun tentativo di superare i problemi operativi e concettuali dei modelli supervisionati a singolo compito, ,.

Inquesto lavoro, introduciamoFashionCLIP , un modello basato su CLIP esplicitamente addestrato e testato per produrre rappresentazioni generali di prodotti per concetti di moda. AddestriamoFashionCLIP su un ampio dataset di moda nuovo e di alta qualità: come discusso nella sezione successiva, il nostro obiettivo è stabilire se tale fine-tuning sia sufficiente a produrre rappresentazioni di prodotti trasferibili in modalità zero-shot a dataset completamente nuovi.

I modelli supervisionati standard per applicazioni specifiche verticali come la moda sono costosi da addestrare e operare, fornendo una grande barriera all'ingresso per i fornitori SaaS e i giocatori più piccoli. Ad esempio, un modello di classificazione dei prodotti potrebbe essere addestrato su coppie < p r o d u c t d e s c r i p t i o n , c a t e g o r y >derivate dai dati del catalogo mentre si ottimizza per l'accuratezza della classificazione: se le etichette cambiano, o il modello viene distribuito su un catalogo diverso, l'accuratezza diminuirebbe. È importante notare che passare a architetture basate su CLIP, comeCMA-CLIP , non risolveipso facto il problema: se CLIP è utilizzato come modello per compito, solleverà gli stessi problemi di scalabilità dei metodi supervisionati tradizionali.

Dopo l'addestramento diFashionCLIP , ci siamo posti una domanda più ampia e potenzialmente più impattante: dato il dataset giusto e la procedura di fine-tuning, possiamo apprendere concetti multi-modali che siano abbastanza generali per l'intero dominio della moda? Procediamo con una combinazione di benchmark quantitativi – ispirati sia dalla letteratura esistente che da problemi noti per essere importanti nell'industria – e sonde qualitative per rispondere: poiché ottenere concetti generali è il nostro obiettivo, è importante verificare cheFashionCLIP non apprenda solo un dataset (ad esempio, una "collezione Armani"), ma concetti genuinamente trasferibili, come "gonna", "maniche", ecc. Ispirandoci a CLIP, i nostri due benchmark iniziali testeranno comeFashionCLIP passa dal testo all'immagine e viceversa (vedi Fig. [2] ):Dal testo all'immagineLa ricerca di prodotti è uno dei principali canali di interazione e ricavi tra un negozio e i suoi utenti, rappresentando in media dal 30% al 60% dei ricavi totali online ,. Storicamente, la ricerca di prodotti è stata eseguita principalmente con caratteristiche testuali, abbinando prima le query e le descrizioni dei prodotti in un indice e poi riordinando i risultati candidati ,,. Tuttavia, ci sono buone ragioni per credere che includere caratteristiche visive possa portare miglioramenti significativi poiché le immagini sono spesso l'aspetto più curato del catalogo. Al contrario, la qualità del testo varia tra verticali, lingue e feed di prodotti specifici. I nostri test estensivi mostrano che FashionCLIPapprende concetti di moda e li applica con successo a prodotti non visti e descrizioni incomplete o ambigue.

Dal testo all'immagineLa ricerca di prodotti è uno dei principali canali di interazione e ricavi tra un negozio e i suoi utenti, rappresentando in media dal 30% al 60% dei ricavi totali online ,. Storicamente, la ricerca di prodotti è stata eseguita principalmente con caratteristiche testuali, abbinando prima le query e le descrizioni dei prodotti in un indice e poi riordinando i risultati candidati ,,. Tuttavia, ci sono buone ragioni per credere che includere caratteristiche visive possa portare miglioramenti significativi poiché le immagini sono spesso l'aspetto più curato del catalogo. Al contrario, la qualità del testo varia tra verticali, lingue e feed di prodotti specifici. I nostri test estensivi mostrano che FashionCLIPapprende concetti di moda e li applica con successo a prodotti non visti e descrizioni incomplete o ambigue.

In questa sezione, dettagliamo le prestazioni diFashionCLIP su una gamma di compiti, dimostrando l'efficacia dell'adattamento al dominio e l'applicabilità dei modelli simili aCLIP alla moda. I dettagli sull'addestramento e sulla valutazione sono disponibili nella Sezione “[Methods]”. Utilizziamo una varietà di dataset in-domain e out-of-domain, con vari gradi di somiglianza:TEST è il set di test diFarfetch contenente 20k prodotti;HOUT-C è il dataset contenente una categoria che abbiamo escluso dall'addestramento;HOUT-B è il dataset contenente due marchi che sono stati esclusi dall'addestramento;STLE è un dataset di merchandising diFarfetch ;KAGL è un sottoinsieme in cui ogni prodotto ha un'immagine su sfondo bianco, una didascalia e una categoria;F-MNIST contiene 10.000 immagini in scala di grigi di 10 classi di prodotti;DEEP contiene 4000 immagini di prodotti non standardizzate (cioè, contengono umani) da 50 categorie. Una panoramica dei dati di immagine e testuali offerti da Farfetch (TEST, HOUT-C, HOUT-B, STLE ),KAGL ,F-MNIST eDEEP può essere trovata in Fig.[3]. I nostri ampi benchmark e valutazioni rispondono quantitativamente a due domande di ricerca: la conoscenza specifica del dominio può migliorare la comprensione di CLIP di un'industria (Fig.[5]) e, se sì, tale conoscenza si traduce in diversi casi d'uso e dataset?

Il compito di Recupero Multi-modale è descritto come segue: data una descrizione testuale e un insieme di immagini, chiediamo al modello di trovare l'immagine correlata a quella descrizione. Ad esempio, un compito di recupero prodotto implica l'abbinamento di una descrizione del prodotto (ad es., “un polo rosso per uomo”) e una sua foto in un catalogo.

Il recupero multi-modale è possibile grazie all'obiettivo di ottimizzazione diFashionCLIP che allinea gli spazi latenti di linguaggio e immagine (vedi Fig. [1] ). TestiamoFashionCLIP sul recupero multi-modale per valutare i benefici del fine-tuning specifico del dominio nella ricerca di prodotti nel mondo reale.

Il nostro benchmark prende come input una descrizione del prodotto dalset di test del catalogo e chiede ai modelli di classificare le immagini dei prodotti corrispondenti alla didascalia—lo standard di riferimento è l'immagine associata al prodotto. Estraggiamo la classifica utilizzando le somiglianze degli embedding:FashionCLIP esegue il prodotto scalare tra l'embedding della didascalia di input e ciascun embedding vettoriale dell'immagine ottenuto tramiteT¯  θ T( · ) eI¯  θ I( · ) rispettivamente e restituisce una classifica in ordine decrescente. UtilizziamoHITS@5 (Hit Rate @ k = 5) eMRR (Mean Reciprocal Rank) come metriche. La Tabella [1] confrontaFashionCLIP conCLIP non specifico per il dominio su diversi set di test esclusi e mostra come il fine-tuning migliori significativamente la comprensione del nostro dominio target.

Eseguiamo anche test qualitativi estensivi confrontandoFashionCLIP con il motore di ricerca di produzione attualmente impiegato nel catalogo. La Fig. [4] mostra un caso di particolare interesse per la ricerca di prodotti: in questo esempio, i concetti visivi non appartengono al dominio della moda e non sono disponibili nella didascalia. Il primo confronto (sinistra ) mostra cheFashionCLIP può recuperare il concetto ditigre quando richiesto con "t-shirt con tigre"; per la stessa query, il motore di ricerca recupera articoli che corrispondono alla categoria, incapace di interpretaretigre basandosi solo sul testo. Il secondo confronto (destra ) mostra cheFashionCLIP può interpretareun gatto da un disegno stilizzato, parzialmente occultato. Al contrario, il motore di ricerca non riesce a generalizzare oltre le didascalie che contengono esplicitamente la stringa "gatto". Infine, visualizzare gli embedding appresi (Fig. [5] ) aiuta anche a costruire un'intuizione della migliore risoluzione concettuale diFashionCLIP quando si tratta del dominio target.

Replichiamo l'impostazione originale di classificazione zero-shot diCLIP , che ci consente di valutare quantitativamente la trasferibilità delle rappresentazioni finemente sintonizzate diFashionCLIP a diverse distribuzioni di dati dallo stesso verticale (cioè Moda). Il modello generaun embedding dell'immagine per l'immagine del prodotto ek embedding di testo, uno per ciascuna delle etichette nello schema di classificazione (ad esempio, "scarpe", "camicia"). L'etichetta prevista è quella più vicina (misurata tramite prodotto scalare) all'immagine nello spazio vettoriale del modello. Utilizziamoweighted macro F1 come metrica di performance. La Tabella [2] riassume i risultati di diversi benchmark SOTA. Su tutti i benchmark testati,FashionCLIP è superiore aCLIP , un risultato che suggerisce che il fine-tuning specifico per il dominio è effettivamente utile nel dominio e che si generalizza ad altri dataset completamente non visti.

Inoltre, ci proponiamo di investigare l'ipotesi del "cheating" sul nostro modello specifico per il dominio, cioè l'ipotesi che i modelli supervisionati non si generalizzino così bene comeCLIP perché si adattano a caratteristiche spurie uniche per ciascun dataset. Congeliamo l'encoder delle immagini diFashionCLIP e affiniamo un classificatore lineare,LINEAR , sugli embedding generati su un sottoinsieme di categorie (47) dal set di validazione diFarfetch. Eseguiamo benchmark su TEST S, KAGL S,F- MNIST Se DEEP S, versioni campionate dei rispettivi dataset. Dove le etichette sono diverse, adattiamoLINEAR alle etichette raggruppando i punteggi delle classi rilevanti. Confrontiamo questo con la performance zero-shot, utilizzando le etichette originali per generare gli embedding di testo.

La Tabella [3] riporta i nostri risultati, che sono parzialmente simili a quelli diCLIP. Dato cheF-MNIST è molto diverso daTEST —comparabile, ad esempio, a CIFAR-100 vs. ImageNet—la diminuzione delle prestazioni può essere un'indicazione di cheating. Tuttavia,LINEAR si comporta bene sugli altri dataset, con il maggior guadagno perKAGL , la cui immagine del prodotto somiglia di più a quelle inTEST (cioè, articoli ad alta risoluzione su sfondo bianco). Rispetto all'impostazione originale, si potrebbe sostenere che il modello supervisionato ha un compito più facile nel nostro caso: molte meno categorie ( 10 1vs.10 3 ) e articoli relativamente omogenei,F-MNIST a parte.

Mentre lasciamo l'indagine sulla classificazione della moda in contesti più ecologici come lavoro futuro, i nostri risultati contengono intuizioni praticabili per implementazioni nel mondo reale. In particolare, i classificatori supervisionati richiedono ancora un buon grado di intervento manuale anche per dataset simili, e sono completamente inutilizzabili su problemi vicini ma diversi. La Tabella [4] riporta le prestazioni suSTLE divise per articoli relativi a Uomo e Donna. I prodotti nel dataset provengono ancora daFarfetch , ma le etichette sono assegnate manualmente dai merchandiser e sono ortogonali alla tassonomia (classico, streetwear, edgy vs.scarpe, cappelli, borse ). La versatilità offerta dalla supervisione linguistica consente ai modelli zero-shot di affrontare la sfida con una semplice ingegneria dei prompt ("un articolo in stileclassico "); al contrario, i modelli supervisionati richiederebbero un nuovo pipeline di addestramento e valutazione. Come sottolineato sopra, l'apprendimento di concetti generali di moda è la motivazione principale dietroquesto lavoro: mentre pipeline specifiche e supervisionate possono ancora essere la scelta migliore per problemi specifici, non sono più l'unica opzione praticabile in scenari multi-task grazie all'avvento di modelli su larga scala comeFashionCLIP. Sebbene nessuna risposta singola possa adattarsi a tutti i casi d'uso, desideriamo incoraggiare il processo decisionale basato sui dati tracciando tutte le opzioni e fornendo valutazioni di costi e prestazioni.

Come argomentato nell'Introduzione , dato che siamo interessati a stabilire una connessione tra generalità e scalabilità attraverso modelli multi-modali di grandi dimensioni, è importante valutare ulteriormente la qualità delle rappresentazioni apprese. Mentre la questione seFashionCLIPapprenda la moda è stata affrontata quantitativamente sopra, siamo anche interessati a valutare il modello da una prospettiva teorica più ampia di comprensione del linguaggio, offrendo uno sguardo sull'estensione delle capacità di generalizzazione “vere” diFashionCLIP ,ala “uso infinito di mezzi finiti”.

La letteratura sulla composizionalità del linguaggio si estende per secoli: limitandoci solo a lavori recenti, ilgrounding è stato esplorato in connessione con l'apprendimento efficiente e la “vera comprensione”, ,. L'uso di principi combinatori per testare le capacità di generalizzazione è una strategia nota nel mondo dei giocattoli: sfruttiamo intuizioni dal nostro dominio di riferimento per operazionalizzare principi simili su oggetti delmondo reale,.

In questa sezione, forniamo prove di grounding semantico inFashionCLIP e su questa base offriamo un'indagine preliminare delle sue capacità composizionali. La nostra analisi parte da due lezioni della ricerca precedente. In primo luogo, lemappe di localizzazione sono un modo efficace per sondare il modello per la conoscenzareferenziale (qui prendiamo in prestito la distinzione referenziale/inferenziale dal lavoro classico di Marconi) e la conoscenza lessicale visivamente radicata. In secondo luogo, dal punto di vista linguistico, la maggior parte delle query di ricerca nella moda ha la forma di Frasi Nominali (NPs)—ad esempio, “vestito armani”. Pertanto, la semantica delle NP può essere considerata una buona generalizzazione del mondo reale per studiare le capacitàcomposizionali einferenziali diFashionCLIP, ,.

Sondiamo  FashionCLIP per prove di conoscenza referenziale e indaghiamo le sue capacità di grounding utilizzando mappe di localizzazione. Applichiamo ulteriormente le mappe di localizzazione al compito di parsing della moda zero-shot—un problema aperto cruciale nell'industria.

Le mappe di localizzazione si ottengono oscurando ripetutamente diverse parti dell'immagine. Codifichiamo quindi ciascuna versione oscurata e misuriamo la sua distanza dal testo target nello spazio contrastivo. Intuitivamente, più l'immagine viene allontanata dall'occultamento, più forte era il legame tra il concetto visivo rimosso e il testo e, a sua volta, più alto è il suo punteggio sulla mappa. Il parsing della moda è un caso specifico di segmentazione semantica in cui le annotazioni delle bounding box contengono articoli di abbigliamento. Estraggiamo le annotazioni delle bounding box (come un'approssimazione della segmentazione fine) dalle mappe di localizzazione trovando il rettangolo di delimitazione minimo delle aree altamente attivate.

Come mostrato nelle Figg.[6]e[8], caratteristiche come "tacchi alti", "cinturino alla caviglia", "maniche lunghe" sono ben rappresentate in  FashionCLIP ; il modello sembra anche essere molto consapevole dei marchi, in forma più o meno esplicita.  FashionCLIP rileva il logo astratto sulle  sneakers (Fig.[6]), oltre a mostrare (simile a  CLIP ) buone capacità di OCR, quando riconosce un logo come stringa di testo esplicita. La Fig.[7]mostra annotazioni di bounding box zero-shot di alcuni campioni nel dataset ModaNet precedentemente non visto. Sebbene sia improbabile che i modelli zero-shot possano sostituire l'addestramento specializzato per la segmentazione, crediamo che modelli come  FashionCLIP potrebbero fornire un modo economico per generare etichette probabilistiche per pipeline di supervisione debole.

Data l'evidenza preliminare che i concetti isolati si mappano in modo affidabile su regioni visive, la nostra ipotesi di lavoro è che  FashionCLIP dovrebbe esibire vere abilità  inferenziali  componendo tali concetti per generare nuovi NPs.

Ci basiamo sulla conoscenza del dominio, sulla letteratura precedente e sull'inventario di  Farfetch per sondare il modello per la conoscenza di  marchi (ad es. "nike"),  caratteristiche ("tacchi alti") e  disegni ("tastiera"), verificando manualmente la mappatura testo-regione per ciascuno di questi concetti tramite mappe di localizzazione. Dato che questi singoli concetti sono radicati in regioni (Fig.[8]), potremmo sfruttare questa conoscenza per generare nuove immagini e NPs  sistematicamente. Fondamentale, possiamo assegnare una semantica definita a un nuovo NP  marchio + oggetto che descrive un "oggetto improbabile" che non è mai stato visto prima (Fig.[9]). Gli oggetti improbabili variano: possono rappresentare combinazioni strane di concetti, come un  abito lungo Nike , un oggetto surreale,  sneakers con maniglie , o un'estensione improbabile di articoli di moda esistenti, come la  pochette tastiera (che generalizza il tema trovato per la prima volta nella  cravatta tastiera di J. Mugatu). Un nuovo NP come "abito nike" richiederebbe che la regione visiva corrispondente alla parola  abito contenga la regione visiva del logo corrispondente alla parola  nike.

Integriamo la nostra analisi riutilizzando la nostra pipeline di classificazione e recupero: nel compito di classificazione,  FashionCLIP raggiunge un'accuratezza di 0.74 quando viene chiesto di scegliere l'etichetta improbabile da un insieme di distrattori credibili. I seguenti sono esempi di casi di test:obiettivo: ABITO NIKE(come visto nella Fig. [9] ), etichette: Abito Nike, un abito Armani, una camicia, la bandiera dell'Italia, un abito Gucci, una t-shirt Nike;

obiettivo: ABITO NIKE(come visto nella Fig. [9] ), etichette: Abito Nike, un abito Armani, una camicia, la bandiera dell'Italia, un abito Gucci, una t-shirt Nike;

Sebbene un'indagine completa delle capacità composizionali sia al di là dello scopo di  questo contributo, le inferenze di  FashionCLIP su prodotti improbabili suggeriscono la presenza di  un certo grado di composizionalità: concetti di moda importanti sono "identificabili" nello spazio latente e possono essere isolati e ricombinati in concetti non visti, esibendo su piccola scala la generalizzazione creativa che solitamente associamo ai sistemi simbolici. Inoltre, la capacità di distinguere "scarpe rosse con tacco nero" da "scarpe nere con tacco rosso" implica una conoscenza che va oltre una semantica di bag-of-words.

Ricerche recenti suggeriscono che le capacità composizionali di CLIP siano limitate.. Come mostrato dai nostri risultati, domini ristretti consentono una manipolazione diretta, senza il rischio di confusione; infatti, i domini ristretti possono essere più facili da esplorare ma è necessaria un'ulteriore indagine per confermare le capacità composizionali. Inoltre, come suggerito dall'uso dell'obiettivo MASKClip introdotto nel modello  ARMANI , l'aggiunta di una segmentazione visiva esplicita può indurre una migliore discriminazione per certi concetti di moda. Sebbene perdite più costose siano un'area interessante all'intersezione tra grounding e composizionalità, dato sia il focus generativo ristretto che l'entità dei miglioramenti nel documento originale, le loro conclusioni non possono essere applicate direttamente a  FashionCLIP. Attendiamo con interesse di condurre ricerche future combinando intuizioni da casi d'uso generativi e discriminativi.

FashionCLIP è un adattamento di dominio diCLIP , motivato da casi d'uso centrali nella moda: diversamente dai metodisupervisionati specifici per compito ,FashionCLIP non necessita di un'architettura specializzata, etichettatura e tuning. Abbiamo verificato ampiamente la flessibilità offerta dalla supervisione linguistica e indagato le capacità semantiche diFashionCLIP su nuovi compiti. Il nostro focus su un'industria specifica consente non solo guadagni pratici ma apre anche possibilità teoriche restringendo il dominio, che è ancora ampio, ma anche facile da manipolare. Fornendo prove quantitative e qualitative che l'apprendimento contrastivo, unito a un dataset ampio e diversificato, può effettivamente produrre concetti industriali multi-modello generali, colleghiamo virtù teoriche con significativi guadagni pratici e apriamo nuove possibilità per scalare il dispiegamento orizzontale dei sistemi di apprendimento automatico in modo efficace.

Come sistema veramente generale, i concetti diFashionCLIP potrebbero essere utilizzati per molti più compiti: ad esempio, le rappresentazioni multi-modali possono essere caratteristiche in sistemi a valle, o utilizzate direttamente per raccomandazioni zero-shot in scenari item-to-item; la classificazione su etichette arbitrarie potrebbe essere utilizzata come un meccanismo di etichettatura veloce e scalabile, supportando l'etichettatura probabilistica o la generazione di dati per modelli IR multi-modali. Pur lasciando questo (e molti altri temi) a future iterazioni, crediamo chequesto lavoro—con i suoi artefatti e metodologia—sia una prima valutazione completa del grande potenziale dei concetti multi-modali generali e trasferibili per il commercio digitale.

Gli autori sono consapevoli dei rischi dei modelli multi-modali simili aCLIP in produzione associati alla loro limitata robustezza, così come delle problematiche generali legate ai bias nei modelli di linguaggio di grandi dimensioni pre-addestrati su larga scala. In particolare, riconosciamo che il rischio di attacchi avversari sui modelli multi-modali è un'area di ricerca attiva,. Nei limiti delle nostre conoscenze, non abbiamo motivo di credere cheFashionCLIP introduca alcun rischioaggiuntivo rispetto al CLIP originale. Come con il modello originale, va notato cheFashionCLIP sembra essere suscettibile agli "attacchi tipografici" (Fig.[10]). Nessun dataset utilizzato per l'addestramento o il test contiene PII e/o altri dati sensibili degli utenti.

Farfetch ha reso disponibile per la prima volta un dataset in inglese comprendente oltre 800k prodotti di moda, con più di 3k marchi attraverso dozzine di tipi di oggetti. Rispetto ad altri grandi dataset di moda, il nostro dataset è significativamente più completo di DeepFashion, che manca di descrizioni testuali dettagliate, e persino più grande di CM-Fashion, che è stato raccolto senza alcun coinvolgimento diretto da parte diFarfetch. Gli articoli sono organizzati in alberi gerarchici, producendo una tassonomia a tre livelli: per esempio, glialberi potrebbero essere qualcosa comeAbbigliamento> Vestiti> Vestiti da giorno oAbbigliamento> Cappotti> Parka , per un totale di oltre 800 alberi. Come input per l'encoder di immagini, utilizziamo l'immagine standard del prodotto, che è una foto dell'articolo su uno sfondo bianco, senza persone (le immagini seguono un insieme specifico di regole riguardanti il posizionamento dell'articolo, le luci della foto, ecc., progettate per evidenziare le caratteristiche dell'articolo); per quanto riguarda il testo,Farfetch ha due tipi di testo,highlight (ad es., “strisce”, “maniche lunghe”, “Armani”) e unabreve descrizione (“t-shirt in stile anni '80”). Vedi Fig. [3] per un esempio.

Creiamo un set di addestramento, validazione e test dal catalogo campionando casualmente i prodotti. I nostri set finali di addestramento e validazione comprendono rispettivamente 700k e 50k prodotti da 188 categorie.

Applichiamo il fine-tuning partendo dalCLIP pre-addestrato con i seguenti parametri: utilizziamo l'Adam Optimizer con betas in (0.9, 0.98), epsilon di 1e−6 e weight decay pari a 0.2 e tre diversi tassi di apprendimento [1e−4, 1e−5, 1e−6]. Addestriamo i modelli per 4 epoche, valutiamo ogni 500 passi e selezioniamo il modello con la perdita di validazione più bassa per ogni configurazione (Tabella [5] , modello selezionato in grassetto). Nei nostri test preliminari, il modello con la perdita di validazione più bassa in generale non ha generalizzato al meglio nell'impostazione zero-shot. Questo pone una domanda interessante, lasciata per lavori futuri, su come effettuare il fine-tuning di questi grandi modelli pre-addestrati senza perdere in generalizzazione. La pipeline è stata implementata con Metaflow, con l'addestramento eseguito da remoto su GPU cloud; il tracciamento degli esperimenti è stato fornito da Comet.

Prepariamo i seguenti dataset per scopi di test e per valutare ulteriormente l'impatto potenziale del modello in produzione su larga scala.TEST è il set di test diFarfetch contenente 20k prodotti;HOUT-C è il dataset contenente una categoria che abbiamo escluso dall'addestramento (Performance Tops ), per un totale di 1.5k articoli;HOUT-B è il dataset contenente due marchi che sono stati esclusi dall'addestramento, per un totale di 1.7k articoli;STLE è un dataset di merchandising diFarfetch , completamente indipendente dal catalogo, che classifica 7749 articoli in 6 stili per il genere femminile e 4 stili per il genere maschile; esempi di stili sonoClassico eStreetwear e ogni articolo può appartenere a più di uno stile;KAGL è un sottoinsieme di, dove ogni prodotto ha un'immagine su sfondo bianco, una didascalia e una categoria, per un totale di 9990 articoli su 62 categorie;F-MNIST contiene 10,000 immagini in scala di grigi da 10 classi di prodotti, con intensità dei pixel invertita per ottenere immagini con sfondo bianco (nota che queste immagini hanno una dimensione di 24 × 24 mostrando quindi molti meno dettagli rispetto alle immagini su cui i modelli sono stati addestrati).DEEP contiene 4000 immagini di prodotti che non sono standardizzate (cioè contengono persone) da 50 categorie.

Riutilizziamo l'architettura principale diCLIP , che descriviamo brevemente nell'Introduzione per completezza. Alla fine, otteniamo uno spazio multi-modale dove immagini e testi sono proiettati e appresi congiuntamente: se l'addestramento è stato efficace, ci aspettiamo che, per esempio, l'embedding testuale per la stringa “abito lungo rosso” sia effettivamente simile (misurato tramite il prodotto scalare) agli embedding delle immagini di abiti rossi. La Tabella [5] mostra il tempo di addestramento, le prestazioni e i costi.

Table 1: Confronto tra FashionCLIP( F-CLIP) e CLIPnel compito di recupero multi-modale.
Modello | Dataset | HITS@5 | MRR
F-CLIP| TEST |  0.66|  0.50
CLIP| 0.28 | 0.21
F-CLIP| HOUT-C |  0.62|  0.47
CLIP| 0.33 | 0.23
F-CLIP| HOUT-B |  0.58|  0.41
CLIP| 0.31 | 0.22
I modelli con le migliori prestazioni sono in grassetto.

Table 2: Confronto delle prestazioni di FashionCLIP( F-CLIP) nel compito di classificazione dei prodotti su diversi dataset ( F1è weighted macro F1).
Modello | Dataset | F1
F-CLIP| TEST |  0.39
CLIP| 0.31
F-CLIP| KAGL |  0.67
CLIP| 0.63
F-CLIP| F-MNIST |  0.71
CLIP| 0.66
F-CLIP| DEEP |  0.47
CLIP| 0.45
I modelli con le migliori prestazioni sono in grassetto.

Table 3: Prestazioni di classificazione LINEARrispetto al zero-shot su F-CLIP( F1è weighted macro F1).
Dataset | F-CLIP | LINEAR |ΔF1
TESTS | 0.746 | 0.900 | + 0.154
KAGLS | 0.764 | 0.881 | + 0.117
DEEPS | 0.411 | 0.444 | + 0.033
F- MNISTS| 0.781 | 0.602 | − 0.179

Table 4: F1 macro su STLE; Priorclassifica utilizzando le probabilità empiriche delle classi.
Modello | Uomo | Donna
Prior| 0.24 | 0.20
F-CLIP|  0.36|  0.27
CLIP| 0.33 | 0.17
I modelli con le migliori prestazioni sono in grassetto.

Table 5: Confronto tra tempo di addestramento, prestazioni, costi ed emissioni di carbonio su varianti dell'architettura FashionCLIPsul catalogo Farfetch.
LR | Loss | Tempo(m) | USD | kgCO 2eq
1e−4 | 16.0 | 618 | 31$ | 0.77
1e−5 | 1.73 | 617 | 31$ | 0.77
1e−6|  2.83|  621|  31$|  0.78
Il costo è calcolato con il prezzo AWS per un p3.2xlarge; le stime sono state condotte utilizzando il calcolatore di impatto del Machine Learning. Modello utilizzato per i test in grassetto.

Figure 1: Rappresentazione bidimensionale delle immagini e del testo nello spazio vettoriale di FashionCLIP prima e dopo l'addestramento. Le immagini e le loro corrispondenti descrizioni testuali sono incorporate più vicine tra loro nello spazio vettoriale latente dopo l'addestramento.

Figure 2: Panoramica schematica del recupero multi-modale (sinistra) e dei compiti di classificazione zero-shot (destra).

Figure 3: Campione di dati da vari dataset utilizzati. Osserviamo una gamma di distribuzioni sia nelle modalità immagine che testuale. Per la modalità immagine, vediamo una gamma da "Bassa risoluzione, B &N" a "Alta risoluzione, In-the-Wild". Per la modalità testuale, Farfetch offre la migliore "risoluzione testuale", mentre DEEP presenta anche una terminologia molto specifica per la moda. I datasetKAGL,F-MNISTeDEEPsono pubblicamente disponibili. Per maggiori dettagli riguardanti i dati, vedere la Sezione Disponibilità dei Dati.

Figure 4: Recupero con concetti non legati alla moda. Risultati di esempio per "t-shirt con tigre" e "t-shirt con gatto" da FashionCLIP ( verde ) vs motore di ricerca di produzione Farfetch ( rosso ).

Figure 5: Confronto della proiezione T-SNE dello Spazio Immagini di F-CLIP e CLIP. Osserviamo un miglior raggruppamento (punteggio silhouette 0.115 vs 0.0745 ) in F-CLIP per categorie come Camicie, Gonne e Vestiti, dove i prodotti formano un cluster più denso con meno sovrapposizione tra le categorie, suggerendo che lo spazio latente di F-CLIP è meglio adattato ai concetti di moda.

Figure 6: Conoscenza lessicale radicata. Le mappe sono sonde facili da usare nella conoscenza della moda del modello. Da sinistra a destra : mappa di localizzazione per "maniche lunghe" su un polo rosso; sneakers e la mappa per "Nike", una cover per telefono e la mappa per "Palm Angels"; la stessa cover per telefono e mappa, quando il logo è scritto con un font fuori distribuzione in un nuovo punto.

Figure 7: Rilevamento delle bounding-box degli oggetti. Le mappe di localizzazione possono essere facilmente estese per fornire bounding-box zero-shot per gli oggetti di interesse. Le bounding-box verdi mostrano le posizioni previste per i concetti di moda "Zaino" (sinistra) e "Cappello di paglia" (destra). Le immagini sopra sono tratte dal dataset pubblico Unsplash Lite Dataset 1.2.0: FashionCLIP è stato testato ampiamente su ModaNet - si prega di contattare gli autori per i link a queste immagini.

Figure 8: Grounding e composizionalità. Mappe di localizzazione per un prodotto recuperato con la query "sandali con cinturino alla caviglia e tacchi alti": da sinistra a destra, il prodotto, "cinturino alla caviglia", "sandali", "tacchi alti").

Figure 9: Prodotti improbabili. Combinando caratteristiche di moda, marchi e articoli in modi nuovi, otteniamo prodotti visivamente realistici con chiari significati composizionali zero-shot. Da sinistra a destra: "abito lungo Nike", "converse con maniglie", "scarpe rosse con tacco alto nero", "pochette tastiera".

Figure 10: Attacco tipografico. FashionCLIP identifica correttamente l'oggetto a sinistra come una "mela", ma classifica erroneamente quello a destra come "nike air", poiché il testo agisce come un elemento di confusione.

