Esplorazione di schemi arricchiti in un dataset con l'analisi delle componenti principali contrastive

Abstract: La visualizzazione e l'esplorazione di dati ad alta dimensionalità è una sfida onnipresente attraverso le discipline. Tecniche ampiamente utilizzate come l'analisi delle componenti principali (PCA) mirano a identificare le tendenze dominanti in un dataset. Tuttavia, in molti contesti abbiamo dataset raccolti in condizioni diverse, ad esempio, un trattamento e un esperimento di controllo, e siamo interessati a visualizzare ed esplorare schemi specifici di un dataset. Questo documento propone un metodo, l'analisi delle componenti principali contrastive (cPCA), che identifica strutture a bassa dimensionalità che sono arricchite in un dataset rispetto ai dati di confronto. In una vasta gamma di esperimenti, dimostriamo che cPCA con un dataset di background ci consente di visualizzare schemi specifici del dataset che vengono persi dalla PCA e da altri metodi standard. Forniamo inoltre un'interpretazione geometrica di cPCA e forti garanzie matematiche. Un'implementazione di cPCA è pubblicamente disponibile e può essere utilizzata per l'analisi esplorativa dei dati in molte applicazioni in cui attualmente si utilizza la PCA.

L'analisi delle componenti principali (PCA) è uno dei metodi più ampiamente utilizzati per l'esplorazione e la visualizzazione dei dati. La PCA proietta i dati in uno spazio a bassa dimensione ed è particolarmente potente come approccio per visualizzare modelli, come cluster, cline e outlier in un dataset. Esiste un gran numero di metodi di visualizzazione correlati; ad esempio, t-SNE e il multidimensional scaling (MDS) consentono proiezioni di dati non lineari e possono catturare meglio i modelli non lineari rispetto alla PCA. Tuttavia, tutti questi metodi sono progettati per esplorare un dataset alla volta. Quando l'analista ha più dataset (o più condizioni in un dataset da confrontare), lo stato attuale della pratica è eseguire la PCA (o t-SNE, MDS, ecc.) su ciascun dataset separatamente, e quindi confrontare manualmente le varie proiezioni per esplorare se ci sono somiglianze e differenze interessanti tra i dataset,. La PCA contrastiva (cPCA) è progettata per colmare questa lacuna nell'esplorazione e visualizzazione dei dati identificando automaticamente le proiezioni che mostrano le differenze più interessanti tra i dataset. La Figura[1]fornisce una panoramica della cPCA che spieghiamo più dettagliatamente in seguito.

La cPCA è motivata da una vasta gamma di problemi attraverso le discipline. Per illustrazione, menzioniamo qui due di questi problemi e ne dimostriamo altri attraverso esperimenti più avanti nel documento. Innanzitutto, consideriamo un dataset di misurazioni dell'espressione genica da individui di diverse etnie e sessi. Questi dati includono i livelli di espressione genica dei pazienti oncologici {x i}, che siamo interessati ad analizzare. Abbiamo anche dati di controllo, che corrispondono ai livelli di espressione genica di pazienti sani {y i} da un background demografico simile. Il nostro obiettivo è trovare tendenze e variazioni all'interno dei pazienti oncologici (ad esempio, per identificare sottotipi molecolari di cancro). Se applichiamo direttamente la PCA a {x i}, tuttavia, le componenti principali superiori possono corrispondere alle variazioni demografiche degli individui invece che ai sottotipi di cancro perché le variazioni genetiche dovute ai primi sono probabilmente maggiori di quelle dei secondi. Affrontiamo questo problema notando che anche i pazienti sani contengono la variazione associata alle differenze demografiche, ma non la variazione corrispondente ai sottotipi di cancro. Pertanto, possiamo cercare componenti in cui {x i} ha alta varianza ma {y i} ha bassa varianza.

Come esempio correlato, consideriamo un dataset {x i} che consiste di cifre scritte a mano su uno sfondo complesso, come diverse immagini di erba (vedi Fig.[2(a), top]). L'obiettivo di un tipico compito di apprendimento non supervisionato può essere quello di raggruppare i dati, rivelando le diverse cifre nell'immagine. Tuttavia, se applichiamo la PCA standard su queste immagini, scopriamo che le componenti principali superiori non rappresentano caratteristiche relative alle cifre scritte a mano, ma riflettono la variazione dominante nelle caratteristiche relative allo sfondo dell'immagine (Fig.[2(b)], top). Mostriamo che è possibile correggere questo utilizzando un dataset di riferimento {y i} che consiste esclusivamente di immagini dell'erba (non necessariamente le stesse immagini usate in {x i} ma con covarianza simile tra le caratteristiche, come mostrato in Fig.[2(a)], bottom), e cercando il sottospazio di maggiore varianza in {x i} rispetto a {y i}. Proiettando su questo sottospazio, possiamo effettivamente separare visivamente le immagini in base al valore della cifra scritta a mano (Fig. 2(b), bottom). Confrontando le componenti principali scoperte dalla PCA con quelle scoperte dalla cPCA, vediamo che la cPCA identifica caratteristiche più rilevanti (Fig.[2(c)]), il che ci consente di utilizzare la cPCA per applicazioni come la selezione delle caratteristiche e la riduzione del rumore.

La PCA contrastiva è uno strumento per l'apprendimento non supervisionato, che riduce efficacemente la dimensionalità per consentire la visualizzazione e l'analisi esplorativa dei dati. Questo separa la cPCA da una vasta classe di metodi di apprendimento supervisionato il cui obiettivo principale è classificare o discriminare tra vari dataset, come l'analisi discriminante lineare (LDA), l'analisi discriminante quadratica (QDA), la PCA supervisionata e QUADRO. Questo distingue anche la cPCA dai metodi che integrano più dataset, con l'obiettivo di identificare modelli correlati tra due o più dataset, piuttosto che quelli unici per ciascun dataset individuale. Esiste anche una ricca famiglia di metodi non supervisionati per la riduzione dimensionale oltre alla PCA. Ad esempio, il multidimensional scaling (MDS) trova un embedding a bassa dimensione che preserva la distanza nello spazio ad alta dimensione; la ricerca delle componenti principali trova un sottospazio a basso rango che è robusto al rumore di piccola entità e agli errori sparsi grossolani. Ma nessuno è progettato per utilizzare informazioni rilevanti da un secondo dataset, come fa la cPCA. Nel supplemento, abbiamo confrontato la cPCA con molte delle tecniche precedentemente menzionate su dataset rappresentativi (vedi Supplementary Figs.[3]e[4]), , ,.

In un dominio applicativo specifico, possono esserci strumenti specializzati in quel dominio con obiettivi simili alla cPCA, ,. Ad esempio, nei risultati, mostriamo come la cPCA applicata sui dati genotipici visualizza l'ascendenza geografica all'interno del Messico. Esplorare cluster di ascendenze genetiche a grana fine è un problema importante nella genetica delle popolazioni, e i ricercatori hanno recentemente sviluppato un algoritmo per visualizzare specificamente tali cluster di ascendenza. Sebbene la cPCA funzioni bene qui, l'algoritmo creato da esperti potrebbe funzionare ancora meglio per un dataset specifico. Tuttavia, l'algoritmo specializzato richiede una conoscenza sostanziale del dominio per essere progettato, è più costoso dal punto di vista computazionale e può essere difficile da usare. L'obiettivo della cPCA non è sostituire tutti questi metodi specializzati all'avanguardia in ciascuno dei loro domini, ma fornire un metodo generale per esplorare dataset arbitrari.

Proponiamo un algoritmo concreto ed efficiente per la cPCA in questo documento. Il metodo prende come input un dataset target {x i} che siamo interessati a visualizzare o identificare modelli all'interno. Come input secondario, la cPCA prende un dataset di background {y i}, che non contiene i modelli di interesse. L'algoritmo cPCA restituisce sottospazi che catturano una grande quantità di variazione nei dati target {x i}, ma poco nel background {y i} (vedi Fig.[1], Metodi, e Metodi Supplementari per maggiori dettagli). Questo sottospazio corrisponde a caratteristiche contenenti struttura specifica a {x i}. Pertanto, quando i dati target sono proiettati su questo sottospazio, siamo in grado di visualizzare e scoprire la struttura aggiuntiva nei dati target rispetto al background. Analogamente alle componenti principali (PC), chiamiamo le direzioni trovate dalla cPCA le componenti principali contrastive (cPC). Sottolineiamo che la cPCA è fondamentalmente una tecnica non supervisionata, progettata per risolvere i modelli in un dataset più chiaramente utilizzando il dataset di background come contrasto. In particolare, la cPCA non cerca di discriminare tra i dataset target e di background; il sottospazio che contiene tendenze arricchite nel dataset target non è necessariamente lo stesso sottospazio che è ottimale per la classificazione tra i dataset.

I ricercatori hanno notato che la PCA standard è spesso inefficace nel scoprire sottogruppi all'interno dei dati biologici, almeno in parte perché "le componenti principali dominanti...correlano con artefatti," piuttosto che con caratteristiche che sono di interesse per il ricercatore. Come può essere utilizzata la cPCA in questi contesti per rilevare i sottogruppi più significativi? Utilizzando un dataset di background per annullare la variazione universale ma non interessante nel target, possiamo cercare una struttura che è unica per il dataset target.

Il nostro primo esperimento utilizza un dataset costituito da misurazioni dell'espressione proteica di topi che hanno ricevuto terapia d'urto,. Alcuni dei topi hanno sviluppato la Sindrome di Down (DS). Per creare un compito di apprendimento non supervisionato in cui abbiamo informazioni di verità a terra per valutare i metodi, assumiamo che queste informazioni DS non siano conosciute dall'analista e le usiamo solo per la valutazione dell'algoritmo. Vorremmo vedere se rileviamo differenze significative all'interno della popolazione di topi sottoposti a shock in modo non supervisionato (la presenza o l'assenza della Sindrome di Down essendo un esempio chiave). Nella Fig. [3a] (top), mostriamo il risultato dell'applicazione della PCA al dataset target: i dati trasformati non rivelano alcun raggruppamento significativo all'interno della popolazione di topi. Le principali fonti di variazione all'interno dei topi possono essere naturali, come il sesso o l'età.

Applichiamo la cPCA a questo dataset utilizzando un background costituito da misurazioni dell'espressione proteica di un set di topi che non sono stati esposti alla terapia d'urto. Sono topi di controllo che probabilmente hanno una variazione naturale simile ai topi sperimentali, ma senza le differenze che derivano dalla terapia d'urto. Con questo dataset come background, la cPCA è in grado di risolvere due gruppi diversi nei dati target trasformati, uno corrispondente a topi che non hanno la Sindrome di Down e uno corrispondente (per lo più) a topi che hanno la Sindrome di Down, come illustrato nella Fig. [3a] (bottom). Come confronto, abbiamo anche applicato altre 8 tecniche di riduzione dimensionale per identificare le direzioni che differenziano tra i dataset target e di background, nessuna delle quali è stata in grado di separare i topi così bene come la cPCA (vedi Supplementary Fig. [4] per i dettagli).

Successivamente, analizziamo un dataset pubblico ad alta dimensione costituito da livelli di espressione di RNA a singola cellula di una miscela di cellule mononucleate del midollo osseo (BMMC) prelevate da un paziente con leucemia prima del trapianto di cellule staminali e BMMC dallo stesso paziente dopo il trapianto di cellule staminali. Tutti i dati di RNA-Seq a singola cellula sono pre-processati utilizzando metodi simili a quelli descritti dagli autori. In particolare, prima di applicare la PCA o la cPCA, tutti i dataset sono ridotti a 500 geni, che sono selezionati sulla base della dispersione più alta [varianza divisa per la media] all'interno dei dati target. Ancora una volta, eseguiamo la PCA per vedere se possiamo scoprire visivamente i due campioni nei dati trasformati. Come mostrato nella Fig. [3b] (top left), entrambi i tipi di cellule seguono una distribuzione simile nello spazio coperto dai primi due PC. Questo è probabilmente dovuto al fatto che le differenze tra i campioni sono piccole e i PC riflettono invece l'eterogeneità di vari tipi di cellule all'interno di ciascun campione o anche variazioni nelle condizioni sperimentali, che possono avere un effetto significativo sulle misurazioni di RNA-Seq a singola cellula.

Applichiamo cPCA utilizzando un dataset di background che consiste in misurazioni RNA-Seq dalle cellule BMMC di un individuo sano. Ci aspettiamo che questo dataset di background contenga la variazione dovuta alla popolazione eterogenea di cellule così come variazioni nelle condizioni sperimentali. Possiamo sperare, quindi, che cPCA possa essere in grado di recuperare direzioni che sono arricchite nei dati target, corrispondenti alle differenze pre- e post-trapianto. Infatti, è ciò che troviamo, come mostrato in Fig. [3b] (in basso a sinistra).

Inoltre, aumentiamo il nostro dataset target con campioni BMMC da un secondo paziente con leucemia, nuovamente prima e dopo il trapianto di cellule staminali. Quindi, ci sono un totale di quattro sottopopolazioni di cellule. L'applicazione di PCA su questi dati mostra che le quattro sottopopolazioni non sono separabili nel sottospazio coperto dai primi due componenti principali (PCs), come mostrato in Fig. [3b] (in alto a destra). Tuttavia, quando cPCA è applicato con lo stesso dataset di background, almeno tre delle sottopopolazioni mostrano una separazione molto più forte, come mostrato in Fig. [3b] (in basso a destra). L'incorporazione di cPCA suggerisce anche che i campioni di cellule di entrambi i pazienti sono più simili tra loro dopo il trapianto di cellule staminali (punti ciano e verdi) rispetto a prima del trapianto (punti oro e rosa), un'ipotesi ragionevole che può essere testata dall'investigatore. Si può fare riferimento alla Fig. Supplementare [5] per maggiori dettagli su questo esperimento. Vediamo che cPCA può essere uno strumento utile per inferire la relazione tra sottopopolazioni, un argomento che esploriamo ulteriormente in seguito.

Negli esempi precedenti, abbiamo visto che cPCA consente all'utente di scoprire sottoclassi all'interno di un dataset target che non sono etichettate a priori. Tuttavia, anche quando le sottoclassi sono conosciute in anticipo, la riduzione della dimensionalità può essere un modo utile per visualizzare la relazione all'interno dei gruppi. Ad esempio, PCA è spesso utilizzato per visualizzare la relazione tra popolazioni etniche basate su varianti genetiche, perché proiettare le varianti genetiche su due dimensioni spesso produce mappe che offrono visualizzazioni sorprendenti di tendenze geografiche e storiche,. Ma ancora, PCA è limitato a identificare la struttura più dominante; quando questo rappresenta una variazione universale o poco interessante, cPCA può essere più efficace nel visualizzare le tendenze.

Il dataset che utilizziamo per questo esempio consiste in polimorfismi a singolo nucleotide (SNPs) dai genomi di individui di cinque stati in Messico, raccolti in uno studio precedente. L'ascendenza messicana è difficile da analizzare utilizzando PCA poiché i PCs di solito non riflettono l'origine geografica all'interno del Messico; invece, riflettono la proporzione di eredità europea/americana nativa di ciascun individuo messicano, che domina e oscura le differenze dovute all'origine geografica all'interno del Messico (vedi Fig. [4a] ). Per superare questo problema, i genetisti della popolazione potano manualmente gli SNPs, rimuovendo quelli noti per derivare dall'ascendenza europea, prima di applicare PCA. Tuttavia, questa procedura è di applicabilità limitata poiché richiede di conoscere l'origine degli SNPs e che la fonte della variazione di background sia molto diversa dalla variazione di interesse, cosa che spesso non è il caso.

Come alternativa, utilizziamo cPCA con un dataset di background che consiste in individui dal Messico e dall'Europa. Questo background è dominato dalla variazione nativa americana/europea, permettendoci di isolare la variazione intra-messicana nel dataset target. I risultati dell'applicazione di cPCA sono mostrati in Fig. [4b]. Troviamo che gli individui dello stesso stato in Messico sono incorporati più vicini tra loro. Inoltre, i due gruppi che sono i più divergenti sono i Sonorani e i Maya dello Yucatan, che sono anche i più distanti geograficamente all'interno del Messico, mentre i messicani degli altri tre stati sono vicini tra loro, sia geograficamente che nell'incorporazione catturata da cPCA (vedi Fig. [4c] ). Vedi anche la Fig. Supplementare [6] per maggiori dettagli.

In molti contesti di scienza dei dati, siamo interessati a visualizzare ed esplorare modelli che sono arricchiti in un dataset rispetto ad altri dati. Abbiamo presentato cPCA come uno strumento generale per eseguire tale esplorazione contrastiva, e abbiamo illustrato la sua utilità in una gamma diversificata di applicazioni. I principali vantaggi di cPCA sono la sua generalità e facilità d'uso. Calcolare un particolare cPCA richiede essenzialmente lo stesso tempo di calcolare un PCA regolare. Questa efficienza computazionale consente a cPCA di essere utile per l'esplorazione interattiva dei dati, dove ogni operazione dovrebbe idealmente essere quasi immediata. In tal modo, qualsiasi impostazione in cui PCA è applicato su dataset correlati, cPCA può essere applicato. Nella Nota Supplementare[3]e nella Fig. Supplementare[8], mostriamo come cPCA può essere kernelizzato per scoprire modelli contrastivi non lineari nei dataset.

L'unico parametro libero di cPCA contrastivo è la forza del contrastoα. Nel nostro algoritmo predefinito, abbiamo sviluppato uno schema automatico basato su cluster di sottospazi per selezionare i valori più informativi diα (vedi Metodi). Tutti gli esperimenti eseguiti per questo documento utilizzano i valori diα generati automaticamente, e crediamo che questo predefinito sarà sufficiente in molte applicazioni di cPCA. L'utente può anche inserire valori specifici perα se è desiderata un'esplorazione più dettagliata.

cPCA, come PCA regolare e altri metodi di riduzione della dimensionalità, non forniscep -valori o altre quantificazioni di significatività statistica. I modelli scoperti attraverso cPCA devono essere convalidati attraverso test di ipotesi o analisi aggiuntive utilizzando conoscenze di dominio rilevanti. Abbiamo rilasciato il codice per cPCA come un pacchetto python insieme a documentazione ed esempi.

Per i dati targetd -dimensionalixi ∈ Rd e dati di backgroundyi ∈ Rd , sianoC X,C Yle loro corrispondenti matrici di covarianza empirica. SiaR unit d = def v ∈ Rd :v2 = 1 l'insieme dei vettori unitari. Per qualsiasi direzionev ∈ R unit d , la varianza che rappresenta nei dati target e nei dati di background può essere scritta come: Varianza dei dati target :λX ( v ) =defvTCXv , Varianza dei dati di background :λY ( v ) =defvTCYv.Dato un parametro di contrastoα ≥ 0 che quantifica il compromesso tra avere alta varianza target e bassa varianza di background, cPCA calcola la direzione contrastivav * ottimizzando 1 v * = argmax v ∈ Runitd λ X(v)- α λ Y(v).  Questo problema può essere riscritto comev * = argmax v ∈ Runitd v T CX - α CY v ,  il che implica chev * corrisponde al primo autovettore della matriceC = def CX - α CY. Pertanto, le direzioni contrastive possono essere calcolate in modo efficiente utilizzando la decomposizione degli autovalori. Analogamente a PCA, chiamiamo i principali autovettori diC i componenti principali contrastivi (cPCs). Notiamo che i cPCs sono autovettori della matriceC e sono quindi ortogonali tra loro. Per unα fisso, calcoliamo ( [1] ) e restituiamo il sottospazio coperto dai primi pochi (tipicamente due) cPCs.

Il parametro di contrastoα rappresenta il compromesso tra avere alta varianza target e bassa varianza di background. Quandoα = 0, cPCA seleziona le direzioni che massimizzano solo la varianza target, e quindi si riduce a PCA applicato sui dati target {x i}. Man mano cheα aumenta, le direzioni con minore varianza di background diventano più importanti e i cPCs sono spinti verso lo spazio nullo dei dati di background {y i}. Nel caso limiteα = ∞, qualsiasi direzione non nello spazio nullo di {y i} riceve una penalità infinita. In questo caso, cPCA corrisponde a proiettare prima i dati target nello spazio nullo dei dati di background, e poi eseguire PCA sui dati proiettati.

Invece di scegliere un singoloα e restituire il suo sottospazio, cPCA calcola i sottospazi di una lista diα e restituisce alcuni sottospazi che sono lontani l'uno dall'altro in termini di angolo principale. Proiettando i dati su ciascuno di questi sottospazi si riveleranno diverse tendenze all'interno dei dati target, e esaminando visivamente i grafici a dispersione restituiti, l'utente può rapidamente discernere il sottospazio rilevante (e il valore corrispondente diα ) per la sua analisi. Vedi Supplementary Fig. [1] per un esempio dettagliato.

L'algoritmo completo di cPCA è descritto nell'Algoritmo 2 (Supplementary Methods). Tipicamente impostiamo la lista dei valori potenziali diα su 40 valori distribuiti logaritmicamente tra 0.1 e 1000 e questo è usato per tutti gli esperimenti nel documento. Per selezionare i sottospazi rappresentativi, cPCA utilizza il clustering spettrale per raggruppare i sottospazi, dove l'affinità è definita come il prodotto del coseno degli angoli principali tra i sottospazi. Poi i medoid (rappresentativi) di ciascun cluster sono usati come i valori diα per generare i grafici a dispersione visti dall'utente.

La scelta del dataset di background ha una grande influenza sul risultato di cPCA. In generale, i dati di background dovrebbero avere la struttura che vorremmo rimuovere dai dati target. Tale struttura di solito corrisponde a direzioni nel target con alta varianza, ma che non sono di interesse per l'analista.

Forniamo alcuni esempi generali di dataset di background che possono fornire contrasti utili ai dati target: (1) Un gruppo di controllo {y i} contrastato con una popolazione malata {x i} perché il gruppo di controllo contiene variazioni a livello di popolazione simili ma non la variazione sottile dovuta a diversi sottotipi della malattia. (2) I dati al tempo zero {y i} usati per contrastare i dati a un punto temporale successivo {x i}. Questo consente visualizzazioni dei cambiamenti più salienti nel tempo. (3) Un gruppo omogeneo {y i} contrastato con un gruppo misto {x i} perché entrambi hanno variazioni intra-popolazione e rumore di misurazione, ma il primo non ha variazioni inter-popolazione. (4) Un dataset pre-trattamento {y i} contrastato con dati post-trattamento {x i} per rimuovere il rumore di misurazione ma preservare le variazioni causate dal trattamento. (5) Un insieme di registrazioni senza segnale {y i} o immagini che contengono solo rumore, contrastate con misurazioni {x i} che consistono sia di segnale che di rumore.

Vale la pena aggiungere che i dati di background non devono avere esattamente la stessa struttura di covarianza di ciò che vorremmo rimuovere dal dataset target. Ad esempio, nell'esperimento mostrato in Fig. [2] , si scopre che non è necessario utilizzare un dataset di background che consiste di immagini di erba. Infatti, risultati simili si ottengono anche se invece di immagini di erba, vengono utilizzate immagini del cielo come dataset di background. Poiché la struttura delle matrici di covarianza è sufficientemente simile, cPCA rimuove la struttura di background dai dati target. Inoltre, cPCA non richiede che i dati target e i dati di background abbiano un numero simile di campioni. Poiché le matrici di covarianza sono calcolate indipendentemente, cPCA richiede solo che le matrici di covarianza empiriche siano buone stime delle matrici di covarianza della popolazione sottostante, essenzialmente lo stesso requisito di PCA.

Qui, discutiamo l'interpretazione geometrica di cPCA così come le sue proprietà statistiche. Innanzitutto, è interessante considerare quali direzioni sono "migliori" per lo scopo dell'analisi contrastiva. Per una direzionev ∈ R unit d , la sua importanza in cPCA è completamente determinata dalla sua coppia di varianza target–background (λ X(v ),λ Y(v )); è desiderabile avere una varianza target più alta e una varianza di background più bassa. Basandosi su questa intuizione, possiamo ulteriormente definire un ordine parziale di contrastività per varie direzioni: per due direzioniv1 ev2 , potremmo dire chev1 è una direzione contrastiva migliore se ha una varianza target più alta e una varianza di background più bassa. In questo caso, la coppia di varianza target–background div1 si troverebbe sul lato inferiore destro di quella div2 nel grafico delle coppie di varianza target–background (λ X(v ),λ Y(v )), ad esempio, Fig. [5]. Basandosi su questo ordine parziale, il set di direzioni più contrastive può essere definito in modo simile alla definizione del fronte di Pareto. SiaU  il set di coppie di varianza target–background per tutte le direzioni, cioèU = def  ( λX ( v ) ,λY ( v ))  v ∈ Runitd. Il set delle direzioni più contrastive corrisponde al confine inferiore destro diU  nel grafico delle coppie di varianza target–background, come mostrato in Fig. [5]. (Per il caso particolare di matrici di background e target simultaneamente diagonalizzabili, vedi Supplementary Fig. [7].)

Riguardo a cPCA, possiamo dimostrare (vedi Supplementary Note [2] ) che variandoα , il set dei cPC principali è identico al set delle direzioni più contrastive. Inoltre, per la direzionev selezionata da cPCA con il parametro di contrasto impostato suα , la sua coppia di varianza (λ X(v ),λ Y(v )) corrisponde al punto di tangenza del confine inferiore destro diU  con una linea di pendenza 1/α. Di conseguenza, variandoα da zero a infinito, cPCA seleziona direzioni con coppie di varianza che viaggiano dall'estremità inferiore sinistra all'estremità superiore destra del confine inferiore destro diU .

Notiamo anche che riguardo alla casualità dei dati, il tasso di convergenza del campione cPC al cPC della popolazione èO p  dmin ( n,m ) sotto ipotesi deboli, doved è la dimensione en ,m sono le dimensioni dei dati target e di background. Questo tasso è simile al tasso di convergenza standard del campione autovettore per una matrice di covarianza. Vedi Supplementary Note [2].

Abbiamo rilasciato un'implementazione Python di cPCA su GitHub (https://github.com/abidlabs/contrastive ). Il repository GitHub include anche notebook Python e dataset che riproducono la maggior parte delle figure in questo documento e nelle Informazioni Supplementari.

I dataset che sono stati utilizzati per valutare cPCA in questo documento sono disponibili da noi o dagli autori degli studi originali. Si prega di consultare il repository GitHub elencato nella sezione precedente per i dataset che abbiamo rilasciato.

Fig. 1: Panoramica schematica di cPCA. Per eseguire cPCA, calcolare le matrici di covarianza C X, C Ydei dataset target e di sfondo. I vettori singolari della differenza ponderata delle matrici di covarianza, C X− α · C Y, sono le direzioni restituite da cPCA. Come mostrato nel grafico a dispersione a destra, PCA (sui dati target) identifica la direzione che ha la varianza più alta nei dati target, mentre cPCA identifica la direzione che ha una varianza più alta nei dati target rispetto ai dati di sfondo. Proiettare i dati target su quest'ultima direzione fornisce pattern unici per i dati target e spesso rivela strutture che vengono perse da PCA. In particolare, in questo esempio, riducendo la dimensionalità dei dati target con cPCA si rivelerebbero due cluster distinti

Fig. 2: PCA contrastiva su cifre rumorose.a, In alto: Creiamo un dataset target di 5.000 immagini sintetiche sovrapponendo casualmente immagini di cifre scritte a mano 0 e 1 dal dataset MNIST su immagini di erba prese dal dataset ImageNet appartenenti al sinset erba. Le immagini di erba sono convertite in scala di grigi, ridimensionate a 100 × 100, e poi ritagliate casualmente per essere della stessa dimensione delle cifre MNIST, 28 × 28.b, In alto: Qui, tracciamo il risultato dell'incorporamento delle immagini sintetiche sui loro primi due componenti principali usando PCA standard. Vediamo che i punti corrispondenti alle immagini con 0 e immagini con 1 sono difficili da distinguere.a, In basso: Viene quindi introdotto un dataset di sfondo composto esclusivamente da immagini di erba appartenenti allo stesso sinset, ma usiamo immagini diverse da quelle usate per creare il dataset target.b, In basso: Usando cPCA sui dataset target e di sfondo (con un valore del parametro di contrasto α impostato a 2.0), emergono due cluster nella rappresentazione a bassa dimensione del dataset target, uno composto da immagini con la cifra 0 e l'altro da immagini con la cifra 1.cOsserviamo il contributo relativo di ciascun pixel al primo componente principale (PC) e al primo componente principale contrastivo (cPC). I pixel più bianchi sono quelli che portano un peso più positivo, mentre quelli più scuri indicano i pixel che portano pesi negativi. PCA tende a enfatizzare i pixel nella periferia dell'immagine e a de-enfatizzare leggermente i pixel al centro e in basso dell'immagine, indicando che la maggior parte della varianza è dovuta alle caratteristiche di sfondo. D'altra parte, cPCA tende a sovrappesare i pixel che si trovano nella posizione dei numeri 1 scritti a mano, a pesare negativamente i pixel nella posizione dei numeri 0 scritti a mano, e a trascurare la maggior parte degli altri pixel, scoprendo efficacemente quelle caratteristiche utili per discriminare tra le cifre sovrapposte

Fig. 3: Scoperta di sottogruppi nei dati biologici. a Usiamo PCA per proiettare un dataset di espressione proteica di topi con e senza sindrome di Down (DS) sui primi due componenti. La rappresentazione a bassa dimensione delle misurazioni di espressione proteica dai topi con e senza DS appare distribuita in modo simile (in alto). Ma, quando usiamo cPCA per proiettare il dataset sui suoi primi due cPC, scopriamo una rappresentazione a bassa dimensione che raggruppa separatamente i topi con e senza DS (in basso). b Inoltre, usiamo PCA e cPCA per visualizzare un dataset di RNA-Seq a singola cellula ad alta dimensione in due dimensioni. Il dataset consiste di quattro campioni di cellule da due pazienti con leucemia: un campione pre-trapianto dal paziente 1, un campione post-trapianto dal paziente 1, un campione pre-trapianto dal paziente 2, e un campione post-trapianto dal paziente 2. b , sinistra: I risultati usando solo i campioni dal paziente 1, che dimostrano che cPCA (in basso) separa più efficacemente i campioni rispetto a PCA (in alto). Quando i campioni dal secondo paziente sono inclusi, in b , destra, ancora una volta cPCA (in basso) è più efficace di PCA (in alto) nel separare i campioni, sebbene le cellule post-trapianto di entrambi i pazienti siano distribuite in modo simile. Mostriamo i grafici di ciascun campione separatamente nella Fig. Supplementare[5] , dove è più facile vedere la sovrapposizione tra i diversi campioni

Fig. 4: Relazione tra gruppi di ascendenza messicana. a PCA applicata ai dati genetici di individui provenienti da 5 stati messicani non rivela alcun pattern visivamente discernibile nei dati incorporati. b cPCA applicata allo stesso dataset rivela pattern nei dati: individui dallo stesso stato sono raggruppati più vicini nell'incorporamento cPCA. c Inoltre, la distribuzione dei punti rivela relazioni tra i gruppi che corrispondono alla posizione geografica dei diversi stati: per esempio, individui da stati geograficamente adiacenti sono adiacenti nell'incorporamento. c Adattato da una mappa del Messico che è originariamente opera di User:Allstrak su Wikipedia, pubblicata sotto una licenza CC-BY-SA, proveniente da https://commons.wikimedia.org/wiki/File:Mexico_Map.svg

Fig. 5: Interpretazione geometrica di cPCA. L'insieme delle coppie di varianza target-sfondoUè tracciato come la regione verde acqua per alcuni dati target e di sfondo generati casualmente. Il confine in basso a destra, colorato in oro, corrisponde all'insieme delle direzioni più contrastiveSλ. I triangoli blu sono le coppie di varianza per i cPC selezionati con valori di α 0.92 e 0.29 rispettivamente. Notiamo che corrispondono ai punti di tangenza della curva dorata e delle linee tangenti con pendenza1α= 1.08, 3.37, rispettivamente

