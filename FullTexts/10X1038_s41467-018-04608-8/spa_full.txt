Explorando patrones enriquecidos en un conjunto de datos con análisis de componentes principales contrastivo

Resumen: La visualización y exploración de datos de alta dimensión es un desafío ubicuo en todas las disciplinas. Las técnicas ampliamente utilizadas, como el análisis de componentes principales (PCA), tienen como objetivo identificar tendencias dominantes en un conjunto de datos. Sin embargo, en muchos contextos, tenemos conjuntos de datos recopilados bajo diferentes condiciones, por ejemplo, un tratamiento y un experimento de control, y estamos interesados en visualizar y explorar patrones que son específicos de un conjunto de datos. Este artículo propone un método, el análisis de componentes principales contrastivo (cPCA), que identifica estructuras de baja dimensión que están enriquecidas en un conjunto de datos en relación con los datos de comparación. En una amplia variedad de experimentos, demostramos que cPCA con un conjunto de datos de fondo nos permite visualizar patrones específicos del conjunto de datos que se pierden con PCA y otros métodos estándar. Además, proporcionamos una interpretación geométrica de cPCA y fuertes garantías matemáticas. Una implementación de cPCA está disponible públicamente y se puede utilizar para el análisis exploratorio de datos en muchas aplicaciones donde actualmente se utiliza PCA.

El análisis de componentes principales (PCA) es uno de los métodos más utilizados para la exploración y visualización de datos. PCA proyecta los datos en un espacio de baja dimensión y es especialmente poderoso como un enfoque para visualizar patrones, como agrupaciones, clines y valores atípicos en un conjunto de datos. Hay un gran número de métodos de visualización relacionados; por ejemplo, t-SNE y el escalado multidimensional (MDS) permiten proyecciones de datos no lineales y pueden capturar mejor patrones no lineales que PCA. Sin embargo, todos estos métodos están diseñados para explorar un conjunto de datos a la vez. Cuando el analista tiene múltiples conjuntos de datos (o múltiples condiciones en un conjunto de datos para comparar), el estado actual de la práctica es realizar PCA (o t-SNE, MDS, etc.) en cada conjunto de datos por separado, y luego comparar manualmente las diversas proyecciones para explorar si hay similitudes y diferencias interesantes entre los conjuntos de datos,. El PCA contrastivo (cPCA) está diseñado para llenar este vacío en la exploración y visualización de datos al identificar automáticamente las proyecciones que exhiben las diferencias más interesantes entre conjuntos de datos. La Figura[1]proporciona una visión general de cPCA que explicamos con más detalle a continuación.

cPCA está motivado por una amplia gama de problemas en diversas disciplinas. Para ilustrar, mencionamos dos de esos problemas aquí y demostramos otros a través de experimentos más adelante en el documento. Primero, considere un conjunto de datos de mediciones de expresión génica de individuos de diferentes etnias y sexos. Estos datos incluyen niveles de expresión génica de pacientes con cáncer {x i}, que estamos interesados en analizar. También tenemos datos de control, que corresponden a los niveles de expresión génica de pacientes sanos {y i} de un trasfondo demográfico similar. Nuestro objetivo es encontrar tendencias y variaciones dentro de los pacientes con cáncer (por ejemplo, para identificar subtipos moleculares de cáncer). Si aplicamos directamente PCA a {x i}, sin embargo, los componentes principales superiores pueden corresponder a las variaciones demográficas de los individuos en lugar de los subtipos de cáncer porque las variaciones genéticas debidas a los primeros probablemente sean mayores que las de los segundos. Abordamos este problema al notar que los pacientes sanos también contienen la variación asociada con las diferencias demográficas, pero no la variación correspondiente a los subtipos de cáncer. Por lo tanto, podemos buscar componentes en los que {x i} tenga alta varianza pero {y i} tenga baja varianza.

Como ejemplo relacionado, considere un conjunto de datos {x i} que consiste en dígitos escritos a mano sobre un fondo complejo, como diferentes imágenes de césped (ver Fig.[2(a), parte superior]). El objetivo de una tarea típica de aprendizaje no supervisado puede ser agrupar los datos, revelando los diferentes dígitos en la imagen. Sin embargo, si aplicamos PCA estándar a estas imágenes, encontramos que los componentes principales superiores no representan características relacionadas con los dígitos escritos a mano, sino que reflejan la variación dominante en las características relacionadas con el fondo de la imagen (Fig.[2(b)], parte superior). Mostramos que es posible corregir esto utilizando un conjunto de datos de referencia {y i} que consiste únicamente en imágenes del césped (no necesariamente las mismas imágenes utilizadas en {x i} pero con covarianza similar entre características, como se muestra en Fig.[2(a)], parte inferior), y buscando el subespacio de mayor varianza en {x i} en comparación con {y i}. Al proyectar en este subespacio, podemos separar visualmente las imágenes basándonos en el valor del dígito escrito a mano (Fig. 2(b), parte inferior). Al comparar los componentes principales descubiertos por PCA con los descubiertos por cPCA, vemos que cPCA identifica características más relevantes (Fig.[2(c)]), lo que nos permite usar cPCA para aplicaciones como la selección de características y la eliminación de ruido.

El PCA contrastivo es una herramienta para el aprendizaje no supervisado, que reduce eficientemente la dimensionalidad para permitir la visualización y el análisis exploratorio de datos. Esto separa cPCA de una gran clase de métodos de aprendizaje supervisado cuyo objetivo principal es clasificar o discriminar entre varios conjuntos de datos, como el análisis discriminante lineal (LDA), el análisis discriminante cuadrático (QDA), el PCA supervisado y QUADRO. Esto también distingue cPCA de métodos que integran múltiples conjuntos de datos, con el objetivo de identificar patrones correlacionados entre dos o más conjuntos de datos, en lugar de aquellos únicos para cada conjunto de datos individual. También hay una rica familia de métodos no supervisados para la reducción de dimensiones además de PCA. Por ejemplo, el escalado multidimensional (MDS) encuentra una incrustación de baja dimensión que preserva la distancia en el espacio de alta dimensión; la búsqueda de componentes principales encuentra un subespacio de bajo rango que es robusto al ruido pequeño en las entradas y a errores dispersos grandes. Pero ninguno está diseñado para utilizar información relevante de un segundo conjunto de datos, como lo hace cPCA. En el suplemento, hemos comparado cPCA con muchas de las técnicas mencionadas anteriormente en conjuntos de datos representativos (ver Figuras Suplementarias[3]y[4]), , ,.

En un dominio de aplicación específico, puede haber herramientas especializadas en ese dominio con objetivos similares a cPCA, ,. Por ejemplo, en los resultados, mostramos cómo cPCA aplicado a datos de genotipo visualiza la ascendencia geográfica dentro de México. Explorar agrupaciones detalladas de ascendencias genéticas es un problema importante en la genética de poblaciones, y los investigadores han desarrollado recientemente un algoritmo para visualizar específicamente tales agrupaciones de ascendencia. Aunque cPCA funciona bien aquí, el algoritmo diseñado por expertos podría funcionar aún mejor para un conjunto de datos específico. Sin embargo, el algoritmo especializado requiere un conocimiento sustancial del dominio para diseñarlo, es más costoso computacionalmente y puede ser difícil de usar. El objetivo de cPCA no es reemplazar todos estos métodos especializados de última generación en cada uno de sus dominios, sino proporcionar un método general para explorar conjuntos de datos arbitrarios.

Proponemos un algoritmo concreto y eficiente para cPCA en este documento. El método toma como entrada un conjunto de datos objetivo {x i} que estamos interesados en visualizar o identificar patrones dentro. Como entrada secundaria, cPCA toma un conjunto de datos de fondo {y i}, que no contiene los patrones de interés. El algoritmo cPCA devuelve subespacios que capturan una gran cantidad de variación en los datos objetivo {x i}, pero poca en el fondo {y i} (ver Fig.[1], Métodos, y Métodos Suplementarios para más detalles). Este subespacio corresponde a características que contienen estructura específica de {x i}. Por lo tanto, cuando los datos objetivo se proyectan en este subespacio, podemos visualizar y descubrir la estructura adicional en los datos objetivo en relación con el fondo. Análogo a los componentes principales (PCs), llamamos a las direcciones encontradas por cPCA los componentes principales contrastivos (cPCs). Enfatizamos que cPCA es fundamentalmente una técnica no supervisada, diseñada para resolver patrones en un conjunto de datos más claramente utilizando el conjunto de datos de fondo como contraste. En particular, cPCA no busca discriminar entre los conjuntos de datos objetivo y de fondo; el subespacio que contiene tendencias que están enriquecidas en el conjunto de datos objetivo no es necesariamente el mismo subespacio que es óptimo para la clasificación entre los conjuntos de datos.

Los investigadores han notado que el PCA estándar a menudo es ineficaz para descubrir subgrupos dentro de los datos biológicos, al menos en parte porque “los componentes principales dominantes…se correlacionan con artefactos,” en lugar de con características que son de interés para el investigador. ¿Cómo puede usarse cPCA en estos entornos para detectar los subgrupos más significativos? Al usar un conjunto de datos de fondo para cancelar la variación universal pero poco interesante en el objetivo, podemos buscar estructura que sea única para el conjunto de datos objetivo.

Nuestro primer experimento utiliza un conjunto de datos que consiste en mediciones de expresión de proteínas de ratones que han recibido terapia de choque,. Algunos de los ratones han desarrollado Síndrome de Down (DS). Para crear una tarea de aprendizaje no supervisado donde tenemos información de verdad de terreno para evaluar los métodos, asumimos que esta información de DS no es conocida por el analista y solo la usamos para la evaluación del algoritmo. Nos gustaría ver si detectamos alguna diferencia significativa dentro de la población de ratones sometidos a choque de manera no supervisada (la presencia o ausencia de Síndrome de Down siendo un ejemplo clave). En Fig. [3a] (parte superior), mostramos el resultado de aplicar PCA al conjunto de datos objetivo: los datos transformados no revelan ninguna agrupación significativa dentro de la población de ratones. Las principales fuentes de variación dentro de los ratones pueden ser naturales, como el sexo o la edad.

Aplicamos cPCA a este conjunto de datos utilizando un fondo que consiste en mediciones de expresión de proteínas de un conjunto de ratones que no han sido expuestos a la terapia de choque. Son ratones de control que probablemente tienen una variación natural similar a la de los ratones experimentales, pero sin las diferencias que resultan de la terapia de choque. Con este conjunto de datos como fondo, cPCA es capaz de resolver dos grupos diferentes en los datos objetivo transformados, uno correspondiente a ratones que no tienen Síndrome de Down y otro correspondiente (principalmente) a ratones que tienen Síndrome de Down, como se ilustra en Fig. [3a] (parte inferior). Como comparación, también aplicamos otras 8 técnicas de reducción de dimensionalidad para identificar direcciones que diferencian entre los conjuntos de datos objetivo y de fondo, ninguna de las cuales pudo separar los ratones tan bien como cPCA (ver Fig. Suplementaria [4] para más detalles).

A continuación, analizamos un conjunto de datos público de mayor dimensión que consiste en niveles de expresión de ARN de célula única de una mezcla de células mononucleares de médula ósea (BMMCs) tomadas de un paciente con leucemia antes del trasplante de células madre y BMMCs del mismo paciente después del trasplante de células madre. Todos los datos de ARN-Seq de célula única se preprocesan utilizando métodos similares a los descritos por los autores. En particular, antes de aplicar PCA o cPCA, todos los conjuntos de datos se reducen a 500 genes, que se seleccionan sobre la base de la mayor dispersión [varianza dividida por la media] dentro de los datos objetivo. Nuevamente, realizamos PCA para ver si podemos descubrir visualmente las dos muestras en los datos transformados. Como se muestra en Fig. [3b] (parte superior izquierda), ambos tipos de células siguen una distribución similar en el espacio abarcado por los dos primeros PCs. Esto probablemente se deba a que las diferencias entre las muestras son pequeñas y los PCs en su lugar reflejan la heterogeneidad de varios tipos de células dentro de cada muestra o incluso variaciones en las condiciones experimentales, que pueden tener un efecto significativo en las mediciones de ARN-Seq de célula única.

Aplicamos cPCA utilizando un conjunto de datos de fondo que consiste en mediciones de RNA-Seq de las células BMMC de un individuo sano. Esperamos que este conjunto de datos de fondo contenga la variación debida a la población heterogénea de células, así como variaciones en las condiciones experimentales. Podemos esperar, entonces, que cPCA pueda recuperar direcciones que estén enriquecidas en los datos objetivo, correspondientes a diferencias pre y post-trasplante. De hecho, eso es lo que encontramos, como se muestra en la Fig. [3b] (abajo a la izquierda).

Además, aumentamos nuestro conjunto de datos objetivo con muestras de BMMC de un segundo paciente con leucemia, nuevamente antes y después del trasplante de células madre. Así, hay un total de cuatro subpoblaciones de células. La aplicación de PCA en estos datos muestra que las cuatro subpoblaciones no son separables en el subespacio abarcado por los dos principales componentes principales (PCs), como se muestra en la Fig. [3b] (arriba a la derecha). Sin embargo, cuando se aplica cPCA con el mismo conjunto de datos de fondo, al menos tres de las subpoblaciones muestran una separación mucho más fuerte, como se muestra en la Fig. [3b] (abajo a la derecha). La incrustación de cPCA también sugiere que las muestras de células de ambos pacientes son más similares entre sí después del trasplante de células madre (puntos cian y verdes) que antes del trasplante (puntos dorados y rosados), una hipótesis razonable que puede ser probada por el investigador. Se puede consultar la Fig. Suplementaria [5] para más detalles de este experimento. Vemos que cPCA puede ser una herramienta útil para inferir la relación entre subpoblaciones, un tema que exploramos más adelante.

En ejemplos anteriores, hemos visto que cPCA permite al usuario descubrir subclases dentro de un conjunto de datos objetivo que no están etiquetadas a priori. Sin embargo, incluso cuando las subclases se conocen de antemano, la reducción de dimensionalidad puede ser una forma útil de visualizar la relación dentro de los grupos. Por ejemplo, PCA se utiliza a menudo para visualizar la relación entre poblaciones étnicas basadas en variantes genéticas, porque proyectar las variantes genéticas en dos dimensiones a menudo produce mapas que ofrecen visualizaciones impactantes de tendencias geográficas e históricas,. Pero nuevamente, PCA está limitado a identificar la estructura más dominante; cuando esto representa una variación universal o poco interesante, cPCA puede ser más efectivo para visualizar tendencias.

El conjunto de datos que usamos para este ejemplo consiste en polimorfismos de nucleótido único (SNPs) de los genomas de individuos de cinco estados en México, recopilados en un estudio previo. La ascendencia mexicana es difícil de analizar usando PCA ya que los PCs generalmente no reflejan el origen geográfico dentro de México; en cambio, reflejan la proporción de herencia europea/nativa americana de cada individuo mexicano, lo que domina y oscurece las diferencias debidas al origen geográfico dentro de México (ver Fig. [4a] ). Para superar este problema, los genetistas de poblaciones podan manualmente los SNPs, eliminando aquellos que se sabe que derivan de la ascendencia europea, antes de aplicar PCA. Sin embargo, este procedimiento tiene una aplicabilidad limitada ya que requiere conocer el origen de los SNPs y que la fuente de variación de fondo sea muy diferente de la variación de interés, lo cual a menudo no es el caso.

Como alternativa, utilizamos cPCA con un conjunto de datos de fondo que consiste en individuos de México y de Europa. Este fondo está dominado por la variación nativa americana/europea, lo que nos permite aislar la variación intra-mexicana en el conjunto de datos objetivo. Los resultados de aplicar cPCA se muestran en la Fig. [4b]. Encontramos que los individuos del mismo estado en México están incrustados más cerca entre sí. Además, los dos grupos que son los más divergentes son los sonorenses y los mayas de Yucatán, que también son los más distantes geográficamente dentro de México, mientras que los mexicanos de los otros tres estados están cerca entre sí, tanto geográficamente como en la incrustación capturada por cPCA (ver Fig. [4c] ). Ver también la Fig. Suplementaria [6] para más detalles.

En muchos entornos de ciencia de datos, estamos interesados en visualizar y explorar patrones que están enriquecidos en un conjunto de datos en relación con otros datos. Hemos presentado cPCA como una herramienta general para realizar tal exploración contrastiva, y hemos ilustrado su utilidad en una amplia gama de aplicaciones. Las principales ventajas de cPCA son su generalidad y facilidad de uso. Calcular un cPCA particular toma esencialmente el mismo tiempo que calcular un PCA regular. Esta eficiencia computacional permite que cPCA sea útil para la exploración interactiva de datos, donde cada operación debería ser idealmente casi inmediata. Como tal, en cualquier configuración donde se aplique PCA en conjuntos de datos relacionados, también se puede aplicar cPCA. En la Nota Suplementaria[3]y la Fig. Suplementaria[8], mostramos cómo cPCA puede ser kernelizado para descubrir patrones contrastivos no lineales en conjuntos de datos.

El único parámetro libre de cPCA contrastivo es la fuerza de contrasteα. En nuestro algoritmo predeterminado, desarrollamos un esquema automático basado en agrupaciones de subespacios para seleccionar los valores más informativos deα (ver Métodos). Todos los experimentos realizados para este artículo utilizan los valores deα generados automáticamente, y creemos que este valor predeterminado será suficiente en muchas aplicaciones de cPCA. El usuario también puede ingresar valores específicos paraα si se desea una exploración más detallada.

cPCA, al igual que PCA regular y otros métodos de reducción de dimensionalidad, no proporciona valoresp ni otras cuantificaciones de significancia estadística. Los patrones descubiertos a través de cPCA deben ser validados mediante pruebas de hipótesis o análisis adicional utilizando el conocimiento relevante del dominio. Hemos lanzado el código para cPCA como un paquete de Python junto con documentación y ejemplos.

Para los datos objetivo ded dimensionesxi ∈ Rd y los datos de fondoyi ∈ Rd , seanC X,C Ysus matrices de covarianza empíricas correspondientes. SeaR unit d = def v ∈ Rd :v2 = 1 el conjunto de vectores unitarios. Para cualquier direcciónv ∈ R unit d , la varianza que representa en los datos objetivo y en los datos de fondo se puede escribir como: Varianza de datos objetivo :λX ( v ) =defvTCXv , Varianza de datos de fondo :λY ( v ) =defvTCYv.Dado un parámetro de contrasteα ≥ 0 que cuantifica el equilibrio entre tener alta varianza objetivo y baja varianza de fondo, cPCA calcula la dirección contrastivav * optimizando 1 v * = argmax v ∈ Runitd λ X(v)- α λ Y(v).  Este problema se puede reescribir comov * = argmax v ∈ Runitd v T CX - α CY v ,  lo que implica quev * corresponde al primer vector propio de la matrizC = def CX - α CY. Por lo tanto, las direcciones contrastivas se pueden calcular eficientemente usando la descomposición de valores propios. Análogo a PCA, llamamos a los vectores propios principales deC los componentes principales contrastivos (cPCs). Notamos que los cPCs son vectores propios de la matrizC y, por lo tanto, son ortogonales entre sí. Para unα fijo, calculamos ( [1] ) y devolvemos el subespacio abarcado por los primeros pocos (típicamente dos) cPCs.

El parámetro de contrasteα representa el equilibrio entre tener alta varianza objetivo y baja varianza de fondo. Cuandoα = 0, cPCA selecciona las direcciones que solo maximizan la varianza objetivo, y por lo tanto se reduce a PCA aplicado en los datos objetivo {x i}. A medida queα aumenta, las direcciones con menor varianza de fondo se vuelven más importantes y los cPCs se dirigen hacia el espacio nulo de los datos de fondo {y i}. En el caso límiteα = ∞, cualquier dirección que no esté en el espacio nulo de {y i} recibe una penalización infinita. En este caso, cPCA corresponde a proyectar primero los datos objetivo en el espacio nulo de los datos de fondo, y luego realizar PCA en los datos proyectados.

En lugar de elegir un únicoα y devolver su subespacio, cPCA calcula los subespacios de una lista deα ’s y devuelve algunos subespacios que están alejados entre sí en términos del ángulo principal. Proyectar los datos en cada uno de estos subespacios revelará diferentes tendencias dentro de los datos objetivo, y al examinar visualmente los diagramas de dispersión que se devuelven, el usuario puede discernir rápidamente el subespacio relevante (y el valor correspondiente deα ) para su análisis. Consulte la Fig. [1] suplementaria para un ejemplo detallado.

El algoritmo completo de cPCA se describe en el Algoritmo 2 (Métodos Suplementarios). Normalmente establecemos la lista de valores potenciales deα en 40 valores espaciados logarítmicamente entre 0.1 y 1000 y esto se utiliza para todos los experimentos en el documento. Para seleccionar los subespacios representativos, cPCA utiliza agrupamiento espectral para agrupar los subespacios, donde la afinidad se define como el producto del coseno de los ángulos principales entre los subespacios. Luego, los medoides (representantes) de cada grupo se utilizan como los valores deα para generar los diagramas de dispersión vistos por el usuario.

La elección del conjunto de datos de fondo tiene una gran influencia en el resultado de cPCA. En general, los datos de fondo deben tener la estructura que nos gustaría eliminar de los datos objetivo. Dicha estructura generalmente corresponde a direcciones en el objetivo con alta varianza, pero que no son de interés para el analista.

Proporcionamos algunos ejemplos generales de conjuntos de datos de fondo que pueden proporcionar contrastes útiles a los datos objetivo: (1) Un grupo de control {y i} contrastado con una población enferma {x i} porque el grupo de control contiene variación a nivel poblacional similar pero no la variación sutil debida a diferentes subtipos de la enfermedad. (2) Los datos en el tiempo cero {y i} utilizados para contrastar con datos en un punto de tiempo posterior {x i}. Esto permite visualizaciones de los cambios más destacados a lo largo del tiempo. (3) Un grupo homogéneo {y i} contrastado con un grupo mixto {x i} porque ambos tienen variación intra-poblacional y ruido de medición, pero el primero no tiene variación inter-poblacional. (4) Un conjunto de datos pretratamiento {y i} contrastado con datos postratamiento {x i} para eliminar el ruido de medición pero preservar las variaciones causadas por el tratamiento. (5) Un conjunto de grabaciones sin señal {y i} o imágenes que contienen solo ruido, contrastadas con mediciones {x i} que consisten en señal y ruido.

Vale la pena agregar que los datos de fondo no necesitan tener exactamente la misma estructura de covarianza que lo que nos gustaría eliminar del conjunto de datos objetivo. Como ejemplo, en el experimento mostrado en la Fig. [2] , resulta que no necesitamos usar un conjunto de datos de fondo que consista en imágenes de césped. De hecho, se obtienen resultados similares incluso si en lugar de imágenes de césped, se utilizan imágenes del cielo como conjunto de datos de fondo. Como la estructura de las matrices de covarianza es lo suficientemente similar, cPCA elimina la estructura de fondo de los datos objetivo. Además, cPCA no requiere que los datos objetivo y los datos de fondo tengan un número similar de muestras. Dado que las matrices de covarianza se calculan de forma independiente, cPCA solo requiere que las matrices de covarianza empíricas sean buenas estimaciones de las matrices de covarianza de la población subyacente, esencialmente el mismo requisito que PCA.

Aquí, discutimos la interpretación geométrica de cPCA así como sus propiedades estadísticas. Primero, es interesante considerar qué direcciones son "mejores" para el propósito del análisis contrastivo. Para una direcciónv ∈ R unit d , su importancia en cPCA está completamente determinada por su par de varianza objetivo-fondo (λ X(v ),λ Y(v )); es deseable tener una mayor varianza objetivo y una menor varianza de fondo. Basado en esta intuición, podemos definir además un orden parcial de contrastividad para varias direcciones: para dos direccionesv1 yv2 , podríamos decir quev1 es una mejor dirección contrastiva si tiene una mayor varianza objetivo y una menor varianza de fondo. En este caso, el par de varianza objetivo-fondo dev1 se encontraría en el lado inferior derecho de la dev2 en el gráfico de pares de varianza objetivo-fondo (λ X(v ),λ Y(v )), por ejemplo, Fig. [5]. Basado en este orden parcial, el conjunto de direcciones más contrastivas se puede definir de manera similar a la definición de la frontera de Pareto. SeaU  el conjunto de pares de varianza objetivo-fondo para todas las direcciones, es decir,U = def  ( λX ( v ) ,λY ( v ))v ∈Runitd. El conjunto de direcciones más contrastivas corresponde al límite inferior derecho deU en el gráfico de pares de varianza objetivo-fondo, como se muestra en la Fig.[5]. (Para el caso particular de matrices de fondo y objetivo diagonalizables simultáneamente, consulte la Fig.[7]suplementaria.)

Con respecto a cPCA, podemos probar (ver Nota Suplementaria [2] ) que al variarα , el conjunto de cPC’s principales es idéntico al conjunto de direcciones más contrastivas. Además, para la direcciónv seleccionada por cPCA con el parámetro de contraste establecido enα , su par de varianza (λ X(v ),λ Y(v )) corresponde al punto de tangencia del límite inferior derecho deU  con una línea de pendiente-1/α. Como resultado, al variarα de cero a infinito, cPCA selecciona direcciones con pares de varianza que viajan desde el extremo inferior izquierdo hasta el extremo superior derecho del límite inferior derecho deU .

También observamos que con respecto a la aleatoriedad de los datos, la tasa de convergencia de la muestra cPC a la población cPC esO p  dmin ( n,m ) bajo suposiciones leves, donded es la dimensión yn ,m son los tamaños de los datos objetivo y de fondo. Esta tasa es similar a la tasa de convergencia estándar del vector propio de muestra para una matriz de covarianza. Ver Nota Suplementaria [2].

Hemos lanzado una implementación de cPCA en Python en GitHub (https://github.com/abidlabs/contrastive ). El repositorio de GitHub también incluye cuadernos de Python y conjuntos de datos que reproducen la mayoría de las figuras en este documento y en la Información Suplementaria.

Los conjuntos de datos que se han utilizado para evaluar cPCA en este documento están disponibles de nosotros o de los autores de los estudios originales. Consulte el repositorio de GitHub mencionado en la sección anterior para los conjuntos de datos que hemos lanzado.

Fig. 1: Visión esquemática de cPCA. Para realizar cPCA, calcule las matrices de covarianza C X, C Yde los conjuntos de datos objetivo y de fondo. Los vectores singulares de la diferencia ponderada de las matrices de covarianza, C X− α · C Y, son las direcciones devueltas por cPCA. Como se muestra en el diagrama de dispersión a la derecha, PCA (en los datos objetivo) identifica la dirección que tiene la mayor varianza en los datos objetivo, mientras que cPCA identifica la dirección que tiene una mayor varianza en los datos objetivo en comparación con los datos de fondo. Proyectar los datos objetivo en la última dirección da patrones únicos a los datos objetivo y a menudo revela estructuras que PCA pasa por alto. Específicamente, en este ejemplo, reducir la dimensionalidad de los datos objetivo mediante cPCA revelaría dos grupos distintos

Fig. 2: PCA contrastiva en dígitos ruidosos.a, Arriba: Creamos un conjunto de datos objetivo de 5,000 imágenes sintéticas superponiendo aleatoriamente imágenes de dígitos escritos a mano 0 y 1 del conjunto de datos MNIST sobre imágenes de hierba tomadas del conjunto de datos ImageNet pertenecientes al sinset hierba. Las imágenes de hierba se convierten a escala de grises, se redimensionan a 100 × 100, y luego se recortan aleatoriamente para tener el mismo tamaño que los dígitos MNIST, 28 × 28.b, Arriba: Aquí, graficamos el resultado de incrustar las imágenes sintéticas en sus dos primeros componentes principales usando PCA estándar. Vemos que los puntos correspondientes a las imágenes con 0's y las imágenes con 1's son difíciles de distinguir.a, Abajo: Luego se introduce un conjunto de datos de fondo que consiste únicamente en imágenes de hierba pertenecientes al mismo sinset, pero usamos imágenes que son diferentes a las utilizadas para crear el conjunto de datos objetivo.b, Abajo: Usando cPCA en los conjuntos de datos objetivo y de fondo (con un valor del parámetro de contraste α establecido en 2.0), emergen dos grupos en la representación de menor dimensión del conjunto de datos objetivo, uno compuesto por imágenes con el dígito 0 y el otro de imágenes con el dígito 1.cObservamos la contribución relativa de cada píxel al primer componente principal (PC) y al primer componente principal contrastivo (cPC). Los píxeles más blancos son aquellos que tienen un peso más positivo, mientras que los más oscuros denotan aquellos píxeles que tienen pesos negativos. PCA tiende a enfatizar los píxeles en la periferia de la imagen y a desestimar ligeramente los píxeles en el centro y la parte inferior de la imagen, indicando que la mayor parte de la varianza se debe a características de fondo. Por otro lado, cPCA tiende a aumentar el peso de los píxeles que están en la ubicación de los 1's escritos a mano, a dar peso negativo a los píxeles en la ubicación de los 0's escritos a mano, y a descuidar la mayoría de los otros píxeles, descubriendo efectivamente aquellas características útiles para discriminar entre los dígitos superpuestos

Fig. 3: Descubrimiento de subgrupos en datos biológicos. a Usamos PCA para proyectar un conjunto de datos de expresión de proteínas de ratones con y sin Síndrome de Down (DS) en los dos primeros componentes. La representación de menor dimensión de las mediciones de expresión de proteínas de ratones con y sin DS se distribuyen de manera similar (arriba). Pero, cuando usamos cPCA para proyectar el conjunto de datos en sus dos primeros cPCs, descubrimos una representación de menor dimensión que agrupa a los ratones con y sin DS por separado (abajo). b Además, usamos PCA y cPCA para visualizar un conjunto de datos de RNA-Seq de célula única de alta dimensión en dos dimensiones. El conjunto de datos consiste en cuatro muestras de células de dos pacientes con leucemia: una muestra pre-trasplante del paciente 1, una muestra post-trasplante del paciente 1, una muestra pre-trasplante del paciente 2, y una muestra post-trasplante del paciente 2. b , izquierda: Los resultados usando solo las muestras del paciente 1, que demuestran que cPCA (abajo) separa más efectivamente las muestras que PCA (arriba). Cuando se incluyen las muestras del segundo paciente, en b , derecha, nuevamente cPCA (abajo) es más efectivo que PCA (arriba) para separar las muestras, aunque las células post-trasplante de ambos pacientes están distribuidas de manera similar. Mostramos gráficos de cada muestra por separado en la Fig. Suplementaria[5] , donde es más fácil ver la superposición entre diferentes muestras

Fig. 4: Relación entre grupos de ascendencia mexicana. a PCA aplicada a datos genéticos de individuos de 5 estados mexicanos no revela ningún patrón visualmente discernible en los datos incrustados. b cPCA aplicada al mismo conjunto de datos revela patrones en los datos: los individuos del mismo estado están agrupados más cerca en la incrustación de cPCA. c Además, la distribución de los puntos revela relaciones entre los grupos que coinciden con la ubicación geográfica de los diferentes estados: por ejemplo, los individuos de estados geográficamente adyacentes están adyacentes en la incrustación. c Adaptado de un mapa de México que es originalmente obra de Usuario:Allstrak en Wikipedia, publicado bajo una licencia CC-BY-SA, obtenido de https://commons.wikimedia.org/wiki/File:Mexico_Map.svg

Fig. 5: Interpretación geométrica de cPCA. El conjunto de pares de varianza objetivo-fondoUse traza como la región verde azulado para algunos datos objetivo y de fondo generados aleatoriamente. El límite inferior derecho, coloreado en oro, corresponde al conjunto de direcciones más contrastantesSλ. Los triángulos azules son los pares de varianza para los cPCs seleccionados con valores de α 0.92 y 0.29 respectivamente. Notamos que corresponden a los puntos de tangencia de la curva dorada y las líneas tangentes con pendiente1α= 1.08, 3.37, respectivamente

