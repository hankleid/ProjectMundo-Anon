Exploration des motifs enrichis dans un ensemble de données avec l'analyse en composantes principales contrastive

Résumé: La visualisation et l'exploration de données de haute dimension sont un défi omniprésent à travers les disciplines. Des techniques largement utilisées telles que l'analyse en composantes principales (PCA) visent à identifier les tendances dominantes dans un ensemble de données. Cependant, dans de nombreux contextes, nous avons des ensembles de données collectés dans des conditions différentes, par exemple, un traitement et une expérience de contrôle, et nous sommes intéressés par la visualisation et l'exploration de motifs spécifiques à un ensemble de données. Cet article propose une méthode, l'analyse en composantes principales contrastive (cPCA), qui identifie des structures de basse dimension enrichies dans un ensemble de données par rapport aux données de comparaison. Dans une grande variété d'expériences, nous démontrons que la cPCA avec un ensemble de données de fond nous permet de visualiser des motifs spécifiques à l'ensemble de données manqués par la PCA et d'autres méthodes standard. Nous fournissons en outre une interprétation géométrique de la cPCA et de solides garanties mathématiques. Une implémentation de la cPCA est disponible publiquement et peut être utilisée pour l'analyse exploratoire de données dans de nombreuses applications où la PCA est actuellement utilisée.

L'analyse en composantes principales (ACP) est l'une des méthodes les plus largement utilisées pour l'exploration et la visualisation des données. L'ACP projette les données dans un espace de faible dimension et est particulièrement puissante comme approche pour visualiser des motifs, tels que des clusters, des clines et des valeurs aberrantes dans un ensemble de données. Il existe un grand nombre de méthodes de visualisation connexes ; par exemple, t-SNE et le scaling multidimensionnel (MDS) permettent des projections de données non linéaires et peuvent mieux capturer les motifs non linéaires que l'ACP. Pourtant, toutes ces méthodes sont conçues pour explorer un ensemble de données à la fois. Lorsque l'analyste dispose de plusieurs ensembles de données (ou de plusieurs conditions dans un ensemble de données à comparer), l'état actuel de la pratique consiste à effectuer une ACP (ou t-SNE, MDS, etc.) sur chaque ensemble de données séparément, puis à comparer manuellement les différentes projections pour explorer s'il existe des similitudes et des différences intéressantes entre les ensembles de données,. L'ACP contrastive (cPCA) est conçue pour combler cette lacune dans l'exploration et la visualisation des données en identifiant automatiquement les projections qui présentent les différences les plus intéressantes entre les ensembles de données. La Figure[1]fournit un aperçu de la cPCA que nous expliquons plus en détail ci-après.

La cPCA est motivée par un large éventail de problèmes à travers les disciplines. Pour illustration, nous mentionnons ici deux de ces problèmes et en démontrons d'autres à travers des expériences plus tard dans l'article. Tout d'abord, considérons un ensemble de données de mesures d'expression génique provenant d'individus de différentes ethnies et sexes. Ces données incluent les niveaux d'expression génique de patients atteints de cancer {x i}, que nous souhaitons analyser. Nous avons également des données de contrôle, qui correspondent aux niveaux d'expression génique de patients en bonne santé {y i} provenant d'un contexte démographique similaire. Notre objectif est de trouver des tendances et des variations chez les patients atteints de cancer (par exemple, pour identifier des sous-types moléculaires de cancer). Si nous appliquons directement l'ACP à {x i}, cependant, les principales composantes principales peuvent correspondre aux variations démographiques des individus au lieu des sous-types de cancers, car les variations génétiques dues aux premières sont probablement plus importantes que celles des secondes. Nous abordons ce problème en notant que les patients en bonne santé contiennent également la variation associée aux différences démographiques, mais pas la variation correspondant aux sous-types de cancers. Ainsi, nous pouvons rechercher des composantes dans lesquelles {x i} a une variance élevée mais {y i} a une faible variance.

Comme exemple connexe, considérons un ensemble de données {x i} qui consiste en des chiffres manuscrits sur un fond complexe, tel que différentes images d'herbe (voir Fig.[2(a), haut]). L'objectif d'une tâche typique d'apprentissage non supervisé peut être de regrouper les données, révélant les différents chiffres dans l'image. Cependant, si nous appliquons l'ACP standard sur ces images, nous constatons que les principales composantes principales ne représentent pas les caractéristiques liées aux chiffres manuscrits, mais reflètent la variation dominante des caractéristiques liées au fond de l'image (Fig.[2(b)], haut). Nous montrons qu'il est possible de corriger cela en utilisant un ensemble de données de référence {y i} qui consiste uniquement en des images de l'herbe (pas nécessairement les mêmes images utilisées dans {x i} mais ayant une covariance similaire entre les caractéristiques, comme montré dans Fig.[2(a)], bas), et en recherchant le sous-espace de variance plus élevée dans {x i} par rapport à {y i}. En projetant sur ce sous-espace, nous pouvons effectivement séparer visuellement les images en fonction de la valeur du chiffre manuscrit (Fig. 2(b), bas). En comparant les composantes principales découvertes par l'ACP avec celles découvertes par la cPCA, nous voyons que la cPCA identifie des caractéristiques plus pertinentes (Fig.[2(c)]), ce qui nous permet d'utiliser la cPCA pour des applications telles que la sélection de caractéristiques et le débruitage.

L'ACP contrastive est un outil pour l'apprentissage non supervisé, qui réduit efficacement la dimensionnalité pour permettre la visualisation et l'analyse exploratoire des données. Cela sépare la cPCA d'une grande classe de méthodes d'apprentissage supervisé dont l'objectif principal est de classifier ou de discriminer entre divers ensembles de données, tels que l'analyse discriminante linéaire (LDA), l'analyse discriminante quadratique (QDA), l'ACP supervisée et QUADRO. Cela distingue également la cPCA des méthodes qui intègrent plusieurs ensembles de données, avec pour objectif d'identifier des motifs corrélés parmi deux ou plusieurs ensembles de données, plutôt que ceux uniques à chaque ensemble de données individuel. Il existe également une riche famille de méthodes non supervisées pour la réduction de dimension en dehors de l'ACP. Par exemple, le scaling multidimensionnel (MDS) trouve un encastrement de faible dimension qui préserve la distance dans l'espace de haute dimension ; la poursuite en composantes principales trouve un sous-espace de faible rang qui est robuste au bruit d'entrée faible et aux erreurs éparses grossières. Mais aucune n'est conçue pour utiliser des informations pertinentes d'un second ensemble de données, comme le fait la cPCA. Dans le supplément, nous avons comparé la cPCA à de nombreuses techniques mentionnées précédemment sur des ensembles de données représentatifs (voir les Figs. supplémentaires[3]et[4]), , ,.

Dans un domaine d'application spécifique, il peut y avoir des outils spécialisés dans ce domaine avec des objectifs similaires à ceux de la cPCA, ,. Par exemple, dans les résultats, nous montrons comment la cPCA appliquée aux données de génotype visualise l'ascendance géographique au sein du Mexique. Explorer des clusters fins d'ascendances génétiques est un problème important en génétique des populations, et les chercheurs ont récemment développé un algorithme pour visualiser spécifiquement ces clusters d'ascendance. Bien que la cPCA fonctionne bien ici, l'algorithme conçu par des experts pourrait fonctionner encore mieux pour un ensemble de données spécifique. Cependant, l'algorithme spécialisé nécessite des connaissances substantielles du domaine pour être conçu, est plus coûteux en calcul et peut être difficile à utiliser. L'objectif de la cPCA n'est pas de remplacer toutes ces méthodes spécialisées à la pointe de la technologie dans chacun de leurs domaines, mais de fournir une méthode générale pour explorer des ensembles de données arbitraires.

Nous proposons un algorithme concret et efficace pour la cPCA dans cet article. La méthode prend en entrée un ensemble de données cible {x i} que nous souhaitons visualiser ou dans lequel nous souhaitons identifier des motifs. En tant qu'entrée secondaire, la cPCA prend un ensemble de données de fond {y i}, qui ne contient pas les motifs d'intérêt. L'algorithme cPCA renvoie des sous-espaces qui capturent une grande quantité de variation dans les données cibles {x i}, mais peu dans le fond {y i} (voir Fig.[1], Méthodes, et Méthodes Supplémentaires pour plus de détails). Ce sous-espace correspond à des caractéristiques contenant une structure spécifique à {x i}. Ainsi, lorsque les données cibles sont projetées sur ce sous-espace, nous sommes capables de visualiser et de découvrir la structure supplémentaire dans les données cibles par rapport au fond. Analogues aux composantes principales (PCs), nous appelons les directions trouvées par la cPCA les composantes principales contrastives (cPCs). Nous soulignons que la cPCA est fondamentalement une technique non supervisée, conçue pour résoudre les motifs dans un ensemble de données plus clairement en utilisant l'ensemble de données de fond comme contraste. En particulier, la cPCA ne cherche pas à discriminer entre les ensembles de données cibles et de fond ; le sous-espace qui contient les tendances enrichies dans l'ensemble de données cible n'est pas nécessairement le même sous-espace qui est optimal pour la classification entre les ensembles de données.

Les chercheurs ont noté que l'ACP standard est souvent inefficace pour découvrir des sous-groupes au sein des données biologiques, au moins en partie parce que "les composantes principales dominantes...correspondent à des artefacts," plutôt qu'à des caractéristiques qui intéressent le chercheur. Comment la cPCA peut-elle être utilisée dans ces contextes pour détecter les sous-groupes plus significatifs ? En utilisant un ensemble de données de fond pour annuler la variation universelle mais inintéressante dans la cible, nous pouvons rechercher une structure qui est unique à l'ensemble de données cible.

Notre première expérience utilise un ensemble de données composé de mesures d'expression protéique de souris ayant reçu une thérapie par choc,. Certaines des souris ont développé le syndrome de Down (SD). Pour créer une tâche d'apprentissage non supervisée où nous avons des informations de vérité terrain pour évaluer les méthodes, nous supposons que ces informations sur le SD ne sont pas connues de l'analyste et ne les utilisons que pour l'évaluation de l'algorithme. Nous aimerions voir si nous détectons des différences significatives au sein de la population de souris choquées de manière non supervisée (la présence ou l'absence du syndrome de Down étant un exemple clé). Dans la Fig. [3a] (haut), nous montrons le résultat de l'application de l'ACP à l'ensemble de données cible : les données transformées ne révèlent aucun regroupement significatif au sein de la population de souris. Les principales sources de variation au sein des souris peuvent être naturelles, telles que le sexe ou l'âge.

Nous appliquons la cPCA à cet ensemble de données en utilisant un fond composé de mesures d'expression protéique d'un ensemble de souris qui n'ont pas été exposées à la thérapie par choc. Ce sont des souris de contrôle qui ont probablement une variation naturelle similaire à celle des souris expérimentales, mais sans les différences résultant de la thérapie par choc. Avec cet ensemble de données comme fond, la cPCA est capable de résoudre deux groupes différents dans les données cibles transformées, l'un correspondant aux souris qui n'ont pas le syndrome de Down et l'autre correspondant (principalement) aux souris qui ont le syndrome de Down, comme illustré dans la Fig. [3a] (bas). En comparaison, nous avons également appliqué 8 autres techniques de réduction de dimension pour identifier les directions qui différencient entre les ensembles de données cibles et de fond, aucune d'entre elles n'a pu séparer les souris aussi bien que la cPCA (voir la Fig. supplémentaire [4] pour plus de détails).

Ensuite, nous analysons un ensemble de données public de plus haute dimension composé de niveaux d'expression d'ARN monocellulaire d'un mélange de cellules mononucléaires de moelle osseuse (BMMCs) prélevées sur un patient atteint de leucémie avant la greffe de cellules souches et de BMMCs du même patient après la greffe de cellules souches. Toutes les données d'ARN-Seq monocellulaires sont prétraitées en utilisant des méthodes similaires à celles décrites par les auteurs. En particulier, avant d'appliquer l'ACP ou la cPCA, tous les ensembles de données sont réduits à 500 gènes, qui sont sélectionnés sur la base de la plus haute dispersion [variance divisée par la moyenne] au sein des données cibles. Encore une fois, nous effectuons une ACP pour voir si nous pouvons découvrir visuellement les deux échantillons dans les données transformées. Comme montré dans la Fig. [3b] (en haut à gauche), les deux types de cellules suivent une distribution similaire dans l'espace englobé par les deux premières PCs. Cela est probablement dû au fait que les différences entre les échantillons sont faibles et que les PCs reflètent plutôt l'hétérogénéité de divers types de cellules au sein de chaque échantillon ou même des variations dans les conditions expérimentales, qui peuvent avoir un effet significatif sur les mesures d'ARN-Seq monocellulaires.

Nous appliquons cPCA en utilisant un jeu de données de fond qui consiste en des mesures RNA-Seq des cellules BMMC d'un individu en bonne santé. Nous nous attendons à ce que ce jeu de données de fond contienne la variation due à la population hétérogène de cellules ainsi que les variations dans les conditions expérimentales. Nous pouvons espérer, alors, que cPCA puisse récupérer des directions enrichies dans les données cibles, correspondant aux différences pré- et post-transplantation. En effet, c'est ce que nous trouvons, comme montré dans la Fig. [3b] (en bas à gauche).

Nous augmentons encore notre jeu de données cible avec des échantillons de BMMC d'un deuxième patient atteint de leucémie, encore une fois avant et après la transplantation de cellules souches. Ainsi, il y a un total de quatre sous-populations de cellules. L'application de la PCA sur ces données montre que les quatre sous-populations ne sont pas séparables dans le sous-espace englobé par les deux premiers composants principaux (PCs), comme montré dans la Fig. [3b] (en haut à droite). Encore une fois, cependant, lorsque cPCA est appliqué avec le même jeu de données de fond, au moins trois des sous-populations montrent une séparation beaucoup plus forte, comme montré dans la Fig. [3b] (en bas à droite). L'intégration cPCA suggère également que les échantillons de cellules des deux patients sont plus similaires les uns aux autres après la transplantation de cellules souches (points cyan et verts) qu'avant la transplantation (points or et roses), une hypothèse raisonnable qui peut être testée par l'investigateur. On peut se référer à la Fig. [5] supplémentaire pour plus de détails sur cette expérience. Nous voyons que cPCA peut être un outil utile pour inférer la relation entre les sous-populations, un sujet que nous explorons plus en détail ensuite.

Dans les exemples précédents, nous avons vu que cPCA permet à l'utilisateur de découvrir des sous-classes au sein d'un jeu de données cible qui ne sont pas étiquetées a priori. Cependant, même lorsque les sous-classes sont connues à l'avance, la réduction de dimensionnalité peut être un moyen utile de visualiser la relation au sein des groupes. Par exemple, la PCA est souvent utilisée pour visualiser la relation entre les populations ethniques basées sur les variantes génétiques, car projeter les variantes génétiques sur deux dimensions produit souvent des cartes qui offrent des visualisations frappantes des tendances géographiques et historiques,. Mais encore une fois, la PCA est limitée à l'identification de la structure la plus dominante ; lorsque cela représente une variation universelle ou peu intéressante, cPCA peut être plus efficace pour visualiser les tendances.

Le jeu de données que nous utilisons pour cet exemple consiste en des polymorphismes nucléotidiques simples (SNPs) des génomes d'individus de cinq états du Mexique, collectés dans une étude précédente. L'ascendance mexicaine est difficile à analyser en utilisant la PCA puisque les PCs ne reflètent généralement pas l'origine géographique au sein du Mexique ; au lieu de cela, ils reflètent la proportion d'héritage européen/amérindien de chaque individu mexicain, ce qui domine et obscurcit les différences dues à l'origine géographique au sein du Mexique (voir Fig. [4a] ). Pour surmonter ce problème, les généticiens des populations élaguent manuellement les SNPs, en supprimant ceux connus pour dériver de l'ascendance européenne, avant d'appliquer la PCA. Cependant, cette procédure est d'une applicabilité limitée car elle nécessite de connaître l'origine des SNPs et que la source de variation de fond soit très différente de la variation d'intérêt, ce qui n'est souvent pas le cas.

Comme alternative, nous utilisons cPCA avec un jeu de données de fond qui consiste en des individus du Mexique et d'Europe. Ce fond est dominé par la variation amérindienne/européenne, nous permettant d'isoler la variation intra-mexicaine dans le jeu de données cible. Les résultats de l'application de cPCA sont montrés dans la Fig. [4b]. Nous constatons que les individus du même état au Mexique sont intégrés plus près les uns des autres. De plus, les deux groupes qui sont les plus divergents sont les Sonorans et les Mayas du Yucatan, qui sont également les plus éloignés géographiquement au sein du Mexique, tandis que les Mexicains des trois autres états sont proches les uns des autres, à la fois géographiquement et dans l'intégration capturée par cPCA (voir Fig. [4c] ). Voir également la Fig. [6] supplémentaire pour plus de détails.

Dans de nombreux contextes de science des données, nous sommes intéressés à visualiser et explorer des motifs qui sont enrichis dans un jeu de données par rapport à d'autres données. Nous avons présenté cPCA comme un outil général pour effectuer une telle exploration contrastive, et nous avons illustré son utilité dans une gamme diversifiée d'applications. Les principaux avantages de cPCA sont sa généralité et sa facilité d'utilisation. Calculer un cPCA particulier prend essentiellement le même temps que de calculer une PCA régulière. Cette efficacité computationnelle permet à cPCA d'être utile pour l'exploration interactive des données, où chaque opération devrait idéalement être presque immédiate. Ainsi, dans tous les contextes où la PCA est appliquée sur des jeux de données connexes, cPCA peut également être appliqué. Dans la Note Supplémentaire[3]et la Fig.[8]supplémentaire, nous montrons comment cPCA peut être kernelisé pour découvrir des motifs contrastifs non linéaires dans les jeux de données.

Le seul paramètre libre de la PCA contrastive est la force de contrasteα. Dans notre algorithme par défaut, nous avons développé un schéma automatique basé sur le regroupement de sous-espaces pour sélectionner les valeurs les plus informatives deα (voir Méthodes). Toutes les expériences réalisées pour cet article utilisent les valeurs deα générées automatiquement, et nous pensons que ce paramètre par défaut sera suffisant dans de nombreuses applications de cPCA. L'utilisateur peut également entrer des valeurs spécifiques pourα si une exploration plus fine est souhaitée.

cPCA, comme la PCA régulière et d'autres méthodes de réduction de dimensionnalité, ne fournit pas de valeursp ou d'autres quantifications de signification statistique. Les motifs découverts par cPCA doivent être validés par des tests d'hypothèse ou une analyse supplémentaire utilisant des connaissances pertinentes du domaine. Nous avons publié le code pour cPCA sous forme de package python avec documentation et exemples.

Pour les données cibles de dimensiondxi ∈ Rd et les données de fondyi ∈ Rd , soientC X,C Yleurs matrices de covariance empiriques correspondantes. SoitR unit d = def v ∈ Rd :v2 = 1 l'ensemble des vecteurs unitaires. Pour toute directionv ∈ R unit d , la variance qu'elle explique dans les données cibles et dans les données de fond peut être écrite comme : Variance des données cibles :λX ( v ) =defvTCXv , Variance des données de fond :λY ( v ) =defvTCYv.Étant donné un paramètre de contrasteα ≥ 0 qui quantifie le compromis entre avoir une variance cible élevée et une variance de fond faible, cPCA calcule la direction contrastivev * en optimisant 1 v * = argmax v ∈ Runitd λ X(v)- α λ Y(v).  Ce problème peut être réécrit commev * = argmax v ∈ Runitd v T CX - α CY v ,  ce qui implique quev * correspond au premier vecteur propre de la matriceC = def CX - α CY. Ainsi, les directions contrastives peuvent être calculées efficacement en utilisant la décomposition en valeurs propres. Analogiquement à la PCA, nous appelons les vecteurs propres principaux deC les composantes principales contrastives (cPCs). Nous notons que les cPCs sont des vecteurs propres de la matriceC et sont donc orthogonaux entre eux. Pour unα fixé, nous calculons ( [1] ) et retournons le sous-espace englobé par les premières (typiquement deux) cPCs.

Le paramètre de contrasteα représente le compromis entre avoir une variance cible élevée et une variance de fond faible. Lorsqueα = 0, cPCA sélectionne les directions qui maximisent uniquement la variance cible, et se réduit donc à la PCA appliquée sur les données cibles {x i}. À mesure queα augmente, les directions avec une variance de fond plus faible deviennent plus importantes et les cPCs sont dirigées vers l'espace nul des données de fond {y i}. Dans le cas limiteα = ∞, toute direction qui n'est pas dans l'espace nul de {y i} reçoit une pénalité infinie. Dans ce cas, cPCA correspond d'abord à projeter les données cibles sur l'espace nul des données de fond, puis à effectuer une PCA sur les données projetées.

Au lieu de choisir un seulα et de retourner son sous-espace, cPCA calcule les sous-espaces d'une liste deα et retourne quelques sous-espaces qui sont éloignés les uns des autres en termes d'angle principal. Projeter les données sur chacun de ces sous-espaces révélera différentes tendances au sein des données cibles, et en examinant visuellement les nuages de points qui sont retournés, l'utilisateur peut rapidement discerner le sous-espace pertinent (et la valeur correspondante deα ) pour son analyse. Voir Fig. [1] pour un exemple détaillé.

L'algorithme complet de cPCA est décrit dans l'Algorithme 2 (Méthodes Supplémentaires). Nous fixons généralement la liste des valeurs potentielles deα à 40 valeurs espacées logarithmiquement entre 0,1 et 1000, et cela est utilisé pour toutes les expériences dans l'article. Pour sélectionner les sous-espaces représentatifs, cPCA utilise le clustering spectral pour regrouper les sous-espaces, où l'affinité est définie comme le produit du cosinus des angles principaux entre les sous-espaces. Ensuite, les médianes (représentatives) de chaque cluster sont utilisées comme les valeurs deα pour générer les nuages de points vus par l'utilisateur.

Le choix du jeu de données de fond a une grande influence sur le résultat de cPCA. En général, les données de fond doivent avoir la structure que nous aimerions supprimer des données cibles. Une telle structure correspond généralement à des directions dans la cible avec une variance élevée, mais qui ne sont pas d'intérêt pour l'analyste.

Nous fournissons quelques exemples généraux de jeux de données de fond qui peuvent fournir des contrastes utiles aux données cibles : (1) Un groupe de contrôle {y i} contrasté avec une population malade {x i} parce que le groupe de contrôle contient une variation au niveau de la population similaire mais pas la variation subtile due à différents sous-types de la maladie. (2) Les données au temps zéro {y i} utilisées pour contraster avec les données à un point temporel ultérieur {x i}. Cela permet des visualisations des changements les plus marquants au fil du temps. (3) Un groupe homogène {y i} contrasté avec un groupe mixte {x i} parce que les deux ont une variation intra-populationnelle et un bruit de mesure, mais le premier n'a pas de variation inter-populationnelle. (4) Un jeu de données pré-traitement {y i} contrasté avec des données post-traitement {x i} pour éliminer le bruit de mesure mais préserver les variations causées par le traitement. (5) Un ensemble d'enregistrements sans signal {y i} ou d'images qui ne contiennent que du bruit, contrasté avec des mesures {x i} qui consistent en à la fois du signal et du bruit.

Il convient d'ajouter que les données de fond n'ont pas besoin d'avoir exactement la même structure de covariance que ce que nous aimerions supprimer du jeu de données cible. Par exemple, dans l'expérience montrée dans la Fig. [2] , il s'avère que nous n'avons pas besoin d'utiliser un jeu de données de fond qui consiste en des images d'herbe. En fait, des résultats similaires sont obtenus même si, au lieu d'images d'herbe, des images du ciel sont utilisées comme jeu de données de fond. Comme la structure des matrices de covariance est suffisamment similaire, cPCA élimine la structure de fond des données cibles. De plus, cPCA ne nécessite pas que les données cibles et les données de fond aient un nombre similaire d'échantillons. Étant donné que les matrices de covariance sont calculées indépendamment, cPCA nécessite seulement que les matrices de covariance empiriques soient de bonnes estimations des matrices de covariance de la population sous-jacente, essentiellement la même exigence que PCA.

Ici, nous discutons de l'interprétation géométrique de cPCA ainsi que de ses propriétés statistiques. Tout d'abord, il est intéressant de considérer quelles directions sont "meilleures" pour l'analyse contrastive. Pour une directionv ∈ R unit d , son importance dans cPCA est entièrement déterminée par sa paire de variances cible-fond (λ X(v ),λ Y(v )); il est souhaitable d'avoir une variance cible plus élevée et une variance de fond plus faible. Sur la base de cette intuition, nous pouvons définir un ordre partiel de contrastivité pour diverses directions : pour deux directionsv1 etv2 , nous pourrions dire quev1 est une meilleure direction contrastive si elle a une variance cible plus élevée et une variance de fond plus faible. Dans ce cas, la paire de variances cible-fond dev1 se situerait sur le côté inférieur droit de celle dev2 dans le graphique des paires de variances cible-fond (λ X(v ),λ Y(v )), par exemple, Fig. [5]. Sur la base de cet ordre partiel, l'ensemble des directions les plus contrastives peut être défini de manière similaire à la définition de la frontière de Pareto. SoitU  l'ensemble des paires de variances cible-fond pour toutes les directions, c'est-à-direU = def  ( λX ( v ) ,λY ( v ))  v ∈ Runitd. L'ensemble des directions les plus contrastives correspond à la limite inférieure droite deU  dans le graphique des paires de variances cible-fond, comme montré dans la Fig. [5]. (Pour le cas particulier des matrices de fond et de cible simultanément diagonalisables, voir Fig. Supplémentaire [7].)

Concernant cPCA, nous pouvons prouver (voir Note Supplémentaire [2] ) qu'en faisant varierα , l'ensemble des cPC les plus élevés est identique à l'ensemble des directions les plus contrastives. De plus, pour la directionv sélectionnée par cPCA avec le paramètre de contraste réglé àα , sa paire de variances (λ X(v ),λ Y(v )) correspond au point de tangence de la limite inférieure droite deU  avec une ligne de pente-1/α. En conséquence, en faisant varierα de zéro à l'infini, cPCA sélectionne des directions avec des paires de variances se déplaçant de l'extrémité inférieure gauche à l'extrémité supérieure droite de la limite inférieure droite deU .

Nous remarquons également qu'en ce qui concerne l'aléatoire des données, le taux de convergence du cPC échantillon à la cPC population estO p  dmin ( n,m ) sous des hypothèses modérées, oùd est la dimension etn ,m sont les tailles des données cibles et de fond. Ce taux est similaire au taux de convergence standard du vecteur propre échantillon pour une matrice de covariance. Voir Note Supplémentaire [2].

Nous avons publié une implémentation Python de cPCA sur GitHub (https://github.com/abidlabs/contrastive ). Le dépôt GitHub inclut également des notebooks Python et des jeux de données qui reproduisent la plupart des figures de cet article et dans les Informations Supplémentaires.

Les jeux de données qui ont été utilisés pour évaluer cPCA dans cet article sont soit disponibles chez nous, soit auprès des auteurs des études originales. Veuillez consulter le dépôt GitHub listé dans la section précédente pour les jeux de données que nous avons publiés.

Fig. 1: Vue schématique de cPCA. Pour effectuer cPCA, calculez les matrices de covariance C X, C Ydes ensembles de données cibles et de fond. Les vecteurs singuliers de la différence pondérée des matrices de covariance, C X− α · C Y, sont les directions renvoyées par cPCA. Comme montré dans le nuage de points à droite, PCA (sur les données cibles) identifie la direction qui a la variance la plus élevée dans les données cibles, tandis que cPCA identifie la direction qui a une variance plus élevée dans les données cibles par rapport aux données de fond. Projeter les données cibles sur cette dernière direction donne des motifs uniques aux données cibles et révèle souvent une structure manquée par PCA. Plus précisément, dans cet exemple, réduire la dimensionnalité des données cibles par cPCA révélerait deux clusters distincts

Fig. 2: PCA contrastive sur des chiffres bruyants.a, Haut : Nous créons un ensemble de données cibles de 5 000 images synthétiques en superposant aléatoirement des images de chiffres manuscrits 0 et 1 du jeu de données MNIST sur des images d'herbe prises du jeu de données ImageNet appartenant au synset herbe. Les images d'herbe sont converties en niveaux de gris, redimensionnées à 100 × 100, puis recadrées aléatoirement pour être de la même taille que les chiffres MNIST, 28 × 28.b, Haut : Ici, nous traçons le résultat de l'intégration des images synthétiques sur leurs deux premiers composants principaux en utilisant PCA standard. Nous voyons que les points correspondant aux images avec des 0 et des images avec des 1 sont difficiles à distinguer.a, Bas : Un ensemble de données de fond est ensuite introduit, consistant uniquement en des images d'herbe appartenant au même synset, mais nous utilisons des images différentes de celles utilisées pour créer l'ensemble de données cible.b, Bas : En utilisant cPCA sur les ensembles de données cibles et de fond (avec une valeur du paramètre de contraste α fixée à 2.0), deux clusters émergent dans la représentation de dimension inférieure de l'ensemble de données cible, l'un consistant en des images avec le chiffre 0 et l'autre d'images avec le chiffre 1.cNous examinons la contribution relative de chaque pixel au premier composant principal (PC) et au premier composant principal contrastif (cPC). Les pixels plus blancs sont ceux qui portent un poids plus positif, tandis que les plus sombres désignent les pixels qui portent des poids négatifs. PCA a tendance à accentuer les pixels à la périphérie de l'image et à légèrement désaccentuer les pixels au centre et en bas de l'image, indiquant que la plupart de la variance est due aux caractéristiques de fond. En revanche, cPCA a tendance à surpondérer les pixels qui se trouvent à l'emplacement des 1 manuscrits, à pondérer négativement les pixels à l'emplacement des 0 manuscrits, et à négliger la plupart des autres pixels, découvrant efficacement les caractéristiques utiles pour discriminer entre les chiffres superposés

Fig. 3: Découverte de sous-groupes dans les données biologiques. a Nous utilisons PCA pour projeter un ensemble de données d'expression protéique de souris avec et sans syndrome de Down (DS) sur les deux premiers composants. La représentation de dimension inférieure des mesures d'expression protéique des souris avec et sans DS semble être distribuée de manière similaire (haut). Mais, lorsque nous utilisons cPCA pour projeter l'ensemble de données sur ses deux premiers cPCs, nous découvrons une représentation de dimension inférieure qui regroupe les souris avec et sans DS séparément (bas). b De plus, nous utilisons PCA et cPCA pour visualiser un ensemble de données RNA-Seq à cellule unique de haute dimension en deux dimensions. L'ensemble de données se compose de quatre échantillons de cellules de deux patients atteints de leucémie : un échantillon pré-transplantation du patient 1, un échantillon post-transplantation du patient 1, un échantillon pré-transplantation du patient 2, et un échantillon post-transplantation du patient 2. b , gauche : Les résultats utilisant uniquement les échantillons du patient 1, qui démontrent que cPCA (bas) sépare plus efficacement les échantillons que PCA (haut). Lorsque les échantillons du deuxième patient sont inclus, dans b , droite, encore une fois cPCA (bas) est plus efficace que PCA (haut) pour séparer les échantillons, bien que les cellules post-transplantation des deux patients soient distribuées de manière similaire. Nous montrons des graphiques de chaque échantillon séparément dans la Fig.[5] supplémentaire, où il est plus facile de voir le chevauchement entre différents échantillons

Fig. 4: Relation entre les groupes d'ascendance mexicaine. a PCA appliqué aux données génétiques d'individus de 5 états mexicains ne révèle aucun motif visuellement discernable dans les données intégrées. b cPCA appliqué au même ensemble de données révèle des motifs dans les données : les individus du même état sont regroupés plus près les uns des autres dans l'intégration cPCA. c De plus, la distribution des points révèle des relations entre les groupes qui correspondent à la localisation géographique des différents états : par exemple, les individus d'états géographiquement adjacents sont adjacents dans l'intégration. c Adapté d'une carte du Mexique qui est à l'origine le travail de l'utilisateur : Allstrak sur Wikipedia, publié sous une licence CC-BY-SA, provenant de https://commons.wikimedia.org/wiki/File:Mexico_Map.svg

Fig. 5: Interprétation géométrique de cPCA. L'ensemble des paires de variance cible-fondUest tracé comme la région bleu-vert pour certaines données cibles et de fond générées aléatoirement. La limite inférieure droite, colorée en or, correspond à l'ensemble des directions les plus contrastéesSλ. Les triangles bleus sont les paires de variance pour les cPCs sélectionnés avec des valeurs de α de 0.92 et 0.29 respectivement. Nous notons qu'ils correspondent aux points de tangence de la courbe dorée et des lignes tangentes avec une pente1α= 1.08, 3.37, respectivement

