Patronen verkennen die verrijkt zijn in een dataset met contrastieve hoofcomponentenanalyse 

SamenvattingVisualisatie en verkenning van hoog-dimensionale data is een alomtegenwoordige uitdaging over verschillende disciplines. Veelgebruikte technieken zoals hoofcomponentenanalyse (PCA) zijn gericht op het identificeren van dominante trends in één dataset. Echter, in veel situaties hebben we datasets die onder verschillende omstandigheden zijn verzameld, bijvoorbeeld een behandeling en een controle-experiment, en we zijn geïnteresseerd in het visualiseren en verkennen van patronen die specifiek zijn voor één dataset. Dit artikel stelt een methode voor, contrastieve hoofcomponentenanalyse (cPCA), die laag-dimensionale structuren identificeert die verrijkt zijn in een dataset ten opzichte van vergelijkingsdata. In een breed scala aan experimenten demonstreren we dat cPCA met een achtergrond dataset ons in staat stelt om dataset-specifieke patronen te visualiseren die gemist worden door PCA en andere standaardmethoden. We bieden verder een geometrische interpretatie van cPCA en sterke wiskundige garanties. Een implementatie van cPCA is openbaar beschikbaar en kan worden gebruikt voor verkennende data-analyse in veel toepassingen waar PCA momenteel wordt gebruikt. 

 Principiële componentenanalyse (PCA) is een van de meest gebruikte methoden voor data-exploratie en visualisatie. PCA projecteert de data op een laag-dimensionale ruimte en is vooral krachtig als een benadering om patronen te visualiseren, zoals clusters, clines en uitschieters in een dataset. Er is een groot aantal gerelateerde visualisatiemethoden; bijvoorbeeld, t-SNE en multidimensionale schaling (MDS) staan niet-lineaire dataprojecties toe en kunnen mogelijk niet-lineaire patronen beter vastleggen dan PCA. Toch zijn al deze methoden ontworpen om één dataset tegelijk te verkennen. Wanneer de analist meerdere datasets heeft (of meerdere condities in één dataset om te vergelijken), is de huidige praktijk om PCA (of t-SNE, MDS, enz.) op elke dataset afzonderlijk uit te voeren en vervolgens handmatig de verschillende projecties te vergelijken om te onderzoeken of er interessante overeenkomsten en verschillen zijn tussen datasets. Contrastieve PCA (cPCA) is ontworpen om deze kloof in data-exploratie en visualisatie te vullen door automatisch de projecties te identificeren die de meest interessante verschillen tussen datasets vertonen. Figuur1 geeft een overzicht van cPCA dat we verderop in meer detail uitleggen. Fig. 1 Schematisch Overzicht van cPCA. Om cPCA uit te voeren, bereken de covariantiematrices CX , CY van de doel- en achtergronddatasets. De singuliere vectoren van het gewogen verschil van de covariantiematrices, CX − α· CY , zijn de richtingen die door cPCA worden geretourneerd. Zoals getoond in de spreidingsdiagram aan de rechterkant, identificeert PCA (op de doeldataset) de richting met de hoogste variantie in de doeldataset, terwijl cPCA de richting identificeert die een hogere variantie heeft in de doeldataset vergeleken met de achtergronddata. Het projecteren van de doeldataset op de laatstgenoemde richting geeft patronen die uniek zijn voor de doeldataset en onthult vaak structuren die door PCA worden gemist. Specifiek, in dit voorbeeld zou het verminderen van de dimensionaliteit van de doeldataset door cPCA twee verschillende clusters onthullen 

 cPCA is gemotiveerd door een breed scala aan problemen over disciplines heen. Ter illustratie noemen we hier twee van dergelijke problemen en demonstreren we andere door middel van experimenten later in het artikel. Ten eerste, overweeg een dataset van genexpressiemetingen van individuen van verschillende etniciteiten en geslachten. Deze data omvat genexpressieniveaus van kankerpatiënten {x i}, die we willen analyseren. We hebben ook controlegegevens, die overeenkomen met de genexpressieniveaus van gezonde patiënten {y i} uit een vergelijkbare demografische achtergrond. Ons doel is om trends en variaties binnen kankerpatiënten te vinden (bijv. om moleculaire subtypes van kanker te identificeren). Als we direct PCA toepassen op {x i}, kunnen de belangrijkste hoofcomponenten echter overeenkomen met de demografische variaties van de individuen in plaats van de subtypes van kankers, omdat de genetische variaties door de eerste waarschijnlijk groter zijn dan die van de laatste. We benaderen dit probleem door op te merken dat de gezonde patiënten ook de variatie bevatten die geassocieerd is met demografische verschillen, maar niet de variatie die overeenkomt met subtypes van kankers. Dus kunnen we zoeken naar componenten waarin {x i} een hoge variantie heeft maar {y i} een lage variantie heeft.

 Als een gerelateerd voorbeeld, overweeg een dataset {x i} die bestaat uit handgeschreven cijfers op een complexe achtergrond, zoals verschillende afbeeldingen van gras (zie Fig.2(a), boven ). Het doel van een typische ongesuperviseerde leertaken kan zijn om de data te clusteren, waarbij de verschillende cijfers in de afbeelding worden onthuld. Echter, als we standaard PCA toepassen op deze afbeeldingen, vinden we dat de belangrijkste hoofcomponenten geen kenmerken vertegenwoordigen die gerelateerd zijn aan de handgeschreven cijfers, maar de dominante variatie in kenmerken gerelateerd aan de afbeeldingsachtergrond weerspiegelen (Fig.2(b) , boven). We tonen aan dat het mogelijk is om dit te corrigeren door een referentiedataset {y i} te gebruiken die uitsluitend bestaat uit afbeeldingen van het gras (niet noodzakelijk dezelfde afbeeldingen die in {x i} zijn gebruikt, maar met een vergelijkbare covariantie tussen kenmerken, zoals getoond in Fig.2(a) , onder), en te zoeken naar de deelruimte van hogere variantie in {x i} vergeleken met {y i}. Door op deze deelruimte te projecteren, kunnen we de afbeeldingen daadwerkelijk visueel scheiden op basis van de waarde van het handgeschreven cijfer (Fig. 2(b), onder). Door de hoofcomponenten ontdekt door PCA te vergelijken met die ontdekt door cPCA, zien we dat cPCA meer relevante kenmerken identificeert (Fig.2(c) ), wat ons in staat stelt om cPCA te gebruiken voor toepassingen zoals kenmerkselectie en ruisonderdrukking. Fig. 2 Contrastieve PCA op Ruisende Cijfers. a, Boven: We creëren een doeldataset van 5.000 synthetische afbeeldingen door willekeurig afbeeldingen van handgeschreven cijfers 0 en 1 uit de MNIST-datasetbovenop afbeeldingen van gras uit de ImageNet-datasetbehorend tot de synset gras te plaatsen. De afbeeldingen van gras worden omgezet naar grijstinten, verkleind naar 100 × 100, en vervolgens willekeurig bijgesneden tot dezelfde grootte als de MNIST-cijfers, 28 × 28. b, Boven: Hier plotten we het resultaat van het inbedden van de synthetische afbeeldingen op hun eerste twee hoofcomponenten met behulp van standaard PCA. We zien dat de punten die overeenkomen met de afbeeldingen met 0's en afbeeldingen met 1's moeilijk te onderscheiden zijn. a, Onder: Een achtergronddataset wordt vervolgens geïntroduceerd die uitsluitend bestaat uit afbeeldingen van gras behorend tot dezelfde synset, maar we gebruiken afbeeldingen die anders zijn dan die gebruikt om de doeldataset te maken. b, Onder: Door cPCA te gebruiken op de doel- en achtergronddatasets (met een waarde van de contrastparameter αingesteld op 2.0), ontstaan er twee clusters in de lager-dimensionale representatie van de doeldataset, één bestaande uit afbeeldingen met het cijfer 0 en de andere uit afbeeldingen met het cijfer 1. cWe kijken naar de relatieve bijdrage van elke pixel aan de eerste hoofcomponent (PC) en de eerste contrastieve hoofcomponent (cPC). Wittere pixels zijn die met een meer positieve gewicht, terwijl donkerdere die pixels aanduiden die negatieve gewichten dragen. PCA heeft de neiging om pixels in de periferie van de afbeelding te benadrukken en pixels in het midden en onderkant van de afbeelding iets te de-emphaseren, wat aangeeft dat de meeste variantie te wijten is aan achtergrondkenmerken. Aan de andere kant heeft cPCA de neiging om de pixels die zich op de locatie van de handgeschreven 1's bevinden te verzwaren, pixels op de locatie van handgeschreven 0's negatief te wegen, en de meeste andere pixels te negeren, waardoor effectief die kenmerken worden ontdekt die nuttig zijn voor het onderscheiden tussen de over elkaar geplaatste cijfers 

 Contrastieve PCA is een hulpmiddel voor ongesuperviseerd leren, dat efficiënt de dimensionaliteit vermindert om visualisatie en verkennende data-analyse mogelijk te maken. Dit scheidt cPCA van een grote klasse van gesuperviseerde leermethoden waarvan het primaire doel is om verschillende datasets te classificeren of te discrimineren, zoals lineaire discriminantanalyse (LDA), kwadratische discriminantanalyse (QDA), gesuperviseerde PCA en QUADRO. Dit onderscheidt cPCA ook van methoden die meerdere datasets integreren, met als doel gecorreleerde patronen te identificeren tussen twee of meer datasets, in plaats van die uniek voor elke individuele dataset. Er is ook een rijke familie van ongesuperviseerde methoden voor dimensiereductie naast PCA. Bijvoorbeeld, multidimensionale schaling (MDS) vindt een laag-dimensionale inbedding die de afstand in de hoog-dimensionale ruimte behoudt; hoofcomponentenachtervolging vindt een laag-rang deelruimte die robuust is tegen kleine invoer-ruis en grove spaarfouten. Maar geen enkele is ontworpen om relevante informatie uit een tweede dataset te gebruiken, zoals cPCA doet. In de bijlage hebben we cPCA vergeleken met veel van de eerder genoemde technieken op representatieve datasets (zie Supplementaire Figs.3 en4 ).

 In een specifiek toepassingsdomein kunnen er gespecialiseerde hulpmiddelen zijn in dat domein met vergelijkbare doelen als cPCA. Bijvoorbeeld, in de resultaten laten we zien hoe cPCA toegepast op genotypegegevens geografische afkomst binnen Mexico visualiseert. Het verkennen van fijnmazige clusters van genetische afkomsten is een belangrijk probleem in de populatiegenetica, en onderzoekers hebben recentelijk een algoritme ontwikkeld om specifiek dergelijke afkomstclusters te visualiseren. Hoewel cPCA hier goed presteert, kan het door experts gemaakte algoritme mogelijk nog beter presteren voor een specifieke dataset. Echter, het gespecialiseerde algoritme vereist aanzienlijke domeinkennis om te ontwerpen, is meer rekenintensief en kan moeilijk te gebruiken zijn. Het doel van cPCA is niet om al deze gespecialiseerde state-of-the-art methoden in elk van hun domeinen te vervangen, maar om een algemene methode te bieden voor het verkennen van willekeurige datasets.

 We stellen een concreet en efficiënt algoritme voor cPCA voor in dit artikel. De methode neemt als invoer een doeldataset {x i} die we willen visualiseren of waarin we patronen willen identificeren. Als secundaire invoer neemt cPCA een achtergronddataset {y i}, die de patronen van interesse niet bevat. Het cPCA-algoritme retourneert deelruimten die een grote hoeveelheid variatie in de doeldataset {x i} vastleggen, maar weinig in de achtergrond {y i} (zie Fig.1 , Methoden en Supplementaire Methoden voor meer details). Deze deelruimte komt overeen met kenmerken die structuur bevatten die specifiek is voor {x i}. Daarom, wanneer de doeldataset op deze deelruimte wordt geprojecteerd, kunnen we de extra structuur in de doeldataset ten opzichte van de achtergrond visualiseren en ontdekken. Analoog aan de hoofcomponenten (PC's), noemen we de richtingen gevonden door cPCA de contrastieve hoofcomponenten (cPC's). We benadrukken dat cPCA fundamenteel een ongesuperviseerde techniek is, ontworpen om patronen in één dataset duidelijker op te lossen door de achtergronddataset als contrast te gebruiken. In het bijzonder streeft cPCA er niet naar om te discrimineren tussen de doel- en achtergronddatasets; de deelruimte die trends bevat die verrijkt zijn in de doeldataset is niet noodzakelijk dezelfde deelruimte die optimaal is voor classificatie tussen de datasets.

Onderzoekers hebben opgemerkt dat standaard PCA vaak ineffectief is bij het ontdekken van subgroepen binnen biologische data, althans gedeeltelijk omdat "dominante hoofcomponenten...correleren met artefacten," in plaats van met kenmerken die van belang zijn voor de onderzoeker. Hoe kan cPCA in deze instellingen worden gebruikt om de meer significante subgroepen te detecteren? Door een achtergronddataset te gebruiken om de universele maar oninteressante variatie in het doel te annuleren, kunnen we zoeken naar structuur die uniek is voor de doeldataset. 

Ons eerste experiment gebruikt een dataset bestaande uit eiwitexpressiemetingen van muizen die schoktherapie hebben ondergaan. Sommige van de muizen hebben het syndroom van Down (DS) ontwikkeld. Om een ongesuperviseerde leertaken te creëren waarbij we grondwaarheidsinformatie hebben om de methoden te evalueren, gaan we ervan uit dat deze DS-informatie niet bekend is bij de analist en gebruiken we deze alleen voor algoritme-evaluatie. We willen zien of we enige significante verschillen binnen de geschokte muizenpopulatie kunnen detecteren op een ongesuperviseerde manier (de aanwezigheid of afwezigheid van het syndroom van Down zijnde een belangrijk voorbeeld). In Fig.3a (boven), tonen we het resultaat van het toepassen van PCA op de doeldataset: de getransformeerde data onthult geen significante clustering binnen de muizenpopulatie. De belangrijkste bronnen van variatie binnen muizen kunnen natuurlijk zijn, zoals geslacht of leeftijd. Fig. 3Ontdekken van subgroepen in biologische data. aWe gebruiken PCA om een eiwitexpressiedataset van muizen met en zonder Downsyndroom (DS) op de eerste twee componenten te projecteren. De lager-dimensionale representatie van eiwitexpressiemetingen van muizen met en zonder DS blijkt vergelijkbaar verdeeld te zijn (boven). Maar, wanneer we cPCA gebruiken om de dataset op zijn eerste twee cPC's te projecteren, ontdekken we een lager-dimensionale representatie die muizen met en zonder DS afzonderlijk clustert (onder). bVerder gebruiken we PCA en cPCA om een hoog-dimensionale single-cell RNA-Seq dataset in twee dimensies te visualiseren. De dataset bestaat uit vier celmonsters van twee leukemiepatiënten: een pre-transplantatiemonster van patiënt 1, een post-transplantatiemonster van patiënt 1, een pre-transplantatiemonster van patiënt 2, en een post-transplantatiemonster van patiënt 2. b, links: De resultaten met alleen de monsters van patiënt 1, die aantonen dat cPCA (onder) de monsters effectiever scheidt dan PCA (boven). Wanneer de monsters van de tweede patiënt worden opgenomen, in b, rechts, is cPCA (onder) opnieuw effectiever dan PCA (boven) bij het scheiden van de monsters, hoewel de post-transplantatiecellen van beide patiënten vergelijkbaar verdeeld zijn. We tonen grafieken van elk monster afzonderlijk in Supplementaire Fig. 5, waar het gemakkelijker is om de overlap tussen verschillende monsters te zien

We passen cPCA toe op deze dataset met behulp van een achtergrond die bestaat uit eiwitexpressiemetingen van een set muizen die niet zijn blootgesteld aan schoktherapie. Het zijn controlemuizen die waarschijnlijk vergelijkbare natuurlijke variatie hebben als de experimentele muizen, maar zonder de verschillen die voortkomen uit de schoktherapie. Met deze dataset als achtergrond is cPCA in staat om twee verschillende groepen in de getransformeerde doeldataset op te lossen, één die overeenkomt met muizen die het syndroom van Down niet hebben en één die (meestal) overeenkomt met muizen die het syndroom van Down hebben, zoals geïllustreerd in Fig.3a (onder). Ter vergelijking hebben we ook 8 andere technieken voor dimensiereductie toegepast om richtingen te identificeren die verschillen tussen de doel- en achtergronddatasets, waarvan geen enkele de muizen zo goed kon scheiden als cPCA (zie Supplementaire Fig.4 voor details). 

Vervolgens analyseren we een hoger-dimensionale openbare dataset bestaande uit enkelcel-RNA-expressieniveaus van een mengsel van beenmerg-mononucleaire cellen (BMMCs) genomen van een leukemiepatiënt vóór stamceltransplantatie en BMMCs van dezelfde patiënt na stamceltransplantatie. Alle enkelcel-RNA-Seq-data worden voorbewerkt met behulp van vergelijkbare methoden zoals beschreven door de auteurs. In het bijzonder, voordat PCA of cPCA wordt toegepast, worden alle datasets teruggebracht tot 500 genen, die worden geselecteerd op basis van de hoogste spreiding [variantie gedeeld door gemiddelde] binnen de doeldataset. Opnieuw voeren we PCA uit om te zien of we de twee monsters visueel kunnen ontdekken in de getransformeerde data. Zoals getoond in Fig.3b (linksboven), volgen beide celtypen een vergelijkbare verdeling in de ruimte die wordt overspannen door de eerste twee PC's. Dit is waarschijnlijk omdat de verschillen tussen de monsters klein zijn en de PC's in plaats daarvan de heterogeniteit van verschillende soorten cellen binnen elk monster weerspiegelen of zelfs variaties in experimentele omstandigheden, die een significante invloed kunnen hebben op enkelcel-RNA-Seq-metingen. 

We passen cPCA toe met behulp van een achtergrond dataset die bestaat uit RNA-Seq metingen van BMMC-cellen van een gezond individu. We verwachten dat deze achtergrond dataset de variatie bevat die te wijten is aan de heterogene populatie van cellen evenals variaties in experimentele omstandigheden. We hopen dan ook dat cPCA in staat is om richtingen te herstellen die verrijkt zijn in de doelgegevens, overeenkomend met pre- en post-transplantatieverschillen. Inderdaad, dat is wat we vinden, zoals getoond in Fig.3b (onder links). 

We breiden onze doel dataset verder uit met BMMC-monsters van een tweede leukemiepatiënt, opnieuw voor en na stamceltransplantatie. Er zijn dus in totaal vier subpopulaties van cellen. Toepassing van PCA op deze gegevens laat zien dat de vier subpopulaties niet scheidbaar zijn in de subruimte die wordt overspannen door de twee belangrijkste hoofcomponenten (PC's), zoals getoond in Fig.3b (boven rechts). Echter, wanneer cPCA wordt toegepast met dezelfde achtergrond dataset, vertonen ten minste drie van de subpopulaties een veel sterkere scheiding, zoals getoond in Fig.3b (onder rechts). De cPCA-embedding suggereert ook dat de celmonsters van beide patiënten meer op elkaar lijken na stamceltransplantatie (cyaan en groene stippen) dan voor de transplantatie (gouden en roze stippen), een redelijke hypothese die door de onderzoeker kan worden getest. Men kan verwijzen naar Supplementaire Fig.5 voor meer details van dit experiment. We zien dat cPCA een nuttig hulpmiddel kan zijn om de relatie tussen subpopulaties af te leiden, een onderwerp dat we verder onderzoeken. 

In eerdere voorbeelden hebben we gezien dat cPCA de gebruiker in staat stelt om subklassen binnen een doel dataset te ontdekken die niet a priori zijn gelabeld. Echter, zelfs wanneer subklassen van tevoren bekend zijn, kan dimensiereductie een nuttige manier zijn om de relatie binnen groepen te visualiseren. Bijvoorbeeld, PCA wordt vaak gebruikt om de relatie tussen etnische populaties te visualiseren op basis van genetische varianten, omdat het projecteren van de genetische varianten op twee dimensies vaak kaarten produceert die opvallende visualisaties van geografische en historische trends bieden. Maar opnieuw, PCA is beperkt tot het identificeren van de meest dominante structuur; wanneer dit universele of oninteressante variatie vertegenwoordigt, kan cPCA effectiever zijn bij het visualiseren van trends. 

De dataset die we voor dit voorbeeld gebruiken bestaat uit enkelvoudige nucleotide polymorfismen (SNP's) uit de genomen van individuen uit vijf staten in Mexico, verzameld in een eerdere studie. Mexicaanse afkomst is moeilijk te analyseren met behulp van PCA, aangezien de PC's meestal niet de geografische oorsprong binnen Mexico weerspiegelen; in plaats daarvan weerspiegelen ze het aandeel van Europese/Native Amerikaanse afkomst van elk Mexicaans individu, wat de verschillen door geografische oorsprong binnen Mexico domineert en verbergt (zie Fig.4a ). Om dit probleem te overwinnen, snoeien populatiegenetici handmatig SNP's, waarbij ze diegenen verwijderen waarvan bekend is dat ze afkomstig zijn van Europese afkomst, voordat ze PCA toepassen. Deze procedure is echter van beperkte toepasbaarheid, omdat het vereist dat de oorsprong van de SNP's bekend is en dat de bron van achtergrondvariatie zeer verschillend is van de variatie van belang, wat vaak niet het geval is. Fig. 4Relatie tussen Mexicaanse afkomstgroepen. aPCA toegepast op genetische data van individuen uit 5 Mexicaanse staten onthult geen visueel waarneembare patronen in de ingebedde data. bcPCA toegepast op dezelfde dataset onthult patronen in de data: individuen uit dezelfde staat zijn dichter bij elkaar geclusterd in de cPCA-embedding. cBovendien onthult de verdeling van de punten relaties tussen de groepen die overeenkomen met de geografische locatie van de verschillende staten: bijvoorbeeld, individuen uit geografisch aangrenzende staten zijn aangrenzend in de embedding. cAangepast van een kaart van Mexico die oorspronkelijk het werk is van Gebruiker:Allstrak op Wikipedia, gepubliceerd onder een CC-BY-SA-licentie, afkomstig van https://commons.wikimedia.org/wiki/File:Mexico_Map.svg 

Als alternatief gebruiken we cPCA met een achtergrond dataset die bestaat uit individuen uit Mexico en Europa. Deze achtergrond wordt gedomineerd door Native Amerikaanse/Europese variatie, waardoor we de intra-Mexicaanse variatie in de doel dataset kunnen isoleren. De resultaten van het toepassen van cPCA worden getoond in Fig.4b. We vinden dat individuen uit dezelfde staat in Mexico dichter bij elkaar zijn ingebed. Bovendien zijn de twee groepen die het meest divergent zijn de Sonorans en de Maya's uit Yucatan, die ook het meest geografisch verwijderd zijn binnen Mexico, terwijl Mexicanen uit de andere drie staten dicht bij elkaar liggen, zowel geografisch als in de embedding vastgelegd door cPCA (zie Fig.4c ). Zie ook Supplementaire Fig.6 voor meer details. 

 In veel datawetenschappelijke omgevingen zijn we geïnteresseerd in het visualiseren en verkennen van patronen die verrijkt zijn in één dataset ten opzichte van andere gegevens. We hebben cPCA gepresenteerd als een algemeen hulpmiddel voor het uitvoeren van dergelijke contrasterende verkenning, en we hebben de bruikbaarheid ervan geïllustreerd in een diverse reeks toepassingen. De belangrijkste voordelen van cPCA zijn de algemeenheid en het gebruiksgemak. Het berekenen van een specifieke cPCA kost in wezen dezelfde hoeveelheid tijd als het berekenen van een reguliere PCA. Deze computationele efficiëntie maakt cPCA nuttig voor interactieve data-exploratie, waar elke bewerking idealiter bijna onmiddellijk zou moeten zijn. Als zodanig kan cPCA ook worden toegepast in elke omgeving waar PCA wordt toegepast op gerelateerde datasets. In de Supplementaire Nota3 en Supplementaire Fig.8 , laten we zien hoe cPCA kan worden gekerneliseerd om niet-lineaire contrasterende patronen in datasets te onthullen.

 De enige vrije parameter van contrasterende PCA is de contraststerkteα. In ons standaardalgoritme hebben we een automatisch schema ontwikkeld op basis van clustering van subruimten voor het selecteren van de meest informatieve waarden vanα (zie Methoden). Alle experimenten die voor dit artikel zijn uitgevoerd, gebruiken de automatisch gegenereerdeα -waarden, en we geloven dat deze standaard voldoende zal zijn in veel toepassingen van cPCA. De gebruiker kan ook specifieke waarden voorα invoeren als meer gedetailleerde verkenning gewenst is.

 cPCA, net als reguliere PCA en andere methoden voor dimensiereductie, geeft geenp -waarden of andere statistische significantiekwantificaties. De patronen die door cPCA worden ontdekt, moeten worden gevalideerd door hypothesetesten of aanvullende analyse met behulp van relevante domeinkennis. We hebben de code voor cPCA vrijgegeven als een python-pakket samen met documentatie en voorbeelden.

Voor ded -dimensionale doelgegevens (equation) en achtergrondgegevens (equation) , laatC X,C Yhun overeenkomstige empirische covariantiematrices zijn. Laat (equation) de set van eenheidsvectoren zijn. Voor elke richting (equation) , kan de variantie die het verklaart in de doelgegevens en in de achtergrondgegevens worden geschreven als: (equation) Gegeven een contrastparameterα ≥ 0 die de afweging kwantificeert tussen het hebben van hoge doelvariantie en lage achtergrondvariantie, berekent cPCA de contrasterende richtingv * door te optimaliseren (equation) Dit probleem kan worden herschreven als (equation) wat impliceert datv * overeenkomt met de eerste eigenvector van de matrix (equation). Vandaar dat de contrasterende richtingen efficiënt kunnen worden berekend met behulp van eigenwaarde decompositie. Analoog aan PCA, noemen we de leidende eigenvectoren vanC de contrasterende hoofcomponenten (cPC's). We merken op dat de cPC's eigenvectoren zijn van de matrixC en dus orthogonaal aan elkaar zijn. Voor een vasteα , berekenen we (1 ) en retourneren we de subruimte die wordt overspannen door de eerste paar (meestal twee) cPC's. 

De contrastparameterα vertegenwoordigt de afweging tussen het hebben van de hoge doelvariantie en de lage achtergrondvariantie. Wanneerα = 0, selecteert cPCA de richtingen die alleen de doelvariantie maximaliseren, en reduceert dus tot PCA toegepast op de doelgegevens {x i}. Naarmateα toeneemt, worden richtingen met kleinere achtergrondvariantie belangrijker en worden de cPC's naar de nulruimte van de achtergrondgegevens {y i} gedreven. In het limietgevalα = ∞, ontvangt elke richting die niet in de nulruimte van {y i} ligt een oneindige straf. In dit geval komt cPCA overeen met het eerst projecteren van de doelgegevens op de nulruimte van de achtergrondgegevens, en vervolgens het uitvoeren van PCA op de geprojecteerde gegevens. 

In plaats van een enkeleα te kiezen en de bijbehorende deelruimte terug te geven, berekent cPCA de deelruimten van een lijst vanα 's en geeft het enkele deelruimten terug die ver van elkaar verwijderd zijn in termen van de hoofdhoek. Door de gegevens op elk van deze deelruimten te projecteren, worden verschillende trends binnen de doelgegevens onthuld, en door de teruggegeven spreidingsdiagrammen visueel te onderzoeken, kan de gebruiker snel de relevante deelruimte (en bijbehorende waarde vanα ) voor zijn of haar analyse onderscheiden. Zie Aanvullende Fig.1 voor een gedetailleerd voorbeeld. 

Het volledige algoritme van cPCA wordt beschreven in Algoritme 2 (Aanvullende Methoden). We stellen de lijst van potentiële waarden vanα meestal in op 40 waarden die logaritmisch zijn verdeeld tussen 0,1 en 1000 en dit wordt gebruikt voor alle experimenten in het artikel. Om de representatieve deelruimten te selecteren, gebruikt cPCA spectrale clustering om de deelruimten te clusteren, waarbij de affiniteit wordt gedefinieerd als het product van de cosinus van de hoofdhoeken tussen de deelruimten. Vervolgens worden de medoids (representatief) van elke cluster gebruikt als de waarden vanα om de spreidingsdiagrammen te genereren die door de gebruiker worden gezien. 

De keuze van de achtergronddataset heeft een grote invloed op het resultaat van cPCA. Over het algemeen moet de achtergrondgegevens de structuur hebben die we uit de doelgegevens willen verwijderen. Dergelijke structuur komt meestal overeen met richtingen in de doelgegevens met hoge variantie, maar die niet van belang zijn voor de analist. 

We geven een paar algemene voorbeelden van achtergronddatasets die nuttige contrasten kunnen bieden met doelgegevens: (1) Een controlegroep {y i} vergeleken met een zieke populatie {x i} omdat de controlegroep vergelijkbare populatieniveauvariatie bevat, maar niet de subtiele variatie door verschillende subtypes van de ziekte. (2) De gegevens op tijdstip nul {y i} gebruikt om te contrasteren met gegevens op een later tijdstip {x i}. Dit maakt visualisaties van de meest opvallende veranderingen in de tijd mogelijk. (3) Een homogene groep {y i} vergeleken met een gemengde groep {x i} omdat beide intra-populatievariatie en meetruis hebben, maar de eerste heeft geen inter-populatievariatie. (4) Een pre-behandelingsdataset {y i} vergeleken met post-behandelingsgegevens {x i} om meetruis te verwijderen maar variaties veroorzaakt door behandeling te behouden. (5) Een set signaalvrije opnamen {y i} of afbeeldingen die alleen ruis bevatten, vergeleken met metingen {x i} die zowel signaal als ruis bevatten. 

Het is de moeite waard om toe te voegen dat de achtergrondgegevens niet precies dezelfde covariatiestructuur hoeven te hebben als wat we uit de doelgegevens willen verwijderen. Als voorbeeld, in het experiment getoond in Fig.2 , blijkt dat we geen achtergronddataset hoeven te gebruiken die bestaat uit afbeeldingen van gras. In feite worden vergelijkbare resultaten verkregen, zelfs als in plaats van afbeeldingen van gras, afbeeldingen van de lucht worden gebruikt als de achtergronddataset. Aangezien de structuur van de covariantiematrices voldoende vergelijkbaar is, verwijdert cPCA de achtergrondstructuur uit de doelgegevens. Bovendien vereist cPCA niet dat de doelgegevens en de achtergrondgegevens een vergelijkbaar aantal monsters hebben. Aangezien de covariantiematrices onafhankelijk worden berekend, vereist cPCA alleen dat de empirische covariantiematrices goede schattingen zijn van de onderliggende populatiecovariantiematrices, in wezen dezelfde vereiste als PCA. 

Hier bespreken we de geometrische interpretatie van cPCA evenals de statistische eigenschappen ervan. Ten eerste is het interessant om te overwegen welke richtingen "beter" zijn voor het doel van contrastieve analyse. Voor een richting (equation) , wordt de betekenis ervan in cPCA volledig bepaald door zijn doel-achtergrond variantie paar (λ X(v ),λ Y(v )); het is wenselijk om een hogere doelvariantie en een lagere achtergrondvariantie te hebben. Op basis van deze intuïtie kunnen we verder een gedeeltelijke volgorde van contrastiviteit definiëren voor verschillende richtingen: voor twee richtingenv1 env2 , kunnen we zeggen datv1 een betere contrastieve richting is als het een hogere doelvariantie en een lagere achtergrondvariantie heeft. In dit geval zou het doel-achtergrond variantie paar vanv1 aan de rechteronderkant van dat vanv2 liggen in de plot van doel-achtergrond variantie paren (λ X(v ),λ Y(v )), bijvoorbeeld, Fig.5. Op basis van deze gedeeltelijke volgorde kan de set van meest contrastieve richtingen worden gedefinieerd op een vergelijkbare manier als de definitie van de Pareto-grens. Laat (equation) de set van doel-achtergrond variantie paren zijn voor alle richtingen, d.w.z. (equation). De set van meest contrastieve richtingen komt overeen met de rechterondergrens van (equation) in de plot van doel-achtergrond variantie paren, zoals getoond in Fig.5. (Voor het specifieke geval van gelijktijdig diagonaliseerbare achtergrond- en doelmatrices, zie Aanvullende Fig.7.) Fig. 5Geometrische Interpretatie van cPCA. De set van doel-achtergrond variantieparen(equation)is uitgezet als de teal regio voor enkele willekeurig gegenereerde doel- en achtergronddata. De rechtsondergrens, gekleurd in goud, komt overeen met de set van meest contrastieve richtingen(equation). De blauwe driehoeken zijn de variantieparen voor de cPC's geselecteerd met αwaarden 0.92 en 0.29 respectievelijk. We merken op dat ze overeenkomen met de raakpunten van de gouden curve en de raaklijnen met helling(equation)= 1.08, 3.37, respectievelijk

Met betrekking tot cPCA kunnen we bewijzen (zie Aanvullende Nota2 ) dat doorα te variëren, de set van top cPC's identiek is aan de set van meest contrastieve richtingen. Bovendien, voor de richtingv geselecteerd door cPCA met de contrastparameter ingesteld opα , komt het variantie paar (λ X(v ),λ Y(v )) overeen met het raakpunt van de rechterondergrens van (equation) met een helling-1/α lijn. Als resultaat, doorα te variëren van nul tot oneindig, selecteert cPCA richtingen met variantie paren die reizen van het linkerondereinde naar het rechterboveneinde van de rechterondergrens van (equation). 

We merken ook op dat met betrekking tot de willekeurigheid van de gegevens, de convergentiesnelheid van de steekproef cPC naar de populatie cPC is (equation) onder milde aannames, waarbijd de dimensie is enn ,m de groottes zijn van de doel- en achtergrondgegevens. Deze snelheid is vergelijkbaar met de standaard convergentiesnelheid van de steekproef eigenvector voor een covariantiematrix. Zie Aanvullende Nota2. 

We hebben een Python-implementatie van contrastieve PCA vrijgegeven op GitHub (https://github.com/abidlabs/contrastive ). De GitHub-repository bevat ook Python-notebooks en datasets die de meeste figuren in dit artikel en in de Aanvullende Informatie reproduceren. 

Datasets die zijn gebruikt om contrastieve PCA in dit artikel te evalueren, zijn ofwel beschikbaar bij ons of bij de auteurs van de oorspronkelijke studies. Zie de GitHub-repository die in de vorige sectie wordt vermeld voor de datasets die we hebben vrijgegeven. 

