Utforska mönster berikade i en dataset med kontrastiv huvudkomponentanalys 

SammanfattningVisualisering och utforskning av högdimensionell data är en allestädes närvarande utmaning över discipliner. Allmänt använda tekniker som huvudkomponentanalys (PCA) syftar till att identifiera dominerande trender i en dataset. Men i många sammanhang har vi dataset insamlade under olika förhållanden, t.ex. en behandling och ett kontrollförsök, och vi är intresserade av att visualisera och utforska mönster som är specifika för en dataset. Denna artikel föreslår en metod, kontrastiv huvudkomponentanalys (cPCA), som identifierar lågdimensionella strukturer som är berikade i en dataset i förhållande till jämförelsedata. I en mängd olika experiment visar vi att cPCA med en bakgrundsdataset gör det möjligt för oss att visualisera dataset-specifika mönster som missas av PCA och andra standardmetoder. Vi ger vidare en geometrisk tolkning av cPCA och starka matematiska garantier. En implementering av cPCA är offentligt tillgänglig och kan användas för utforskande dataanalys i många applikationer där PCA för närvarande används. 

 Huvudkomponentanalys (PCA) är en av de mest använda metoderna för datautforskning och visualisering. PCA projicerar data på ett lågdimensionellt utrymme och är särskilt kraftfull som en metod för att visualisera mönster, såsom kluster, kliner och avvikare i en dataset. Det finns ett stort antal relaterade visualiseringsmetoder; till exempel tillåter t-SNE och multidimensionell skalning (MDS) icke-linjära dataprojektioner och kan bättre fånga icke-linjära mönster än PCA. Ändå är alla dessa metoder utformade för att utforska en dataset åt gången. När analytikern har flera dataset (eller flera förhållanden i en dataset att jämföra), är den nuvarande praxis att utföra PCA (eller t-SNE, MDS, etc.) på varje dataset separat, och sedan manuellt jämföra de olika projektionerna för att utforska om det finns intressanta likheter och skillnader mellan dataset. Kontrastiv PCA (cPCA) är utformad för att fylla denna lucka i datautforskning och visualisering genom att automatiskt identifiera de projektioner som uppvisar de mest intressanta skillnaderna mellan dataset. Figur1 ger en översikt över cPCA som vi förklarar mer detaljerat längre fram. Fig. 1 Schematisk översikt av cPCA. För att utföra cPCA, beräkna kovariansmatriserna CX , CY för mål- och bakgrundsdatamängderna. De singulära vektorerna av den viktade skillnaden av kovariansmatriserna, CX − α· CY , är riktningarna som returneras av cPCA. Som visas i spridningsdiagrammet till höger, identifierar PCA (på måldata) den riktning som har den högsta variansen i måldata, medan cPCA identifierar den riktning som har en högre varians i måldata jämfört med bakgrundsdata. Att projicera måldata på den senare riktningen ger mönster unika för måldata och avslöjar ofta strukturer som missas av PCA. Specifikt, i detta exempel, skulle minskning av måldatans dimensioner med cPCA avslöja två distinkta kluster 

 cPCA är motiverad av ett brett spektrum av problem över discipliner. För illustration nämner vi två sådana problem här och demonstrerar andra genom experiment senare i artikeln. Först, överväg en dataset av genuttrycksmätningar från individer av olika etniciteter och kön. Denna data inkluderar genuttrycksnivåer hos cancerpatienter {x i}, som vi är intresserade av att analysera. Vi har också kontrolldata, som motsvarar genuttrycksnivåerna hos friska patienter {y i} från en liknande demografisk bakgrund. Vårt mål är att hitta trender och variationer inom cancerpatienter (t.ex. för att identifiera molekylära subtyper av cancer). Om vi direkt tillämpar PCA på {x i}, kan de främsta huvudkomponenterna dock motsvara de demografiska variationerna hos individerna istället för subtyperna av cancer eftersom de genetiska variationerna på grund av de förra sannolikt är större än de senare. Vi närmar oss detta problem genom att notera att de friska patienterna också innehåller variationen associerad med demografiska skillnader, men inte variationen som motsvarar subtyper av cancer. Således kan vi söka efter komponenter där {x i} har hög varians men {y i} har låg varians.

 Som ett relaterat exempel, överväg en dataset {x i} som består av handskrivna siffror på en komplex bakgrund, såsom olika bilder av gräs (se Fig.2(a), top ). Målet med en typisk oövervakad inlärningsuppgift kan vara att klustra data, vilket avslöjar de olika siffrorna i bilden. Men om vi tillämpar standard-PCA på dessa bilder, finner vi att de främsta huvudkomponenterna inte representerar funktioner relaterade till de handskrivna siffrorna, utan reflekterar den dominerande variationen i funktioner relaterade till bildbakgrunden (Fig.2(b) , top). Vi visar att det är möjligt att korrigera för detta genom att använda en referensdataset {y i} som enbart består av bilder av gräset (inte nödvändigtvis samma bilder som används i {x i} men med liknande kovarians mellan funktioner, som visas i Fig.2(a) , bottom), och leta efter det delrum med högre varians i {x i} jämfört med {y i}. Genom att projicera på detta delrum kan vi faktiskt visuellt separera bilderna baserat på värdet av den handskrivna siffran (Fig. 2(b), bottom). Genom att jämföra de huvudkomponenter som upptäckts av PCA med de som upptäckts av cPCA, ser vi att cPCA identifierar mer relevanta funktioner (Fig.2(c) ), vilket tillåter oss att använda cPCA för sådana tillämpningar som funktionsval och brusreducering. Fig. 2 Kontrastiv PCA på brusiga siffror. a, Topp: Vi skapar en måldatamängd av 5 000 syntetiska bilder genom att slumpmässigt överlagra bilder av handskrivna siffror 0 och 1 från MNIST-datamängdenovanpå bilder av gräs tagna från ImageNet-datamängdentillhörande synset gräs. Bilderna av gräs konverteras till gråskala, ändras till storleken 100 × 100 och beskärs sedan slumpmässigt till samma storlek som MNIST-siffrorna, 28 × 28. b, Topp: Här plottar vi resultatet av att inbädda de syntetiska bilderna på deras två första huvudkomponenter med standard-PCA. Vi ser att punkterna som motsvarar bilderna med 0:or och bilderna med 1:or är svåra att skilja åt. a, Botten: En bakgrundsdatamängd introduceras sedan bestående enbart av bilder av gräs tillhörande samma synset, men vi använder bilder som är olika de som användes för att skapa måldatamängden. b, Botten: Genom att använda cPCA på mål- och bakgrundsdatamängderna (med ett värde av kontrastparametern αsatt till 2,0), framträder två kluster i den lägre dimensionella representationen av måldatamängden, ett bestående av bilder med siffran 0 och det andra av bilder med siffran 1. cVi tittar på det relativa bidraget av varje pixel till den första huvudkomponenten (PC) och den första kontrastiva huvudkomponenten (cPC). Vitare pixlar är de som bär en mer positiv vikt, medan mörkare anger de pixlar som bär negativa vikter. PCA tenderar att betona pixlar i bildens periferi och något avbetona pixlar i mitten och botten av bilden, vilket indikerar att det mesta av variansen beror på bakgrundsfunktioner. Å andra sidan tenderar cPCA att öka vikten på pixlarna som är vid platsen för de handskrivna 1:orna, negativt vikta pixlar vid platsen för de handskrivna 0:orna och försumma de flesta andra pixlar, vilket effektivt upptäcker de funktioner som är användbara för att skilja mellan de överlagrade siffrorna 

 Kontrastiv PCA är ett verktyg för oövervakad inlärning, som effektivt reducerar dimensioner för att möjliggöra visualisering och utforskande dataanalys. Detta skiljer cPCA från en stor klass av övervakade inlärningsmetoder vars primära mål är att klassificera eller diskriminera mellan olika dataset, såsom linjär diskriminantanalys (LDA), kvadratisk diskriminantanalys (QDA), övervakad PCA och QUADRO. Detta skiljer också cPCA från metoder som integrerar flera dataset, med målet att identifiera korrelerade mönster bland två eller fler dataset, snarare än de som är unika för varje enskild dataset. Det finns också en rik familj av oövervakade metoder för dimensionsreduktion förutom PCA. Till exempel, multidimensionell skalning (MDS) finner en lågdimensionell inbäddning som bevarar avståndet i det högdimensionella utrymmet; huvudkomponentjakt finner ett lågrankigt delrum som är robust mot små punktvisa brus och grova glesa fel. Men ingen är utformad för att utnyttja relevant information från en andra dataset, som cPCA gör. I supplementet har vi jämfört cPCA med många av de tidigare nämnda teknikerna på representativa dataset (se Supplementära Figurer3 och4 ).

 I ett specifikt tillämpningsområde kan det finnas specialiserade verktyg i det området med liknande mål som cPCA. Till exempel, i resultaten, visar vi hur cPCA tillämpad på genotypdata visualiserar geografisk härkomst inom Mexiko. Att utforska finkorniga kluster av genetiska härkomster är ett viktigt problem inom populationsgenetik, och forskare har nyligen utvecklat en algoritm för att specifikt visualisera sådana härkomstkluster. Medan cPCA presterar bra här, kan den expertutformade algoritmen prestera ännu bättre för en specifik dataset. Dock kräver den specialiserade algoritmen omfattande domänkunskap för att designa, är mer beräkningsmässigt kostsam och kan vara utmanande att använda. Målet med cPCA är inte att ersätta alla dessa specialiserade toppmoderna metoder i var och en av deras domäner, utan att tillhandahålla en allmän metod för att utforska godtyckliga dataset.

 Vi föreslår en konkret och effektiv algoritm för cPCA i denna artikel. Metoden tar som indata en måldataset {x i} som vi är intresserade av att visualisera eller identifiera mönster inom. Som en sekundär indata tar cPCA en bakgrundsdataset {y i}, som inte innehåller de intressanta mönstren. cPCA-algoritmen returnerar delrum som fångar en stor mängd variation i måldatan {x i}, men lite i bakgrunden {y i} (se Fig.1 , Metoder, och Supplementära Metoder för mer detaljer). Detta delrum motsvarar funktioner som innehåller struktur specifik för {x i}. Därför, när måldatan projiceras på detta delrum, kan vi visualisera och upptäcka den ytterligare strukturen i måldatan relativt till bakgrunden. Analogt med huvudkomponenterna (PCs), kallar vi de riktningar som hittas av cPCA för de kontrastiva huvudkomponenterna (cPCs). Vi betonar att cPCA är fundamentalt en oövervakad teknik, utformad för att lösa mönster i en dataset tydligare genom att använda bakgrundsdatasetet som en kontrast. I synnerhet söker cPCA inte att diskriminera mellan mål- och bakgrundsdataset; det delrum som innehåller trender som är berikade i måldatasetet är inte nödvändigtvis samma delrum som är optimalt för klassificering mellan dataseten.

Forskare har noterat att standard-PCA ofta är ineffektiv vid upptäckten av undergrupper inom biologiska data, åtminstone delvis eftersom "dominerande huvudkomponenter...korrelerar med artefakter," snarare än med funktioner som är av intresse för forskaren. Hur kan cPCA användas i dessa inställningar för att upptäcka de mer betydande undergrupperna? Genom att använda en bakgrundsdataset för att avbryta den universella men ointressanta variationen i målet, kan vi söka efter struktur som är unik för måldatasetet. 

Vårt första experiment använder en dataset bestående av proteinuttrycksmätningar av möss som har fått chockterapi. Några av mössen har utvecklat Downs syndrom (DS). För att skapa en oövervakad inlärningsuppgift där vi har grundläggande sanning för att utvärdera metoderna, antar vi att denna DS-information inte är känd för analytikern och använder den endast för algoritmutvärdering. Vi skulle vilja se om vi upptäcker några betydande skillnader inom den chockade muspopulationen på ett oövervakat sätt (närvaron eller frånvaron av Downs syndrom är ett nyckel exempel). I Fig.3a (top), visar vi resultatet av att tillämpa PCA på måldatasetet: den transformerade datan avslöjar inga betydande kluster inom muspopulationen. De huvudsakliga källorna till variation inom möss kan vara naturliga, såsom kön eller ålder. Fig. 3Upptäcka undergrupper i biologiska data. aVi använder PCA för att projicera en proteinuttrycksdatamängd av möss med och utan Downs syndrom (DS) på de två första komponenterna. Den lägre dimensionella representationen av proteinuttrycksmätningar från möss med och utan DS ses vara distribuerade på liknande sätt (topp). Men när vi använder cPCA för att projicera datamängden på dess två första cPC:er, upptäcker vi en lägre dimensionell representation som klustrar möss med och utan DS separat (botten). bVidare använder vi PCA och cPCA för att visualisera en högdimensionell enkelcells-RNA-Seq-datamängd i två dimensioner. Datamängden består av fyra cellprover från två leukemipatienter: ett pre-transplantationsprov från patient 1, ett post-transplantationsprov från patient 1, ett pre-transplantationsprov från patient 2 och ett post-transplantationsprov från patient 2. b, vänster: Resultaten med endast proverna från patient 1, som visar att cPCA (botten) mer effektivt separerar proverna än PCA (topp). När proverna från den andra patienten inkluderas, i b, höger, är återigen cPCA (botten) mer effektiv än PCA (topp) på att separera proverna, även om post-transplantationscellerna från båda patienterna är liknande distribuerade. Vi visar plottar av varje prov separat i Supplementary Fig. 5, där det är lättare att se överlappningen mellan olika prover

Vi tillämpar cPCA på denna dataset med en bakgrund bestående av proteinuttrycksmätningar från en uppsättning möss som inte har utsatts för chockterapi. De är kontrollmöss som sannolikt har liknande naturlig variation som de experimentella mössen, men utan de skillnader som resulterar från chockterapin. Med denna dataset som bakgrund kan cPCA lösa två olika grupper i den transformerade måldatan, en motsvarande möss som inte har Downs syndrom och en motsvarande (mestadels) möss som har Downs syndrom, som illustreras i Fig.3a (bottom). Som en jämförelse tillämpade vi också 8 andra dimensionsreduktionstekniker för att identifiera riktningar som skiljer mellan mål- och bakgrundsdataset, ingen av vilka kunde separera mössen lika bra som cPCA (se Supplementär Fig.4 för detaljer). 

Nästa, analyserar vi en högdimensionell offentlig dataset bestående av enkelcells-RNA-uttrycksnivåer av en blandning av benmärgsmononukleära celler (BMMCs) tagna från en leukemipatient före stamcellstransplantation och BMMCs från samma patient efter stamcellstransplantation. All enkelcells-RNA-Seq-data förbehandlas med liknande metoder som beskrivs av författarna. I synnerhet, innan vi tillämpar PCA eller cPCA, reduceras alla dataset till 500 gener, som väljs på basis av högsta spridning [varians dividerad med medelvärde] inom måldatan. Återigen utför vi PCA för att se om vi visuellt kan upptäcka de två proverna i den transformerade datan. Som visas i Fig.3b (top left), följer båda celltyperna en liknande fördelning i det utrymme som spänns av de två första PC:erna. Detta beror sannolikt på att skillnaderna mellan proverna är små och PC:erna istället reflekterar heterogeniteten hos olika typer av celler inom varje prov eller till och med variationer i experimentella förhållanden, vilket kan ha en betydande effekt på enkelcells-RNA-Seq-mätningar. 

Vi tillämpar cPCA med en bakgrundsdatamängd som består av RNA-Seq-mätningar från en frisk individs BMMC-celler. Vi förväntar oss att denna bakgrundsdatamängd innehåller variationen på grund av den heterogena populationen av celler samt variationer i experimentella förhållanden. Vi kan hoppas att cPCA kan återhämta riktningar som är berikade i måldatan, motsvarande skillnader före och efter transplantation. Det är precis vad vi finner, som visas i Fig.3b (nere till vänster). 

Vi utökar vår måldatamängd med BMMC-prover från en andra leukemipatient, återigen före och efter stamcellstransplantation. Således finns det totalt fyra subpopulationer av celler. Tillämpning av PCA på dessa data visar att de fyra subpopulationerna inte är separerbara i det delrum som spänns av de två främsta huvudkomponenterna (PCs), som visas i Fig.3b (uppe till höger). Återigen, när cPCA tillämpas med samma bakgrundsdatamängd, visar åtminstone tre av subpopulationerna en mycket starkare separation, som visas i Fig.3b (nere till höger). cPCA-inbäddningen antyder också att cellproverna från båda patienterna är mer lika varandra efter stamcellstransplantationen (cyan och gröna punkter) än före transplantationen (guld och rosa punkter), en rimlig hypotes som kan testas av forskaren. Man kan hänvisa till Supplementary Fig.5 för mer detaljer om detta experiment. Vi ser att cPCA kan vara ett användbart verktyg för att dra slutsatser om relationen mellan subpopulationer, ett ämne vi utforskar vidare härnäst. 

I tidigare exempel har vi sett att cPCA tillåter användaren att upptäcka underklasser inom en måldatamängd som inte är märkta i förväg. Men även när underklasser är kända i förväg kan dimensionsreduktion vara ett användbart sätt att visualisera relationen inom grupper. Till exempel används PCA ofta för att visualisera relationen mellan etniska populationer baserat på genetiska varianter, eftersom projicering av de genetiska varianterna på två dimensioner ofta ger kartor som erbjuder slående visualiseringar av geografiska och historiska trender. Men återigen är PCA begränsad till att identifiera den mest dominerande strukturen; när detta representerar universell eller ointressant variation kan cPCA vara mer effektivt för att visualisera trender. 

Datamängden som vi använder för detta exempel består av enkel-nukleotid-polymorfismer (SNPs) från genomerna av individer från fem delstater i Mexiko, insamlade i en tidigare studie. Mexikanskt ursprung är utmanande att analysera med PCA eftersom PCs vanligtvis inte reflekterar geografiskt ursprung inom Mexiko; istället reflekterar de andelen europeiskt/ursprungsamerikanskt arv hos varje mexikansk individ, vilket dominerar och döljer skillnader på grund av geografiskt ursprung inom Mexiko (se Fig.4a ). För att övervinna detta problem beskär populationsgenetiker manuellt SNPs, tar bort de som är kända för att härstamma från europeiskt ursprung, innan de tillämpar PCA. Men denna procedur är av begränsad tillämplighet eftersom den kräver att man känner till ursprunget för SNPs och att källan till bakgrundsvariationen är mycket annorlunda än den intressanta variationen, vilket ofta inte är fallet. Fig. 4Förhållande mellan mexikanska härkomstgrupper. aPCA tillämpad på genetiska data från individer från 5 mexikanska delstater avslöjar inga visuellt urskiljbara mönster i de inbäddade data. bcPCA tillämpad på samma datamängd avslöjar mönster i data: individer från samma delstat är klustrade närmare varandra i cPCA-inbäddningen. cVidare avslöjar fördelningen av punkterna relationer mellan grupperna som matchar den geografiska platsen för de olika delstaterna: till exempel är individer från geografiskt angränsande delstater angränsande i inbäddningen. cAnpassad från en karta över Mexiko som ursprungligen är arbetet av Användare:Allstrak på Wikipedia, publicerad under en CC-BY-SA-licens, hämtad från https://commons.wikimedia.org/wiki/File:Mexico_Map.svg 

Som ett alternativ använder vi cPCA med en bakgrundsdatamängd som består av individer från Mexiko och från Europa. Denna bakgrund domineras av ursprungsamerikansk/europeisk variation, vilket tillåter oss att isolera den intra-mexikanska variationen i måldatamängden. Resultaten av att tillämpa cPCA visas i Fig.4b. Vi finner att individer från samma delstat i Mexiko är inbäddade närmare varandra. Dessutom är de två grupperna som är mest olika Sonorans och Mayans från Yucatan, som också är de mest geografiskt avlägsna inom Mexiko, medan mexikaner från de andra tre delstaterna är nära varandra, både geografiskt och i inbäddningen fångad av cPCA (se Fig.4c ). Se även Supplementary Fig.6 för mer detaljer. 

 I många datavetenskapliga sammanhang är vi intresserade av att visualisera och utforska mönster som är berikade i en datamängd i förhållande till annan data. Vi har presenterat cPCA som ett allmänt verktyg för att utföra sådan kontrastiv utforskning, och vi har illustrerat dess användbarhet i en mängd olika tillämpningar. De huvudsakliga fördelarna med cPCA är dess allmängiltighet och användarvänlighet. Att beräkna en specifik cPCA tar i princip samma tid som att beräkna en vanlig PCA. Denna beräkningsmässiga effektivitet gör cPCA användbar för interaktiv datautforskning, där varje operation helst bör vara nästan omedelbar. Således kan cPCA tillämpas i alla sammanhang där PCA används på relaterade datamängder. I Supplementary Note3 och Supplementary Fig.8 visar vi hur cPCA kan kerneliseras för att avslöja icke-linjära kontrastiva mönster i datamängder.

 Den enda fria parametern för kontrastiv PCA är kontraststyrkanα. I vår standardalgoritm har vi utvecklat ett automatiskt schema baserat på klustringar av delrum för att välja de mest informativa värdena avα (se Metoder). Alla experiment som utförts för denna artikel använder de automatiskt genereradeα -värdena, och vi tror att denna standard kommer att vara tillräcklig i många tillämpningar av cPCA. Användaren kan också ange specifika värden förα om mer detaljerad utforskning önskas.

 cPCA, liksom vanlig PCA och andra metoder för dimensionsreduktion, ger intep -värden eller andra statistiska signifikanskvantifieringar. Mönstren som upptäcks genom cPCA måste valideras genom hypotesprövning eller ytterligare analys med relevant domänkunskap. Vi har släppt koden för cPCA som ett python-paket tillsammans med dokumentation och exempel.

För dend -dimensionella måldatan (equation) och bakgrundsdata (equation) , låtC X,C Yvara deras motsvarande empiriska kovariansmatriser. Låt (equation) vara mängden av enhetsvektorer. För någon riktning (equation) , kan variansen den står för i måldatan och i bakgrundsdatan skrivas som: (equation) Givet en kontrastparameterα ≥ 0 som kvantifierar avvägningen mellan att ha hög måldatavarians och låg bakgrundsdatavarians, beräknar cPCA den kontrastiva riktningenv * genom att optimera (equation) Detta problem kan skrivas om som (equation) vilket innebär attv * motsvarar den första egenvektorn av matrisen (equation). Därför kan de kontrastiva riktningarna beräknas effektivt med hjälp av egenvärdesdekomposition. Analogt med PCA kallar vi de ledande egenvektorerna avC för de kontrastiva huvudkomponenterna (cPCs). Vi noterar att cPCs är egenvektorer av matrisenC och är därför ortogonala mot varandra. För en fastα beräknar vi (1 ) och returnerar delrummet som spänns av de första få (vanligtvis två) cPCs. 

Kontrastparameternα representerar avvägningen mellan att ha hög måldatavarians och låg bakgrundsdatavarians. Närα = 0 väljer cPCA de riktningar som endast maximerar måldatavariansen, och reduceras därmed till PCA tillämpad på måldatan {x i}. Närα ökar blir riktningar med mindre bakgrundsdatavarians viktigare och cPCs drivs mot nollrummet av bakgrundsdatan {y i}. I det begränsande falletα = ∞ får alla riktningar som inte ligger i nollrummet av {y i} en oändlig straff. I detta fall motsvarar cPCA först att projicera måldatan på nollrummet av bakgrundsdatan, och sedan utföra PCA på den projicerade datan. 

Istället för att välja en endaα och returnera dess delrum, beräknar cPCA delrummen för en lista avα och returnerar några delrum som är långt ifrån varandra i termer av huvudvinkeln. Genom att projicera data på vart och ett av dessa delrum kommer olika trender inom måldatan att avslöjas, och genom att visuellt undersöka de spridningsdiagram som returneras kan användaren snabbt urskilja det relevanta delrummet (och motsvarande värde avα ) för sin analys. Se Supplementary Fig.1 för ett detaljerat exempel. 

Den fullständiga algoritmen för cPCA beskrivs i Algoritm 2 (Supplementary Methods). Vi ställer vanligtvis in listan över potentiella värden avα till 40 värden logaritmiskt fördelade mellan 0,1 och 1000 och detta används för alla experiment i artikeln. För att välja de representativa delrummen använder cPCA spektralklustering för att klustra delrummen, där affiniteten definieras som produkten av cosinus för huvudvinklarna mellan delrummen. Sedan används medoid (representant) för varje kluster som värdena avα för att generera de spridningsdiagram som användaren ser. 

Valet av bakgrundsdatamängden har stort inflytande på resultatet av cPCA. Generellt bör bakgrundsdata ha den struktur som vi vill ta bort från måldatan. Sådan struktur motsvarar vanligtvis riktningar i målet med hög varians, men som inte är av intresse för analytikern. 

Vi ger några allmänna exempel på bakgrundsdatamängder som kan ge användbara kontraster till måldata: (1) En kontrollgrupp {y i} kontrasterad med en sjuk population {x i} eftersom kontrollgruppen innehåller liknande variation på populationsnivå men inte den subtila variationen på grund av olika subtyper av sjukdomen. (2) Data vid tidpunkt noll {y i} används för att kontrastera mot data vid en senare tidpunkt {x i}. Detta möjliggör visualiseringar av de mest framträdande förändringarna över tid. (3) En homogen grupp {y i} kontrasterad med en blandad grupp {x i} eftersom båda har variation inom populationen och mätbrus, men den förra har inte variation mellan populationer. (4) En förbehandlingsdatamängd {y i} kontrasterad med efterbehandlingsdata {x i} för att ta bort mätbrus men bevara variationer orsakade av behandlingen. (5) En uppsättning signalfria inspelningar {y i} eller bilder som endast innehåller brus, kontrasterade med mätningar {x i} som består av både signal och brus. 

Det är värt att tillägga att bakgrundsdata inte behöver ha exakt samma kovariansstruktur som det vi vill ta bort från måldatamängden. Som ett exempel, i experimentet som visas i Fig.2 , visar det sig att vi inte behöver använda en bakgrundsdatamängd som består av bilder av gräs. Faktum är att liknande resultat erhålls även om istället för bilder av gräs, bilder av himlen används som bakgrundsdatamängd. Eftersom strukturen av kovariansmatriserna är tillräckligt lika, tar cPCA bort bakgrundsstrukturen från måldatan. Dessutom kräver cPCA inte att måldatan och bakgrundsdatan har ett liknande antal prover. Eftersom kovariansmatriserna beräknas oberoende, kräver cPCA endast att de empiriska kovariansmatriserna är bra uppskattningar av de underliggande populationskovariansmatriserna, i princip samma krav som PCA. 

Här diskuterar vi den geometriska tolkningen av cPCA samt dess statistiska egenskaper. Först är det intressant att överväga vilka riktningar som är "bättre" för syftet med kontrastanalys. För en riktning (equation) , bestäms dess betydelse i cPCA helt av dess mål-bakgrundsvarianspar (λ X(v ),λ Y(v )); det är önskvärt att ha en högre målvarians och en lägre bakgrundsvarians. Baserat på denna intuition kan vi vidare definiera en partiell ordning av kontrast för olika riktningar: för två riktningarv1 ochv2 , kan vi säga attv1 är en bättre kontrastiv riktning om den har en högre målvarians och en lägre bakgrundsvarians. I detta fall skulle mål-bakgrundsvariansparet förv1 ligga på den nedre högra sidan av den förv2 i diagrammet över mål-bakgrundsvarianspar (λ X(v ),λ Y(v )), t.ex., Fig.5. Baserat på denna partiella ordning kan mängden av de mest kontrastiva riktningarna definieras på ett liknande sätt som definitionen av Paretofronten. Låt (equation) vara mängden av mål-bakgrundsvarianspar för alla riktningar, dvs. (equation). Mängden av de mest kontrastiva riktningarna motsvarar den nedre högra gränsen av (equation) i diagrammet över mål-bakgrundsvarianspar, som visas i Fig.5. (För det särskilda fallet av samtidigt diagonaliserbara bakgrunds- och målmatriser, se Supplementary Fig.7.) Fig. 5Geometrisk tolkning av cPCA. Mängden av mål-bakgrundsvarianspar(equation)är plottad som det teal-färgade området för några slumpmässigt genererade mål- och bakgrundsdata. Den nedre högra gränsen, färgad i guld, motsvarar mängden av de mest kontrastiva riktningarna(equation). De blå trianglarna är variansparen för de cPC:er som valts med α-värden 0,92 respektive 0,29. Vi noterar att de motsvarar tangenspunkterna för den gyllene kurvan och tangentlinjerna med lutning(equation)= 1,08, 3,37, respektive

Angående cPCA kan vi bevisa (se Supplementary Note2 ) att genom att varieraα , är mängden av de bästa cPC:erna identisk med mängden av de mest kontrastiva riktningarna. Dessutom, för riktningenv vald av cPCA med kontrastparametern inställd påα , motsvarar dess varianspar (λ X(v ),λ Y(v )) tangenspunkten för den nedre högra gränsen av (equation) med en lutning-1/α linje. Som ett resultat, genom att varieraα från noll till oändlighet, väljer cPCA riktningar med varianspar som rör sig från den nedre vänstra änden till den övre högra änden av den nedre högra gränsen av (equation). 

Vi noterar också att angående slumpmässigheten i datan, är konvergenshastigheten för prov-cPC till populations-cPC (equation) under milda antaganden, därd är dimensionen ochn ,m är storlekarna på mål- och bakgrundsdatan. Denna hastighet är liknande den standardkonvergenshastighet för provets egenvektor för en kovariansmatris. Se Supplementary Note2. 

Vi har släppt en Python-implementation av kontrastiv PCA på GitHub (https://github.com/abidlabs/contrastive ). GitHub-förvaret inkluderar också Python-notebooks och datamängder som återskapar de flesta av figurerna i denna artikel och i Supplementary Information. 

Datamängder som har använts för att utvärdera kontrastiv PCA i denna artikel är antingen tillgängliga från oss eller från författarna av de ursprungliga studierna. Vänligen se GitHub-förvaret listat i föregående avsnitt för de datamängder som vi har släppt. 

