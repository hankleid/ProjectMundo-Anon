Explorando padrões enriquecidos em um conjunto de dados com análise de componentes principais contrastiva

Resumo: A visualização e exploração de dados de alta dimensão é um desafio ubíquo em várias disciplinas. Técnicas amplamente utilizadas, como a análise de componentes principais (PCA), visam identificar tendências dominantes em um conjunto de dados. No entanto, em muitos contextos, temos conjuntos de dados coletados sob diferentes condições, por exemplo, um experimento de tratamento e controle, e estamos interessados em visualizar e explorar padrões específicos de um conjunto de dados. Este artigo propõe um método, a análise de componentes principais contrastiva (cPCA), que identifica estruturas de baixa dimensão que são enriquecidas em um conjunto de dados em relação aos dados de comparação. Em uma ampla variedade de experimentos, demonstramos que a cPCA com um conjunto de dados de fundo nos permite visualizar padrões específicos do conjunto de dados que são perdidos pela PCA e outros métodos padrão. Além disso, fornecemos uma interpretação geométrica da cPCA e fortes garantias matemáticas. Uma implementação da cPCA está disponível publicamente e pode ser usada para análise exploratória de dados em muitas aplicações onde a PCA é atualmente utilizada.

A análise de componentes principais (PCA) é um dos métodos mais amplamente utilizados para exploração e visualização de dados. PCA projeta os dados em um espaço de baixa dimensionalidade e é especialmente poderosa como uma abordagem para visualizar padrões, como agrupamentos, clinas e outliers em um conjunto de dados. Existe um grande número de métodos de visualização relacionados; por exemplo, t-SNE e escalonamento multidimensional (MDS) permitem projeções de dados não lineares e podem capturar melhor padrões não lineares do que PCA. No entanto, todos esses métodos são projetados para explorar um conjunto de dados por vez. Quando o analista possui múltiplos conjuntos de dados (ou múltiplas condições em um conjunto de dados para comparar), o estado atual da prática é realizar PCA (ou t-SNE, MDS, etc.) em cada conjunto de dados separadamente e, em seguida, comparar manualmente as várias projeções para explorar se há semelhanças e diferenças interessantes entre os conjuntos de dados,. A PCA contrastiva (cPCA) é projetada para preencher essa lacuna na exploração e visualização de dados, identificando automaticamente as projeções que exibem as diferenças mais interessantes entre os conjuntos de dados. A Figura[1]fornece uma visão geral da cPCA que explicamos com mais detalhes adiante.

A cPCA é motivada por uma ampla gama de problemas em várias disciplinas. Para ilustração, mencionamos dois desses problemas aqui e demonstramos outros por meio de experimentos mais adiante no artigo. Primeiro, considere um conjunto de dados de medições de expressão gênica de indivíduos de diferentes etnias e sexos. Esses dados incluem níveis de expressão gênica de pacientes com câncer {x i}, que estamos interessados em analisar. Também temos dados de controle, que correspondem aos níveis de expressão gênica de pacientes saudáveis {y i} de um contexto demográfico semelhante. Nosso objetivo é encontrar tendências e variações dentro dos pacientes com câncer (por exemplo, identificar subtipos moleculares de câncer). Se aplicarmos diretamente PCA a {x i}, no entanto, os principais componentes principais podem corresponder às variações demográficas dos indivíduos em vez dos subtipos de câncer, porque as variações genéticas devido ao primeiro são provavelmente maiores do que as do último. Abordamos esse problema observando que os pacientes saudáveis também contêm a variação associada às diferenças demográficas, mas não a variação correspondente aos subtipos de câncer. Assim, podemos procurar componentes nos quais {x i} tenha alta variância, mas {y i} tenha baixa variância.

Como um exemplo relacionado, considere um conjunto de dados {x i} que consiste em dígitos manuscritos em um fundo complexo, como diferentes imagens de grama (veja Fig.[2(a), topo]). O objetivo de uma tarefa típica de aprendizado não supervisionado pode ser agrupar os dados, revelando os diferentes dígitos na imagem. No entanto, se aplicarmos PCA padrão nessas imagens, descobrimos que os principais componentes principais não representam características relacionadas aos dígitos manuscritos, mas refletem a variação dominante em características relacionadas ao fundo da imagem (Fig.[2(b)], topo). Mostramos que é possível corrigir isso usando um conjunto de dados de referência {y i} que consiste apenas em imagens de grama (não necessariamente as mesmas imagens usadas em {x i}, mas com covariância semelhante entre características, como mostrado na Fig.[2(a)], inferior), e procurando o subespaço de maior variância em {x i} em comparação com {y i}. Ao projetar neste subespaço, podemos realmente separar visualmente as imagens com base no valor do dígito manuscrito (Fig. 2(b), inferior). Ao comparar os componentes principais descobertos por PCA com aqueles descobertos por cPCA, vemos que cPCA identifica características mais relevantes (Fig.[2(c)]), o que nos permite usar cPCA para aplicações como seleção de características e redução de ruído.

A PCA contrastiva é uma ferramenta para aprendizado não supervisionado, que reduz eficientemente a dimensionalidade para permitir a visualização e análise exploratória de dados. Isso separa a cPCA de uma grande classe de métodos de aprendizado supervisionado cujo objetivo principal é classificar ou discriminar entre vários conjuntos de dados, como análise discriminante linear (LDA), análise discriminante quadrática (QDA), PCA supervisionada e QUADRO. Isso também distingue a cPCA de métodos que integram múltiplos conjuntos de dados, com o objetivo de identificar padrões correlacionados entre dois ou mais conjuntos de dados, em vez daqueles únicos para cada conjunto de dados individual. Há também uma rica família de métodos não supervisionados para redução de dimensionalidade além da PCA. Por exemplo, o escalonamento multidimensional (MDS) encontra uma incorporação de baixa dimensionalidade que preserva a distância no espaço de alta dimensionalidade; a busca de componentes principais encontra um subespaço de baixa classificação que é robusto a pequenos ruídos de entrada e erros esparsos grosseiros. Mas nenhum é projetado para utilizar informações relevantes de um segundo conjunto de dados, como a cPCA faz. No suplemento, comparamos a cPCA com muitas das técnicas mencionadas anteriormente em conjuntos de dados representativos (veja Figuras Suplementares[3]e[4]), , ,.

Em um domínio de aplicação específico, pode haver ferramentas especializadas nesse domínio com objetivos semelhantes aos da cPCA, ,. Por exemplo, nos resultados, mostramos como a cPCA aplicada a dados de genótipos visualiza a ancestralidade geográfica dentro do México. Explorar agrupamentos detalhados de ancestralidades genéticas é um problema importante em genética de populações, e pesquisadores recentemente desenvolveram um algoritmo para visualizar especificamente esses agrupamentos de ancestralidade. Embora a cPCA funcione bem aqui, o algoritmo elaborado por especialistas pode funcionar ainda melhor para um conjunto de dados específico. No entanto, o algoritmo especializado requer um conhecimento substancial do domínio para ser projetado, é mais caro computacionalmente e pode ser desafiador de usar. O objetivo da cPCA não é substituir todos esses métodos especializados de ponta em cada um de seus domínios, mas fornecer um método geral para explorar conjuntos de dados arbitrários.

Propomos um algoritmo concreto e eficiente para cPCA neste artigo. O método recebe como entrada um conjunto de dados alvo {x i} que estamos interessados em visualizar ou identificar padrões. Como entrada secundária, a cPCA recebe um conjunto de dados de fundo {y i}, que não contém os padrões de interesse. O algoritmo cPCA retorna subespaços que capturam uma grande quantidade de variação nos dados alvo {x i}, mas pouca nos dados de fundo {y i} (veja Fig.[1], Métodos, e Métodos Suplementares para mais detalhes). Este subespaço corresponde a características que contêm estrutura específica de {x i}. Assim, quando os dados alvo são projetados neste subespaço, somos capazes de visualizar e descobrir a estrutura adicional nos dados alvo em relação ao fundo. Análogo aos componentes principais (PCs), chamamos as direções encontradas pela cPCA de componentes principais contrastivos (cPCs). Enfatizamos que a cPCA é fundamentalmente uma técnica não supervisionada, projetada para resolver padrões em um conjunto de dados de forma mais clara usando o conjunto de dados de fundo como contraste. Em particular, a cPCA não busca discriminar entre os conjuntos de dados alvo e de fundo; o subespaço que contém tendências enriquecidas no conjunto de dados alvo não é necessariamente o mesmo subespaço que é ótimo para classificação entre os conjuntos de dados.

Pesquisadores notaram que a PCA padrão é frequentemente ineficaz em descobrir subgrupos dentro de dados biológicos, pelo menos em parte porque "componentes principais dominantes...correlacionam-se com artefatos", em vez de com características que são de interesse para o pesquisador. Como a cPCA pode ser usada nesses contextos para detectar os subgrupos mais significativos? Usando um conjunto de dados de fundo para cancelar a variação universal, mas não interessante, no alvo, podemos buscar uma estrutura que seja única para o conjunto de dados alvo.

Nosso primeiro experimento usa um conjunto de dados consistindo em medições de expressão de proteínas de camundongos que receberam terapia de choque,. Alguns dos camundongos desenvolveram Síndrome de Down (SD). Para criar uma tarefa de aprendizado não supervisionado onde temos informações de verdade fundamental para avaliar os métodos, assumimos que essa informação de SD não é conhecida pelo analista e a usamos apenas para avaliação do algoritmo. Gostaríamos de ver se detectamos diferenças significativas dentro da população de camundongos chocados de maneira não supervisionada (a presença ou ausência de Síndrome de Down sendo um exemplo chave). Na Fig. [3a] (topo), mostramos o resultado de aplicar PCA ao conjunto de dados alvo: os dados transformados não revelam nenhum agrupamento significativo dentro da população de camundongos. As principais fontes de variação dentro dos camundongos podem ser naturais, como sexo ou idade.

Aplicamos cPCA a este conjunto de dados usando um fundo que consiste em medições de expressão de proteínas de um conjunto de camundongos que não foram expostos à terapia de choque. Eles são camundongos de controle que provavelmente têm variação natural semelhante aos camundongos experimentais, mas sem as diferenças que resultam da terapia de choque. Com este conjunto de dados como fundo, a cPCA é capaz de resolver dois grupos diferentes nos dados alvo transformados, um correspondente a camundongos que não têm Síndrome de Down e outro correspondente (principalmente) a camundongos que têm Síndrome de Down, como ilustrado na Fig. [3a] (inferior). Como comparação, também aplicamos outras 8 técnicas de redução de dimensionalidade para identificar direções que diferenciam entre os conjuntos de dados alvo e de fundo, nenhuma das quais foi capaz de separar os camundongos tão bem quanto a cPCA (veja a Figura Suplementar [4] para detalhes).

Em seguida, analisamos um conjunto de dados público de alta dimensionalidade consistindo em níveis de expressão de RNA de célula única de uma mistura de células mononucleares da medula óssea (BMMCs) retiradas de um paciente com leucemia antes do transplante de células-tronco e BMMCs do mesmo paciente após o transplante de células-tronco. Todos os dados de RNA-Seq de célula única são pré-processados usando métodos semelhantes aos descritos pelos autores. Em particular, antes de aplicar PCA ou cPCA, todos os conjuntos de dados são reduzidos para 500 genes, que são selecionados com base na maior dispersão [variância dividida pela média] dentro dos dados alvo. Novamente, realizamos PCA para ver se podemos descobrir visualmente as duas amostras nos dados transformados. Como mostrado na Fig. [3b] (canto superior esquerdo), ambos os tipos de células seguem uma distribuição semelhante no espaço abrangido pelos dois primeiros PCs. Isso provavelmente ocorre porque as diferenças entre as amostras são pequenas e os PCs refletem, em vez disso, a heterogeneidade de vários tipos de células dentro de cada amostra ou até mesmo variações nas condições experimentais, que podem ter um efeito significativo nas medições de RNA-Seq de célula única.

Aplicamos cPCA usando um conjunto de dados de fundo que consiste em medições de RNA-Seq de células BMMC de um indivíduo saudável. Esperamos que este conjunto de dados de fundo contenha a variação devido à população heterogênea de células, bem como variações nas condições experimentais. Podemos esperar, então, que o cPCA possa recuperar direções que são enriquecidas nos dados-alvo, correspondendo às diferenças pré e pós-transplante. De fato, é isso que encontramos, como mostrado na Fig. [3b] (inferior esquerdo).

Aumentamos ainda mais nosso conjunto de dados-alvo com amostras de BMMC de um segundo paciente com leucemia, novamente antes e depois do transplante de células-tronco. Assim, há um total de quatro subpopulações de células. A aplicação de PCA nesses dados mostra que as quatro subpopulações não são separáveis no subespaço abrangido pelos dois principais componentes principais (PCs), como mostrado na Fig. [3b] (superior direito). Novamente, no entanto, quando cPCA é aplicado com o mesmo conjunto de dados de fundo, pelo menos três das subpopulações mostram uma separação muito mais forte, como mostrado na Fig. [3b] (inferior direito). A incorporação de cPCA também sugere que as amostras de células de ambos os pacientes são mais semelhantes entre si após o transplante de células-tronco (pontos ciano e verde) do que antes do transplante (pontos dourado e rosa), uma hipótese razoável que pode ser testada pelo investigador. Pode-se consultar a Fig. Suplementar [5] para mais detalhes deste experimento. Vemos que cPCA pode ser uma ferramenta útil para inferir a relação entre subpopulações, um tópico que exploramos mais adiante.

Em exemplos anteriores, vimos que cPCA permite ao usuário descobrir subclasses dentro de um conjunto de dados-alvo que não são rotuladas a priori. No entanto, mesmo quando subclasses são conhecidas antecipadamente, a redução de dimensionalidade pode ser uma maneira útil de visualizar a relação dentro dos grupos. Por exemplo, PCA é frequentemente usado para visualizar a relação entre populações étnicas com base em variantes genéticas, porque projetar as variantes genéticas em duas dimensões frequentemente produz mapas que oferecem visualizações impressionantes de tendências geográficas e históricas,. Mas novamente, PCA é limitado a identificar a estrutura mais dominante; quando isso representa variação universal ou desinteressante, cPCA pode ser mais eficaz na visualização de tendências.

O conjunto de dados que usamos para este exemplo consiste em polimorfismos de nucleotídeo único (SNPs) dos genomas de indivíduos de cinco estados do México, coletados em um estudo anterior. A ancestralidade mexicana é desafiadora de analisar usando PCA, pois os PCs geralmente não refletem a origem geográfica dentro do México; em vez disso, refletem a proporção de herança europeia/nativa americana de cada indivíduo mexicano, o que domina e obscurece as diferenças devido à origem geográfica dentro do México (veja a Fig. [4a] ). Para superar esse problema, geneticistas de populações podam manualmente os SNPs, removendo aqueles conhecidos por derivar de ancestralidade europeia, antes de aplicar PCA. No entanto, este procedimento tem aplicabilidade limitada, pois requer saber a origem dos SNPs e que a fonte de variação de fundo seja muito diferente da variação de interesse, o que muitas vezes não é o caso.

Como alternativa, usamos cPCA com um conjunto de dados de fundo que consiste em indivíduos do México e da Europa. Este fundo é dominado pela variação nativa americana/europeia, permitindo-nos isolar a variação intra-mexicana no conjunto de dados-alvo. Os resultados da aplicação de cPCA são mostrados na Fig. [4b]. Descobrimos que indivíduos do mesmo estado no México estão embutidos mais próximos uns dos outros. Além disso, os dois grupos que são mais divergentes são os Sonorans e os Maias de Yucatán, que também são os mais distantes geograficamente dentro do México, enquanto os mexicanos dos outros três estados estão próximos uns dos outros, tanto geograficamente quanto na incorporação capturada por cPCA (veja a Fig. [4c] ). Veja também a Fig. Suplementar [6] para mais detalhes.

Em muitos contextos de ciência de dados, estamos interessados em visualizar e explorar padrões que são enriquecidos em um conjunto de dados em relação a outros dados. Apresentamos cPCA como uma ferramenta geral para realizar tal exploração contrastiva, e ilustramos sua utilidade em uma ampla gama de aplicações. As principais vantagens do cPCA são sua generalidade e facilidade de uso. Calcular um cPCA específico leva essencialmente o mesmo tempo que calcular um PCA regular. Essa eficiência computacional permite que cPCA seja útil para exploração interativa de dados, onde cada operação deve idealmente ser quase imediata. Assim, em qualquer configuração onde PCA é aplicado em conjuntos de dados relacionados, cPCA também pode ser aplicado. Na Nota Suplementar[3]e na Fig. Suplementar[8], mostramos como cPCA pode ser kernelizado para descobrir padrões contrastivos não lineares em conjuntos de dados.

O único parâmetro livre do PCA contrastivo é a força do contrasteα. Em nosso algoritmo padrão, desenvolvemos um esquema automático baseado em agrupamentos de subespaços para selecionar os valores mais informativos deα (veja Métodos). Todos os experimentos realizados para este artigo usam os valores deα gerados automaticamente, e acreditamos que esse padrão será suficiente em muitas aplicações de cPCA. O usuário também pode inserir valores específicos paraα se desejar uma exploração mais detalhada.

cPCA, como PCA regular e outros métodos de redução de dimensionalidade, não fornece valores dep ou outras quantificações de significância estatística. Os padrões descobertos através de cPCA precisam ser validados por meio de testes de hipótese ou análise adicional usando conhecimento relevante do domínio. Lançamos o código para cPCA como um pacote Python junto com documentação e exemplos.

Para os dados-alvod -dimensionaisxi ∈ Rd e dados de fundoyi ∈ Rd , sejamC X,C Ysuas matrizes de covariância empíricas correspondentes. SejaR unit d = def v ∈ Rd :v2 = 1 seja o conjunto de vetores unitários. Para qualquer direçãov ∈ R unit d , a variância que ela explica nos dados-alvo e nos dados de fundo pode ser escrita como: Variância dos dados alvo :λX ( v ) =defvTCXv , Variância dos dados de fundo :λY ( v ) =defvTCYv.Dado um parâmetro de contrasteα ≥ 0 que quantifica o equilíbrio entre ter alta variância no alvo e baixa variância no fundo, cPCA calcula a direção contrastivav * otimizando 1 v * = argmax v ∈ Runitd λ X(v)- α λ Y(v).  Este problema pode ser reescrito comov * = argmax v ∈ Runitd v T CX - α CY v ,  o que implica quev * corresponde ao primeiro autovetor da matrizC = def CX - α CY. Assim, as direções contrastivas podem ser eficientemente calculadas usando decomposição de autovalores. Análogo ao PCA, chamamos os autovetores principais deC de componentes principais contrastivos (cPCs). Notamos que os cPCs são autovetores da matrizC e, portanto, são ortogonais entre si. Para umα fixo, calculamos ( [1] ) e retornamos o subespaço abrangido pelos primeiros poucos (tipicamente dois) cPCs.

O parâmetro de contrasteα representa o equilíbrio entre ter alta variância no alvo e baixa variância no fundo. Quandoα = 0, cPCA seleciona as direções que apenas maximizam a variância do alvo, e assim se reduz ao PCA aplicado nos dados-alvo {x i}. À medida queα aumenta, direções com menor variância de fundo tornam-se mais importantes e os cPCs são direcionados para o espaço nulo dos dados de fundo {y i}. No caso limiteα = ∞, qualquer direção que não esteja no espaço nulo de {y i} recebe uma penalidade infinita. Neste caso, cPCA corresponde a primeiro projetar os dados-alvo no espaço nulo dos dados de fundo, e então realizar PCA nos dados projetados.

Em vez de escolher um únicoα e retornar seu subespaço, o cPCA calcula os subespaços de uma lista deα 's e retorna alguns subespaços que estão distantes uns dos outros em termos do ângulo principal. Projetar os dados em cada um desses subespaços revelará diferentes tendências dentro dos dados-alvo, e ao examinar visualmente os gráficos de dispersão que são retornados, o usuário pode rapidamente discernir o subespaço relevante (e o valor correspondente deα ) para sua análise. Veja a Fig. Suplementar [1] para um exemplo detalhado.

O algoritmo completo do cPCA é descrito no Algoritmo 2 (Métodos Suplementares). Normalmente, definimos a lista de valores potenciais deα para ser 40 valores espaçados logaritmicamente entre 0,1 e 1000, e isso é usado para todos os experimentos no artigo. Para selecionar os subespaços representativos, o cPCA usa agrupamento espectral para agrupar os subespaços, onde a afinidade é definida como o produto do cosseno dos ângulos principais entre os subespaços. Em seguida, os medoids (representantes) de cada grupo são usados como os valores deα para gerar os gráficos de dispersão vistos pelo usuário.

A escolha do conjunto de dados de fundo tem uma grande influência no resultado do cPCA. Em geral, os dados de fundo devem ter a estrutura que gostaríamos de remover dos dados-alvo. Tal estrutura geralmente corresponde a direções no alvo com alta variância, mas que não são de interesse para o analista.

Fornecemos alguns exemplos gerais de conjuntos de dados de fundo que podem fornecer contrastes úteis para os dados-alvo: (1) Um grupo de controle {y i} contrastado com uma população doente {x i} porque o grupo de controle contém variação em nível populacional semelhante, mas não a variação sutil devido a diferentes subtipos da doença. (2) Os dados no tempo zero {y i} usados para contrastar com dados em um ponto de tempo posterior {x i}. Isso permite visualizações das mudanças mais salientes ao longo do tempo. (3) Um grupo homogêneo {y i} contrastado com um grupo misto {x i} porque ambos têm variação intra-populacional e ruído de medição, mas o primeiro não tem variação inter-populacional. (4) Um conjunto de dados pré-tratamento {y i} contrastado com dados pós-tratamento {x i} para remover o ruído de medição, mas preservar as variações causadas pelo tratamento. (5) Um conjunto de gravações sem sinal {y i} ou imagens que contêm apenas ruído, contrastadas com medições {x i} que consistem em sinal e ruído.

Vale a pena acrescentar que os dados de fundo não precisam ter exatamente a mesma estrutura de covariância que gostaríamos de remover do conjunto de dados-alvo. Como exemplo, no experimento mostrado na Fig. [2] , verifica-se que não precisamos usar um conjunto de dados de fundo que consiste em imagens de grama. De fato, resultados semelhantes são obtidos mesmo se, em vez de imagens de grama, imagens do céu forem usadas como conjunto de dados de fundo. Como a estrutura das matrizes de covariância é suficientemente semelhante, o cPCA remove a estrutura de fundo dos dados-alvo. Além disso, o cPCA não requer que os dados-alvo e os dados de fundo tenham um número semelhante de amostras. Como as matrizes de covariância são calculadas independentemente, o cPCA apenas requer que as matrizes de covariância empíricas sejam boas estimativas das matrizes de covariância da população subjacente, essencialmente o mesmo requisito do PCA.

Aqui, discutimos a interpretação geométrica do cPCA, bem como suas propriedades estatísticas. Primeiro, é interessante considerar quais direções são "melhores" para o propósito de análise contrastiva. Para uma direçãov ∈ R unit d , sua significância no cPCA é totalmente determinada por seu par de variância alvo-fundo (λ X(v ),λ Y(v )); é desejável ter uma variância alvo mais alta e uma variância de fundo mais baixa. Com base nessa intuição, podemos definir ainda uma ordem parcial de contrastividade para várias direções: para duas direçõesv1 ev2 , podemos dizer quev1 é uma melhor direção contrastiva se tiver uma variância alvo mais alta e uma variância de fundo mais baixa. Nesse caso, o par de variância alvo-fundo dev1 estaria no lado inferior direito do dev2 no gráfico de pares de variância alvo-fundo (λ X(v ),λ Y(v )), por exemplo, Fig. [5]. Com base nessa ordem parcial, o conjunto de direções mais contrastivas pode ser definido de maneira semelhante à definição da fronteira de Pareto. SejaU  o conjunto de pares de variância alvo-fundo para todas as direções, ou seja,U = def  ( λX ( v ) ,λY ( v ))  v ∈ Runitd. O conjunto de direções mais contrastivas corresponde à fronteira inferior direita deU  no gráfico de pares de variância alvo-fundo, como mostrado na Fig. [5]. (Para o caso particular de matrizes de fundo e alvo simultaneamente diagonalizáveis, veja a Fig. Suplementar [7].)

Em relação ao cPCA, podemos provar (veja a Nota Suplementar [2] ) que, ao variarα , o conjunto de principais cPC's é idêntico ao conjunto de direções mais contrastivas. Além disso, para a direçãov selecionada pelo cPCA com o parâmetro de contraste definido paraα , seu par de variância (λ X(v ),λ Y(v )) corresponde ao ponto de tangência da fronteira inferior direita deU  com uma linha de inclinação -1/α. Como resultado, ao variarα de zero a infinito, o cPCA seleciona direções com pares de variância que viajam do extremo inferior esquerdo ao extremo superior direito da fronteira inferior direita deU .

Também observamos que, em relação à aleatoriedade dos dados, a taxa de convergência do cPC amostral para o cPC populacional éO p  dmin ( n,m ) sob suposições leves, onded é a dimensão en ,m são os tamanhos dos dados-alvo e de fundo. Esta taxa é semelhante à taxa de convergência padrão do autovetor amostral para uma matriz de covariância. Veja a Nota Suplementar [2].

Lançamos uma implementação em Python do PCA contrastivo no GitHub (https://github.com/abidlabs/contrastive ). O repositório do GitHub também inclui notebooks Python e conjuntos de dados que reproduzem a maioria das figuras deste artigo e nas Informações Suplementares.

Os conjuntos de dados que foram usados para avaliar o PCA contrastivo neste artigo estão disponíveis conosco ou com os autores dos estudos originais. Por favor, consulte o repositório do GitHub listado na seção anterior para os conjuntos de dados que lançamos.

Fig. 1: Visão Esquemática do cPCA. Para realizar o cPCA, calcule as matrizes de covariância C X, C Ydos conjuntos de dados alvo e de fundo. Os vetores singulares da diferença ponderada das matrizes de covariância, C X− α · C Y, são as direções retornadas pelo cPCA. Como mostrado no gráfico de dispersão à direita, a PCA (nos dados alvo) identifica a direção que tem a maior variância nos dados alvo, enquanto o cPCA identifica a direção que tem uma variância maior nos dados alvo em comparação com os dados de fundo. Projetar os dados alvo nesta última direção fornece padrões únicos para os dados alvo e frequentemente revela estruturas que são perdidas pela PCA. Especificamente, neste exemplo, reduzir a dimensionalidade dos dados alvo pelo cPCA revelaria dois clusters distintos

Fig. 2: PCA Contrastiva em Dígitos Ruidosos.a, Topo: Criamos um conjunto de dados alvo de 5.000 imagens sintéticas sobrepondo aleatoriamente imagens de dígitos manuscritos 0 e 1 do conjunto de dados MNIST sobre imagens de grama retiradas do conjunto de dados ImageNet pertencentes ao synset grama. As imagens de grama são convertidas para escala de cinza, redimensionadas para 100 × 100, e então recortadas aleatoriamente para ter o mesmo tamanho dos dígitos MNIST, 28 × 28.b, Topo: Aqui, plotamos o resultado de embutir as imagens sintéticas em seus dois primeiros componentes principais usando a PCA padrão. Vemos que os pontos correspondentes às imagens com 0's e imagens com 1's são difíceis de distinguir.a, Inferior: Um conjunto de dados de fundo é então introduzido consistindo apenas de imagens de grama pertencentes ao mesmo synset, mas usamos imagens que são diferentes daquelas usadas para criar o conjunto de dados alvo.b, Inferior: Usando cPCA nos conjuntos de dados alvo e de fundo (com um valor do parâmetro de contraste α definido para 2.0), dois clusters emergem na representação de menor dimensão do conjunto de dados alvo, um consistindo de imagens com o dígito 0 e o outro de imagens com o dígito 1.cObservamos a contribuição relativa de cada pixel para o primeiro componente principal (PC) e o primeiro componente principal contrastivo (cPC). Pixels mais claros são aqueles que carregam um peso mais positivo, enquanto os mais escuros denotam aqueles pixels que carregam pesos negativos. A PCA tende a enfatizar pixels na periferia da imagem e desvalorizar ligeiramente pixels no centro e na parte inferior da imagem, indicando que a maior parte da variância é devida a características de fundo. Por outro lado, o cPCA tende a aumentar o peso dos pixels que estão na localização dos 1's manuscritos, pesar negativamente os pixels na localização dos 0's manuscritos, e negligenciar a maioria dos outros pixels, descobrindo efetivamente aquelas características úteis para discriminar entre os dígitos sobrepostos

Fig. 3: Descobrindo subgrupos em dados biológicos. a Usamos PCA para projetar um conjunto de dados de expressão proteica de camundongos com e sem Síndrome de Down (SD) nos dois primeiros componentes. A representação de menor dimensão das medições de expressão proteica de camundongos com e sem SD são vistas como distribuídas de forma semelhante (topo). Mas, quando usamos cPCA para projetar o conjunto de dados em seus dois primeiros cPCs, descobrimos uma representação de menor dimensão que agrupa camundongos com e sem SD separadamente (inferior). b Além disso, usamos PCA e cPCA para visualizar um conjunto de dados de RNA-Seq de célula única de alta dimensão em duas dimensões. O conjunto de dados consiste em quatro amostras de células de dois pacientes com leucemia: uma amostra pré-transplante do paciente 1, uma amostra pós-transplante do paciente 1, uma amostra pré-transplante do paciente 2, e uma amostra pós-transplante do paciente 2. b , esquerda: Os resultados usando apenas as amostras do paciente 1, que demonstram que o cPCA (inferior) separa mais efetivamente as amostras do que a PCA (topo). Quando as amostras do segundo paciente são incluídas, em b , direita, novamente o cPCA (inferior) é mais eficaz do que a PCA (topo) em separar as amostras, embora as células pós-transplante de ambos os pacientes estejam distribuídas de forma semelhante. Mostramos gráficos de cada amostra separadamente na Fig. Suplementar[5] , onde é mais fácil ver a sobreposição entre diferentes amostras

Fig. 4: Relação entre grupos de ancestralidade mexicana. a PCA aplicada a dados genéticos de indivíduos de 5 estados mexicanos não revela quaisquer padrões visualmente discerníveis nos dados embutidos. b cPCA aplicado ao mesmo conjunto de dados revela padrões nos dados: indivíduos do mesmo estado estão agrupados mais próximos no embutimento do cPCA. c Além disso, a distribuição dos pontos revela relações entre os grupos que correspondem à localização geográfica dos diferentes estados: por exemplo, indivíduos de estados geograficamente adjacentes estão adjacentes no embutimento. c Adaptado de um mapa do México que é originalmente obra do Usuário:Allstrak na Wikipedia, publicado sob uma licença CC-BY-SA, fonte https://commons.wikimedia.org/wiki/File:Mexico_Map.svg

Fig. 5: Interpretação Geométrica do cPCA. O conjunto de pares de variância alvo-fundoUé plotado como a região azul-petróleo para alguns dados alvo e de fundo gerados aleatoriamente. A fronteira inferior-direita, colorida em dourado, corresponde ao conjunto de direções mais contrastantesSλ. Os triângulos azuis são os pares de variância para os cPCs selecionados com valores de α 0.92 e 0.29, respectivamente. Notamos que eles correspondem aos pontos de tangência da curva dourada e das linhas tangentes com inclinação1α= 1.08, 3.37, respectivamente

