Erforschung von Mustern, die in einem Datensatz mit kontrastiver Hauptkomponentenanalyse angereichert sind 

ZusammenfassungDie Visualisierung und Erforschung hochdimensionaler Daten ist eine allgegenwärtige Herausforderung in verschiedenen Disziplinen. Weit verbreitete Techniken wie die Hauptkomponentenanalyse (PCA) zielen darauf ab, dominante Trends in einem Datensatz zu identifizieren. In vielen Fällen haben wir jedoch Datensätze, die unter unterschiedlichen Bedingungen gesammelt wurden, z.B. ein Behandlungs- und ein Kontrollexperiment, und wir sind daran interessiert, Muster zu visualisieren und zu erforschen, die spezifisch für einen Datensatz sind. Dieses Papier schlägt eine Methode vor, die kontrastive Hauptkomponentenanalyse (cPCA), die niedrigdimensionale Strukturen identifiziert, die in einem Datensatz im Vergleich zu Vergleichsdaten angereichert sind. In einer Vielzahl von Experimenten zeigen wir, dass cPCA mit einem Hintergrunddatensatz es uns ermöglicht, datensatzspezifische Muster zu visualisieren, die von PCA und anderen Standardmethoden übersehen werden. Wir bieten außerdem eine geometrische Interpretation von cPCA und starke mathematische Garantien. Eine Implementierung von cPCA ist öffentlich verfügbar und kann für explorative Datenanalysen in vielen Anwendungen verwendet werden, in denen derzeit PCA eingesetzt wird. 

 Die Hauptkomponentenanalyse (PCA) ist eine der am häufigsten verwendeten Methoden zur Datenexploration und -visualisierung. PCA projiziert die Daten in einen niedrigdimensionalen Raum und ist besonders leistungsfähig, um Muster wie Cluster, Klinale und Ausreißer in einem Datensatz zu visualisieren. Es gibt eine große Anzahl verwandter Visualisierungsmethoden; zum Beispiel erlauben t-SNE und multidimensionale Skalierung (MDS) nichtlineare Datenprojektionen und können nichtlineare Muster besser erfassen als PCA. Dennoch sind all diese Methoden darauf ausgelegt, jeweils einen Datensatz zu erkunden. Wenn der Analyst mehrere Datensätze (oder mehrere Bedingungen in einem Datensatz zum Vergleich) hat, besteht die derzeitige Praxis darin, PCA (oder t-SNE, MDS usw.) auf jeden Datensatz separat anzuwenden und dann die verschiedenen Projektionen manuell zu vergleichen, um zu erkunden, ob es interessante Ähnlichkeiten und Unterschiede zwischen den Datensätzen gibt. Die kontrastive PCA (cPCA) ist darauf ausgelegt, diese Lücke in der Datenexploration und -visualisierung zu schließen, indem sie automatisch die Projektionen identifiziert, die die interessantesten Unterschiede zwischen den Datensätzen aufweisen. Fig.1 bietet einen Überblick über cPCA, den wir im Folgenden ausführlicher erklären. Fig. 1 Schematische Übersicht von cPCA. Um cPCA durchzuführen, berechnen Sie die Kovarianzmatrizen CX , CY der Ziel- und Hintergrunddatensätze. Die singulären Vektoren der gewichteten Differenz der Kovarianzmatrizen, CX − α· CY , sind die von cPCA zurückgegebenen Richtungen. Wie im Streudiagramm rechts gezeigt, identifiziert PCA (auf den Zieldaten) die Richtung mit der höchsten Varianz in den Zieldaten, während cPCA die Richtung identifiziert, die eine höhere Varianz in den Zieldaten im Vergleich zu den Hintergrunddaten aufweist. Das Projizieren der Zieldaten auf die letztere Richtung ergibt Muster, die einzigartig für die Zieldaten sind, und offenbart oft Strukturen, die von PCA übersehen werden. Insbesondere in diesem Beispiel würde die Reduzierung der Dimensionalität der Zieldaten durch cPCA zwei unterschiedliche Cluster aufdecken 

 cPCA wird durch eine breite Palette von Problemen in verschiedenen Disziplinen motiviert. Zur Veranschaulichung erwähnen wir hier zwei solcher Probleme und demonstrieren andere durch Experimente später im Artikel. Betrachten Sie zunächst einen Datensatz von Genexpressionsmessungen von Individuen unterschiedlicher Ethnien und Geschlechter. Diese Daten umfassen Genexpressionsniveaus von Krebspatienten {x i}, die wir analysieren möchten. Wir haben auch Kontrolldaten, die den Genexpressionsniveaus gesunder Patienten {y i} aus einem ähnlichen demografischen Hintergrund entsprechen. Unser Ziel ist es, Trends und Variationen innerhalb der Krebspatienten zu finden (z. B. um molekulare Subtypen von Krebs zu identifizieren). Wenn wir direkt PCA auf {x i} anwenden, können die Hauptkomponenten jedoch den demografischen Variationen der Individuen entsprechen, anstatt den Subtypen von Krebs, da die genetischen Variationen aufgrund der ersteren wahrscheinlich größer sind als die der letzteren. Wir nähern uns diesem Problem, indem wir feststellen, dass die gesunden Patienten auch die mit demografischen Unterschieden verbundenen Variationen enthalten, jedoch nicht die Variationen, die den Subtypen von Krebs entsprechen. Daher können wir nach Komponenten suchen, in denen {x i} eine hohe Varianz aufweist, aber {y i} eine niedrige Varianz hat.

 Als ein verwandtes Beispiel betrachten Sie einen Datensatz {x i}, der aus handgeschriebenen Ziffern auf einem komplexen Hintergrund besteht, wie z. B. verschiedenen Bildern von Gras (siehe Fig.2(a), oben ). Das Ziel einer typischen unüberwachten Lernaufgabe könnte darin bestehen, die Daten zu clustern und die verschiedenen Ziffern im Bild zu enthüllen. Wenn wir jedoch Standard-PCA auf diese Bilder anwenden, stellen wir fest, dass die Hauptkomponenten keine Merkmale darstellen, die mit den handgeschriebenen Ziffern zusammenhängen, sondern die dominierende Variation in Merkmalen widerspiegeln, die mit dem Bildhintergrund zusammenhängen (Fig.2(b) , oben). Wir zeigen, dass es möglich ist, dies zu korrigieren, indem ein Referenzdatensatz {y i} verwendet wird, der ausschließlich aus Bildern des Grases besteht (nicht unbedingt die gleichen Bilder, die in {x i} verwendet werden, aber eine ähnliche Kovarianz zwischen den Merkmalen aufweisen, wie in Fig.2(a) , unten gezeigt), und nach dem Unterraum mit höherer Varianz in {x i} im Vergleich zu {y i} suchen. Durch die Projektion auf diesen Unterraum können wir die Bilder tatsächlich visuell basierend auf dem Wert der handgeschriebenen Ziffer trennen (Fig. 2(b), unten). Durch den Vergleich der von PCA entdeckten Hauptkomponenten mit denen, die von cPCA entdeckt wurden, sehen wir, dass cPCA relevantere Merkmale identifiziert (Fig.2(c) ), was es uns ermöglicht, cPCA für Anwendungen wie Merkmalsauswahl und Rauschunterdrückung zu verwenden. Fig. 2 Kontrastive PCA bei verrauschten Ziffern. a, Oben: Wir erstellen einen Zieldatensatz von 5.000 synthetischen Bildern, indem wir zufällig Bilder von handgeschriebenen Ziffern 0 und 1 aus dem MNIST-Datensatzauf Bilder von Gras aus dem ImageNet-Datensatzüberlagern, die zum Synset Gras gehören. Die Grasbilder werden in Graustufen umgewandelt, auf 100 × 100 skaliert und dann zufällig auf die gleiche Größe wie die MNIST-Ziffern, 28 × 28, zugeschnitten. b, Oben: Hier plotten wir das Ergebnis der Einbettung der synthetischen Bilder auf ihre ersten beiden Hauptkomponenten unter Verwendung der Standard-PCA. Wir sehen, dass die Punkte, die den Bildern mit 0 und den Bildern mit 1 entsprechen, schwer zu unterscheiden sind. a, Unten: Ein Hintergrunddatensatz wird dann eingeführt, der ausschließlich aus Bildern von Gras besteht, die zum gleichen Synset gehören, aber wir verwenden Bilder, die sich von denen unterscheiden, die zur Erstellung des Zieldatensatzes verwendet wurden. b, Unten: Durch die Verwendung von cPCA auf den Ziel- und Hintergrunddatensätzen (mit einem Wert des Kontrastparameters αauf 2.0 gesetzt) entstehen zwei Cluster in der niedrigdimensionalen Darstellung des Zieldatensatzes, eines bestehend aus Bildern mit der Ziffer 0 und das andere aus Bildern mit der Ziffer 1. cWir betrachten den relativen Beitrag jedes Pixels zur ersten Hauptkomponente (PC) und zur ersten kontrastiven Hauptkomponente (cPC). Weiße Pixel sind diejenigen, die ein positiveres Gewicht tragen, während dunklere Pixel diejenigen sind, die negative Gewichte tragen. PCA neigt dazu, Pixel am Rand des Bildes zu betonen und Pixel in der Mitte und am unteren Rand des Bildes leicht zu de-emphasieren, was darauf hindeutet, dass die meiste Varianz auf Hintergrundmerkmale zurückzuführen ist. Andererseits neigt cPCA dazu, die Pixel an der Position der handgeschriebenen 1 zu gewichten, Pixel an der Position der handgeschriebenen 0 negativ zu gewichten und die meisten anderen Pixel zu vernachlässigen, wodurch effektiv die Merkmale entdeckt werden, die nützlich sind, um zwischen den überlagerten Ziffern zu unterscheiden 

 Kontrastive PCA ist ein Werkzeug für unüberwachtes Lernen, das die Dimensionalität effizient reduziert, um Visualisierung und explorative Datenanalyse zu ermöglichen. Dies trennt cPCA von einer großen Klasse von überwachten Lernmethoden, deren Hauptziel es ist, zwischen verschiedenen Datensätzen zu klassifizieren oder zu unterscheiden, wie z. B. lineare Diskriminanzanalyse (LDA), quadratische Diskriminanzanalyse (QDA), überwachte PCA und QUADRO. Dies unterscheidet cPCA auch von Methoden, die mehrere Datensätze integrieren, mit dem Ziel, korrelierte Muster zwischen zwei oder mehr Datensätzen zu identifizieren, anstatt solche, die für jeden einzelnen Datensatz einzigartig sind. Es gibt auch eine reiche Familie von unüberwachten Methoden zur Dimensionsreduktion neben PCA. Zum Beispiel findet die multidimensionale Skalierung (MDS) eine niedrigdimensionale Einbettung, die den Abstand im hochdimensionalen Raum bewahrt; die Hauptkomponentenverfolgung findet einen niedrigrangigen Unterraum, der robust gegenüber kleinem eintragsweisem Rauschen und groben spärlichen Fehlern ist. Aber keine ist darauf ausgelegt, relevante Informationen aus einem zweiten Datensatz zu nutzen, wie es cPCA tut. Im Supplement haben wir cPCA mit vielen der zuvor erwähnten Techniken auf repräsentativen Datensätzen verglichen (siehe Supplementäre Figs.3 und4 ).

 In einem spezifischen Anwendungsbereich kann es spezialisierte Werkzeuge in diesem Bereich mit ähnlichen Zielen wie cPCA geben. Zum Beispiel zeigen wir in den Ergebnissen, wie cPCA auf Genotypdaten angewendet wird, um die geografische Abstammung innerhalb Mexikos zu visualisieren. Die Erforschung feinkörniger Cluster genetischer Abstammungen ist ein wichtiges Problem in der Populationsgenetik, und Forscher haben kürzlich einen Algorithmus entwickelt, um solche Abstammungscluster speziell zu visualisieren. Während cPCA hier gut abschneidet, könnte der von Experten entwickelte Algorithmus für einen spezifischen Datensatz noch besser abschneiden. Der spezialisierte Algorithmus erfordert jedoch umfangreiches Fachwissen, um entworfen zu werden, ist rechnerisch aufwendiger und kann schwierig zu verwenden sein. Das Ziel von cPCA ist es nicht, all diese spezialisierten hochmodernen Methoden in jedem ihrer Bereiche zu ersetzen, sondern eine allgemeine Methode zur Erkundung beliebiger Datensätze bereitzustellen.

 Wir schlagen in diesem Artikel einen konkreten und effizienten Algorithmus für cPCA vor. Die Methode nimmt als Eingabe einen Ziel-Datensatz {x i}, den wir visualisieren oder in dem wir Muster identifizieren möchten. Als sekundäre Eingabe nimmt cPCA einen Hintergrunddatensatz {y i}, der die interessierenden Muster nicht enthält. Der cPCA-Algorithmus gibt Unterräume zurück, die eine große Menge an Variation in den Zieldaten {x i} erfassen, aber wenig im Hintergrund {y i} (siehe Fig.1 , Methoden und Supplementäre Methoden für weitere Details). Dieser Unterraum entspricht Merkmalen, die eine Struktur enthalten, die spezifisch für {x i} ist. Daher sind wir in der Lage, die zusätzliche Struktur in den Zieldaten im Vergleich zum Hintergrund zu visualisieren und zu entdecken, wenn die Zieldaten auf diesen Unterraum projiziert werden. Analog zu den Hauptkomponenten (PCs) nennen wir die von cPCA gefundenen Richtungen die kontrastiven Hauptkomponenten (cPCs). Wir betonen, dass cPCA grundsätzlich eine unüberwachte Technik ist, die darauf ausgelegt ist, Muster in einem Datensatz klarer zu lösen, indem der Hintergrunddatensatz als Kontrast verwendet wird. Insbesondere versucht cPCA nicht, zwischen den Ziel- und Hintergrunddatensätzen zu unterscheiden; der Unterraum, der Trends enthält, die im Zieldatensatz angereichert sind, ist nicht notwendigerweise derselbe Unterraum, der für die Klassifikation zwischen den Datensätzen optimal ist.

Forscher haben festgestellt, dass Standard-PCA oft ineffektiv ist, um Untergruppen innerhalb biologischer Daten zu entdecken, zumindest teilweise, weil „dominante Hauptkomponenten… mit Artefakten korrelieren“, anstatt mit Merkmalen, die für den Forscher von Interesse sind. Wie kann cPCA in diesen Einstellungen verwendet werden, um die bedeutenderen Untergruppen zu erkennen? Indem ein Hintergrunddatensatz verwendet wird, um die universelle, aber uninteressante Variation im Ziel zu eliminieren, können wir nach einer Struktur suchen, die einzigartig für den Zieldatensatz ist. 

Unser erstes Experiment verwendet einen Datensatz, der aus Proteinexpressionsmessungen von Mäusen besteht, die einer Schocktherapie unterzogen wurden. Einige der Mäuse haben das Down-Syndrom (DS) entwickelt. Um eine unüberwachte Lernaufgabe zu erstellen, bei der wir über Ground-Truth-Informationen zur Bewertung der Methoden verfügen, nehmen wir an, dass diese DS-Information dem Analysten nicht bekannt ist und verwenden sie nur zur Bewertung des Algorithmus. Wir möchten sehen, ob wir in der Lage sind, signifikante Unterschiede innerhalb der geschockten Mäusepopulation auf unüberwachte Weise zu erkennen (das Vorhandensein oder Fehlen des Down-Syndroms ist ein wichtiges Beispiel). In Fig.3a (oben) zeigen wir das Ergebnis der Anwendung von PCA auf den Zieldatensatz: Die transformierten Daten zeigen keine signifikante Clusterbildung innerhalb der Mäusepopulation. Die Hauptquellen der Variation innerhalb der Mäuse können natürlich sein, wie Geschlecht oder Alter. Fig. 3Entdeckung von Untergruppen in biologischen Daten. aWir verwenden PCA, um einen Proteinexpressionsdatensatz von Mäusen mit und ohne Down-Syndrom (DS) auf die ersten beiden Komponenten zu projizieren. Die niedrigdimensionale Darstellung der Proteinexpressionsmessungen von Mäusen mit und ohne DS scheint ähnlich verteilt zu sein (oben). Aber wenn wir cPCA verwenden, um den Datensatz auf seine ersten beiden cPCs zu projizieren, entdecken wir eine niedrigdimensionale Darstellung, die Mäuse mit und ohne DS separat clustert (unten). bDarüber hinaus verwenden wir PCA und cPCA, um einen hochdimensionalen Einzelzell-RNA-Seq-Datensatz in zwei Dimensionen zu visualisieren. Der Datensatz besteht aus vier Zellproben von zwei Leukämiepatienten: eine Vortransplantationsprobe von Patient 1, eine Nachtransplantationsprobe von Patient 1, eine Vortransplantationsprobe von Patient 2 und eine Nachtransplantationsprobe von Patient 2. b, links: Die Ergebnisse unter Verwendung nur der Proben von Patient 1, die zeigen, dass cPCA (unten) die Proben effektiver trennt als PCA (oben). Wenn die Proben des zweiten Patienten einbezogen werden, in b, rechts, ist cPCA (unten) erneut effektiver als PCA (oben) bei der Trennung der Proben, obwohl die Nachtransplantationszellen beider Patienten ähnlich verteilt sind. Wir zeigen Diagramme jeder Probe separat in Supplementary Fig. 5, wo es einfacher ist, die Überlappung zwischen verschiedenen Proben zu sehen

Wir wenden cPCA auf diesen Datensatz an, indem wir einen Hintergrund verwenden, der aus Proteinexpressionsmessungen von einer Gruppe von Mäusen besteht, die keiner Schocktherapie ausgesetzt waren. Es handelt sich um Kontrollmäuse, die wahrscheinlich eine ähnliche natürliche Variation wie die experimentellen Mäuse aufweisen, jedoch ohne die Unterschiede, die sich aus der Schocktherapie ergeben. Mit diesem Datensatz als Hintergrund ist cPCA in der Lage, zwei verschiedene Gruppen in den transformierten Zieldaten zu lösen, eine, die Mäusen entspricht, die kein Down-Syndrom haben, und eine, die (meistens) Mäusen entspricht, die das Down-Syndrom haben, wie in Fig.3a (unten) dargestellt. Zum Vergleich haben wir auch 8 andere Techniken zur Dimensionsreduktion angewendet, um Richtungen zu identifizieren, die zwischen den Ziel- und Hintergrunddatensätzen unterscheiden, von denen keine in der Lage war, die Mäuse so gut wie cPCA zu trennen (siehe Supplementäre Fig.4 für Details). 

Als nächstes analysieren wir einen höherdimensionalen öffentlichen Datensatz, der aus Einzelzell-RNA-Expressionsniveaus einer Mischung von Knochenmark-Monozytenzellen (BMMCs) besteht, die von einem Leukämiepatienten vor der Stammzelltransplantation und BMMCs vom selben Patienten nach der Stammzelltransplantation entnommen wurden. Alle Einzelzell-RNA-Seq-Daten werden unter Verwendung ähnlicher Methoden wie von den Autoren beschrieben vorverarbeitet. Insbesondere werden vor der Anwendung von PCA oder cPCA alle Datensätze auf 500 Gene reduziert, die auf der Grundlage der höchsten Dispersion [Varianz geteilt durch Mittelwert] innerhalb der Zieldaten ausgewählt werden. Wieder führen wir PCA durch, um zu sehen, ob wir die beiden Proben in den transformierten Daten visuell entdecken können. Wie in Fig.3b (oben links) gezeigt, folgen beide Zelltypen einer ähnlichen Verteilung im Raum, der von den ersten beiden PCs aufgespannt wird. Dies liegt wahrscheinlich daran, dass die Unterschiede zwischen den Proben gering sind und die PCs stattdessen die Heterogenität verschiedener Zellarten innerhalb jeder Probe oder sogar Variationen in den experimentellen Bedingungen widerspiegeln, die einen signifikanten Einfluss auf Einzelzell-RNA-Seq-Messungen haben können. 

Wir wenden cPCA an, indem wir einen Hintergrunddatensatz verwenden, der aus RNA-Seq-Messungen von BMMC-Zellen eines gesunden Individuums besteht. Wir erwarten, dass dieser Hintergrunddatensatz die Variation aufgrund der heterogenen Zellpopulation sowie Variationen in den experimentellen Bedingungen enthält. Wir können hoffen, dass cPCA in der Lage ist, Richtungen wiederherzustellen, die im Ziel-Datensatz angereichert sind und den Unterschieden vor und nach der Transplantation entsprechen. Tatsächlich ist dies das, was wir finden, wie in Fig.3b (unten links) gezeigt. 

Wir erweitern unseren Ziel-Datensatz weiter mit BMMC-Proben von einem zweiten Leukämiepatienten, erneut vor und nach der Stammzelltransplantation. Somit gibt es insgesamt vier Zellunterpopulationen. Die Anwendung von PCA auf diese Daten zeigt, dass die vier Unterpopulationen im durch die beiden Hauptkomponenten (PCs) aufgespannten Unterraum nicht trennbar sind, wie in Fig.3b (oben rechts) gezeigt. Wenn jedoch cPCA mit demselben Hintergrunddatensatz angewendet wird, zeigen mindestens drei der Unterpopulationen eine viel stärkere Trennung, wie in Fig.3b (unten rechts) gezeigt. Die cPCA-Einbettung deutet auch darauf hin, dass die Zellproben beider Patienten nach der Stammzelltransplantation (cyan und grüne Punkte) einander ähnlicher sind als vor der Transplantation (goldene und rosa Punkte), eine vernünftige Hypothese, die vom Forscher getestet werden kann. Weitere Details zu diesem Experiment finden Sie in der ergänzenden Fig.5. Wir sehen, dass cPCA ein nützliches Werkzeug sein kann, um die Beziehung zwischen Unterpopulationen zu ermitteln, ein Thema, das wir als nächstes weiter untersuchen. 

In früheren Beispielen haben wir gesehen, dass cPCA es dem Benutzer ermöglicht, Unterklassen innerhalb eines Ziel-Datensatzes zu entdecken, die nicht a priori gekennzeichnet sind. Selbst wenn Unterklassen im Voraus bekannt sind, kann die Dimensionsreduktion eine nützliche Methode sein, um die Beziehung innerhalb von Gruppen zu visualisieren. Zum Beispiel wird PCA häufig verwendet, um die Beziehung zwischen ethnischen Populationen basierend auf genetischen Varianten zu visualisieren, da die Projektion der genetischen Varianten auf zwei Dimensionen oft Karten erzeugt, die eindrucksvolle Visualisierungen geografischer und historischer Trends bieten. Aber auch hier ist PCA darauf beschränkt, die dominierendste Struktur zu identifizieren; wenn dies universelle oder uninteressante Variation darstellt, kann cPCA effektiver sein, um Trends zu visualisieren. 

Der Datensatz, den wir für dieses Beispiel verwenden, besteht aus Einzelnukleotid-Polymorphismen (SNPs) aus den Genomen von Individuen aus fünf Bundesstaaten in Mexiko, die in einer früheren Studie gesammelt wurden. Die mexikanische Abstammung ist mit PCA schwer zu analysieren, da die PCs normalerweise nicht den geografischen Ursprung innerhalb Mexikos widerspiegeln; stattdessen spiegeln sie den Anteil des europäischen/indigenen Erbes jedes mexikanischen Individuums wider, was die Unterschiede aufgrund des geografischen Ursprungs innerhalb Mexikos dominiert und verschleiert (siehe Fig.4a ). Um dieses Problem zu überwinden, schneiden Populationsgenetiker manuell SNPs, die bekanntermaßen von europäischer Abstammung stammen, bevor sie PCA anwenden. Dieses Verfahren ist jedoch von begrenzter Anwendbarkeit, da es erfordert, die Herkunft der SNPs zu kennen und dass die Quelle der Hintergrundvariation sehr unterschiedlich von der interessierenden Variation ist, was oft nicht der Fall ist. Fig. 4Beziehung zwischen mexikanischen Abstammungsgruppen. aPCA, angewendet auf genetische Daten von Individuen aus 5 mexikanischen Bundesstaaten, zeigt keine visuell erkennbaren Muster in den eingebetteten Daten. bcPCA, angewendet auf denselben Datensatz, zeigt Muster in den Daten: Individuen aus demselben Bundesstaat sind in der cPCA-Einbettung näher beieinander gruppiert. cDarüber hinaus zeigt die Verteilung der Punkte Beziehungen zwischen den Gruppen, die der geografischen Lage der verschiedenen Bundesstaaten entsprechen: Zum Beispiel sind Individuen aus geografisch benachbarten Bundesstaaten in der Einbettung benachbart. cAdaptiert von einer Karte von Mexiko, die ursprünglich von User:Allstrak bei Wikipedia erstellt wurde, veröffentlicht unter einer CC-BY-SA-Lizenz, bezogen von https://commons.wikimedia.org/wiki/File:Mexico_Map.svg 

Als Alternative verwenden wir cPCA mit einem Hintergrunddatensatz, der aus Individuen aus Mexiko und Europa besteht. Dieser Hintergrund wird von der Variation der indigenen/Europäischen Abstammung dominiert, was es uns ermöglicht, die intra-mexikanische Variation im Ziel-Datensatz zu isolieren. Die Ergebnisse der Anwendung von cPCA sind in Fig.4b gezeigt. Wir finden, dass Individuen aus demselben Bundesstaat in Mexiko näher beieinander eingebettet sind. Darüber hinaus sind die beiden Gruppen, die am meisten divergieren, die Sonoraner und die Maya aus Yucatan, die auch geografisch am weitesten innerhalb Mexikos entfernt sind, während Mexikaner aus den anderen drei Bundesstaaten sowohl geografisch als auch in der von cPCA erfassten Einbettung nahe beieinander liegen (siehe Fig.4c ). Siehe auch ergänzende Fig.6 für weitere Details. 

 In vielen Datenwissenschaftsbereichen sind wir daran interessiert, Muster zu visualisieren und zu erforschen, die in einem Datensatz im Vergleich zu anderen Daten angereichert sind. Wir haben cPCA als ein allgemeines Werkzeug für die Durchführung solcher kontrastiven Erkundungen vorgestellt und seine Nützlichkeit in einer Vielzahl von Anwendungen veranschaulicht. Die Hauptvorteile von cPCA sind seine Allgemeinheit und Benutzerfreundlichkeit. Die Berechnung eines bestimmten cPCA dauert im Wesentlichen genauso lange wie die Berechnung eines regulären PCA. Diese rechnerische Effizienz ermöglicht es cPCA, für die interaktive Datenexploration nützlich zu sein, bei der jede Operation idealerweise fast sofort erfolgen sollte. In allen Einstellungen, in denen PCA auf verwandte Datensätze angewendet wird, kann auch cPCA angewendet werden. In der ergänzenden Anmerkung3 und der ergänzenden Fig.8 zeigen wir, wie cPCA kernelisiert werden kann, um nichtlineare kontrastive Muster in Datensätzen aufzudecken.

 Der einzige freie Parameter der kontrastiven PCA ist die Kontraststärkeα. In unserem Standardalgorithmus haben wir ein automatisches Schema entwickelt, das auf der Clusterbildung von Unterräumen basiert, um die informativsten Werte vonα auszuwählen (siehe Methoden). Alle für dieses Papier durchgeführten Experimente verwenden die automatisch generiertenα -Werte, und wir glauben, dass dieser Standard in vielen Anwendungen von cPCA ausreichend sein wird. Der Benutzer kann auch spezifische Werte fürα eingeben, wenn eine detailliertere Erkundung gewünscht wird.

 cPCA, wie reguläre PCA und andere Methoden zur Dimensionsreduktion, liefert keinep -Werte oder andere statistische Signifikanzquantifizierungen. Die durch cPCA entdeckten Muster müssen durch Hypothesentests oder zusätzliche Analysen unter Verwendung relevanten Fachwissens validiert werden. Wir haben den Code für cPCA als Python-Paket zusammen mit Dokumentation und Beispielen veröffentlicht.

Für died -dimensionale Ziel-Daten (equation) und Hintergrunddaten (equation) , seienC X,C Yihre entsprechenden empirischen Kovarianzmatrizen. Sei (equation) die Menge der Einheitsvektoren. Für jede Richtung (equation) 

Der Kontrastparameterα repräsentiert den Kompromiss zwischen hoher Zielvarianz und niedriger Hintergrundvarianz. Wennα = 0, wählt cPCA die Richtungen aus, die nur die Zielvarianz maximieren, und reduziert sich daher auf PCA, die auf die Ziel-Daten {x i} angewendet wird. Wennα zunimmt, werden Richtungen mit kleinerer Hintergrundvarianz wichtiger und die cPCs werden in den Nullraum der Hintergrunddaten {y i} getrieben. Im Grenzfallα = ∞ erhält jede Richtung, die nicht im Nullraum von {y i} liegt, eine unendliche Strafe. In diesem Fall entspricht cPCA dem ersten Projektieren der Ziel-Daten auf den Nullraum der Hintergrunddaten und dann der Durchführung von PCA auf den projizierten Daten. 

Anstatt ein einzelnesα zu wählen und dessen Unterraum zurückzugeben, berechnet cPCA die Unterräume einer Liste vonα -Werten und gibt einige Unterräume zurück, die in Bezug auf den Hauptwinkel weit voneinander entfernt sind. Das Projizieren der Daten auf jeden dieser Unterräume wird unterschiedliche Trends innerhalb der Zieldaten aufzeigen, und durch visuelle Untersuchung der zurückgegebenen Streudiagramme kann der Benutzer schnell den relevanten Unterraum (und den entsprechenden Wert vonα ) für seine Analyse erkennen. Siehe Supplementäre Fig.1 für ein detailliertes Beispiel. 

Der vollständige Algorithmus von cPCA wird in Algorithmus 2 (Supplementäre Methoden) beschrieben. Wir setzen typischerweise die Liste der potenziellen Werte vonα auf 40 Werte, die logarithmisch zwischen 0,1 und 1000 verteilt sind, und dies wird für alle Experimente im Papier verwendet. Um die repräsentativen Unterräume auszuwählen, verwendet cPCA spektrales Clustering, um die Unterräume zu clustern, wobei die Affinität als das Produkt des Kosinus der Hauptwinkel zwischen den Unterräumen definiert ist. Dann werden die Medoide (Repräsentanten) jedes Clusters als die Werte vonα verwendet, um die vom Benutzer gesehenen Streudiagramme zu erzeugen. 

Die Wahl des Hintergrunddatensatzes hat einen großen Einfluss auf das Ergebnis von cPCA. Im Allgemeinen sollten die Hintergrunddaten die Struktur haben, die wir aus den Zieldaten entfernen möchten. Solche Strukturen entsprechen normalerweise Richtungen im Ziel mit hoher Varianz, die jedoch für den Analysten nicht von Interesse sind. 

Wir bieten einige allgemeine Beispiele für Hintergrunddatensätze, die nützliche Kontraste zu Zieldaten bieten können: (1) Eine Kontrollgruppe {y i} im Kontrast zu einer erkrankten Population {x i}, da die Kontrollgruppe ähnliche Populationsvariationen enthält, aber nicht die subtile Variation aufgrund unterschiedlicher Subtypen der Krankheit. (2) Die Daten zum Zeitpunkt null {y i} werden verwendet, um gegen Daten zu einem späteren Zeitpunkt {x i} zu kontrastieren. Dies ermöglicht Visualisierungen der auffälligsten Veränderungen im Laufe der Zeit. (3) Eine homogene Gruppe {y i} im Kontrast zu einer gemischten Gruppe {x i}, da beide intra-populationale Variation und Messrauschen haben, aber die erstere keine inter-populationale Variation hat. (4) Ein Vorbehandlungsdatensatz {y i} im Kontrast zu Nachbehandlungsdaten {x i}, um Messrauschen zu entfernen, aber Variationen durch die Behandlung zu bewahren. (5) Eine Reihe von signalfreien Aufnahmen {y i} oder Bilder, die nur Rauschen enthalten, im Kontrast zu Messungen {x i}, die sowohl Signal als auch Rauschen enthalten. 

Es ist erwähnenswert, dass die Hintergrunddaten nicht genau die gleiche Kovarianzstruktur haben müssen, die wir aus dem Zieldatensatz entfernen möchten. Als Beispiel, im Experiment gezeigt in Fig.2 , stellt sich heraus, dass wir keinen Hintergrunddatensatz verwenden müssen, der aus Bildern von Gras besteht. Tatsächlich werden ähnliche Ergebnisse erzielt, selbst wenn anstelle von Bildern von Gras Bilder des Himmels als Hintergrunddatensatz verwendet werden. Da die Struktur der Kovarianzmatrizen ähnlich genug ist, entfernt cPCA die Hintergrundstruktur aus den Zieldaten. Darüber hinaus erfordert cPCA nicht, dass die Ziel- und Hintergrunddaten eine ähnliche Anzahl von Stichproben haben. Da die Kovarianzmatrizen unabhängig berechnet werden, erfordert cPCA nur, dass die empirischen Kovarianzmatrizen gute Schätzungen der zugrunde liegenden Populationskovarianzmatrizen sind, im Wesentlichen die gleiche Anforderung wie bei PCA. 

Hier diskutieren wir die geometrische Interpretation von cPCA sowie seine statistischen Eigenschaften. Zuerst ist es interessant zu überlegen, welche Richtungen für den Zweck der kontrastiven Analyse "besser" sind. Für eine Richtung (equation) , wird seine Bedeutung in cPCA vollständig durch sein Ziel-Hintergrund-Varianzpaar (λ X(v ),λ Y(v )) bestimmt; es ist wünschenswert, eine höhere Zielvarianz und eine niedrigere Hintergrundvarianz zu haben. Basierend auf dieser Intuition können wir eine partielle Ordnung der Kontrastivität für verschiedene Richtungen weiter definieren: Für zwei Richtungenv1 undv2 könnten wir sagen, dassv1 eine bessere kontrastive Richtung ist, wenn sie eine höhere Zielvarianz und eine niedrigere Hintergrundvarianz hat. In diesem Fall würde das Ziel-Hintergrund-Varianzpaar vonv1 auf der unteren rechten Seite von dem vonv2 im Diagramm der Ziel-Hintergrund-Varianzpaare (λ X(v ),λ Y(v )) liegen, z.B. Fig.5. Basierend auf dieser partiellen Ordnung kann die Menge der kontrastivsten Richtungen in ähnlicher Weise wie die Definition der Pareto-Front definiert werden. Lassen Sie (equation) die Menge der Ziel-Hintergrund-Varianzpaare für alle Richtungen sein, d.h. (equation). Die Menge der kontrastivsten Richtungen entspricht der unteren rechten Grenze von (equation) im Diagramm der Ziel-Hintergrund-Varianzpaare, wie in Fig.5 gezeigt. (Für den speziellen Fall von gleichzeitig diagonalisierbaren Hintergrund- und Zielmatrizen siehe Supplementäre Fig.7.) Fig. 5Geometrische Interpretation von cPCA. Die Menge der Ziel-Hintergrund-Varianzpaare(equation)wird als die türkisfarbene Region für einige zufällig generierte Ziel- und Hintergrunddaten geplottet. Die untere rechte Grenze, in Gold gefärbt, entspricht der Menge der kontrastreichsten Richtungen(equation). Die blauen Dreiecke sind die Varianzpaare für die cPCs, die mit α-Werten von 0.92 und 0.29 ausgewählt wurden. Wir stellen fest, dass sie den Berührungspunkten der goldenen Kurve und den Tangentenlinien mit der Steigung(equation)= 1.08, 3.37, jeweils entsprechen

Bezüglich cPCA können wir beweisen (siehe Supplementäre Anmerkung2 ), dass durch Variieren vonα die Menge der obersten cPCs identisch mit der Menge der kontrastivsten Richtungen ist. Darüber hinaus entspricht für die von cPCA mit dem Kontrastparameterα ausgewählte Richtungv ihr Varianzpaar (λ X(v ),λ Y(v )) dem Berührungspunkt der unteren rechten Grenze von (equation) mit einer Steigung-1/α -Linie. Infolgedessen wählt cPCA durch Variieren vonα von null bis unendlich Richtungen mit Varianzpaaren aus, die von der unteren linken bis zur oberen rechten Grenze der unteren rechten Grenze von (equation). 

Wir bemerken auch, dass in Bezug auf die Zufälligkeit der Daten die Konvergenzrate der Stichproben-cPC zur Populations-cPC (equation) unter milden Annahmen ist, wobeid die Dimension ist undn ,m die Größen der Ziel- und Hintergrunddaten sind. Diese Rate ist ähnlich der Standardkonvergenzrate des Stichprobeneigenvektors für eine Kovarianzmatrix. Siehe Supplementäre Anmerkung2. 

Wir haben eine Python-Implementierung der kontrastiven PCA auf GitHub veröffentlicht (https://github.com/abidlabs/contrastive ). Das GitHub-Repository enthält auch Python-Notebooks und Datensätze, die die meisten der in diesem Papier und in den Supplementären Informationen dargestellten Abbildungen reproduzieren. 

Datensätze, die in diesem Papier zur Bewertung der kontrastiven PCA verwendet wurden, sind entweder von uns oder von den Autoren der Originalstudien erhältlich. Bitte sehen Sie im GitHub-Repository im vorherigen Abschnitt nach den Datensätzen, die wir veröffentlicht haben. 

