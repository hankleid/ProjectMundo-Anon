Udforskning af mønstre beriget i et datasæt med kontrastiv hovedkomponentanalyse

Resumé: Visualisering og udforskning af højdimensionelle data er en udbredt udfordring på tværs af discipliner. Vidt anvendte teknikker som hovedkomponentanalyse (PCA) sigter mod at identificere dominerende tendenser i ét datasæt. Dog har vi i mange sammenhænge datasæt indsamlet under forskellige betingelser, f.eks. en behandling og et kontrolforsøg, og vi er interesserede i at visualisere og udforske mønstre, der er specifikke for ét datasæt. Denne artikel foreslår en metode, kontrastiv hovedkomponentanalyse (cPCA), som identificerer lavdimensionelle strukturer, der er beriget i et datasæt i forhold til sammenligningsdata. I en bred vifte af eksperimenter demonstrerer vi, at cPCA med et baggrundsdatasæt gør det muligt for os at visualisere datasætspecifikke mønstre, som PCA og andre standardmetoder overser. Vi giver yderligere en geometrisk fortolkning af cPCA og stærke matematiske garantier. En implementering af cPCA er offentligt tilgængelig og kan bruges til eksplorativ dataanalyse i mange anvendelser, hvor PCA i øjeblikket anvendes.

Hovedkomponentanalyse (PCA) er en af de mest udbredte metoder til dataudforskning og visualisering. PCA projicerer dataene på et lavdimensionelt rum og er især kraftfuld som en tilgang til at visualisere mønstre, såsom klynger, klines og outliers i et datasæt. Der er et stort antal relaterede visualiseringsmetoder; for eksempel tillader t-SNE og multidimensionel skalering (MDS) ikke-lineære dataprojiceringer og kan bedre fange ikke-lineære mønstre end PCA. Alligevel er alle disse metoder designet til at udforske ét datasæt ad gangen. Når analytikeren har flere datasæt (eller flere betingelser i ét datasæt at sammenligne), er den nuværende praksis at udføre PCA (eller t-SNE, MDS osv.) på hvert datasæt separat og derefter manuelt sammenligne de forskellige projiceringer for at undersøge, om der er interessante ligheder og forskelle på tværs af datasæt,. Kontrastiv PCA (cPCA) er designet til at udfylde dette hul i dataudforskning og visualisering ved automatisk at identificere de projiceringer, der udviser de mest interessante forskelle på tværs af datasæt. Figur[1]giver et overblik over cPCA, som vi forklarer mere detaljeret senere.

cPCA er motiveret af en bred vifte af problemer på tværs af discipliner. Til illustration nævner vi her to sådanne problemer og demonstrerer andre gennem eksperimenter senere i artiklen. Først, overvej et datasæt af genekspressionsmålinger fra individer af forskellige etniciteter og køn. Disse data inkluderer genekspressionsniveauer af kræftpatienter {x i}, som vi er interesserede i at analysere. Vi har også kontroldata, som svarer til genekspressionsniveauerne hos raske patienter {y i} fra en lignende demografisk baggrund. Vores mål er at finde tendenser og variationer inden for kræftpatienter (f.eks. at identificere molekylære subtyper af kræft). Hvis vi direkte anvender PCA på {x i}, kan de øverste hovedkomponenter dog svare til de demografiske variationer af individerne i stedet for subtyperne af kræft, fordi de genetiske variationer på grund af førstnævnte sandsynligvis er større end sidstnævnte. Vi nærmer os dette problem ved at bemærke, at de raske patienter også indeholder variationen forbundet med demografiske forskelle, men ikke variationen svarende til subtyper af kræft. Således kan vi søge efter komponenter, hvor {x i} har høj varians, men {y i} har lav varians.

Som et relateret eksempel, overvej et datasæt {x i} der består af håndskrevne cifre på en kompleks baggrund, såsom forskellige billeder af græs (se Fig.[2(a), top]). Målet med en typisk usuperviseret læringsopgave kan være at klynge dataene, hvilket afslører de forskellige cifre i billedet. Men hvis vi anvender standard PCA på disse billeder, finder vi, at de øverste hovedkomponenter ikke repræsenterer funktioner relateret til de håndskrevne cifre, men afspejler den dominerende variation i funktioner relateret til billedets baggrund (Fig.[2(b)], top). Vi viser, at det er muligt at korrigere for dette ved at bruge et referencedatasæt {y i} der udelukkende består af billeder af græs (ikke nødvendigvis de samme billeder brugt i {x i} men med lignende kovarians mellem funktioner, som vist i Fig.[2(a)], bund), og lede efter det underrum med højere varians i {x i} sammenlignet med {y i}. Ved at projicere på dette underrum kan vi faktisk visuelt adskille billederne baseret på værdien af det håndskrevne ciffer (Fig. 2(b), bund). Ved at sammenligne de hovedkomponenter, der er opdaget af PCA, med dem, der er opdaget af cPCA, ser vi, at cPCA identificerer mere relevante funktioner (Fig.[2(c)]), hvilket tillader os at bruge cPCA til sådanne anvendelser som funktionsvalg og støjreduktion.

Kontrastiv PCA er et værktøj til usuperviseret læring, som effektivt reducerer dimensionalitet for at muliggøre visualisering og eksplorativ dataanalyse. Dette adskiller cPCA fra en stor klasse af superviserede læringsmetoder, hvis primære mål er at klassificere eller skelne mellem forskellige datasæt, såsom lineær diskriminantanalyse (LDA), kvadratisk diskriminantanalyse (QDA), superviseret PCA og QUADRO. Dette adskiller også cPCA fra metoder, der integrerer flere datasæt, med det mål at identificere korrelerede mønstre blandt to eller flere datasæt, snarere end dem, der er unikke for hvert enkelt datasæt. Der er også en rig familie af usuperviseret metoder til dimensionsreduktion udover PCA. For eksempel finder multidimensionel skalering (MDS) en lavdimensionel indlejring, der bevarer afstanden i det højdimensionelle rum; hovedkomponentforfølgelse finder et lav-rang underrum, der er robust over for små indgangsvis støj og grove sparsomme fejl. Men ingen er designet til at udnytte relevant information fra et andet datasæt, som cPCA gør. I supplementet har vi sammenlignet cPCA med mange af de tidligere nævnte teknikker på repræsentative datasæt (se Supplementære Figurer[3]og[4]), , ,.

I et specifikt anvendelsesdomæne kan der være specialiserede værktøjer i det domæne med lignende mål som cPCA, ,. For eksempel viser vi i resultaterne, hvordan cPCA anvendt på genotypedata visualiserer geografisk herkomst inden for Mexico. At udforske finkornede klynger af genetiske herkomster er et vigtigt problem i populationsgenetik, og forskere har for nylig udviklet en algoritme til specifikt at visualisere sådanne herkomstklynger. Mens cPCA fungerer godt her, kan den ekspertudformede algoritme muligvis fungere endnu bedre for et specifikt datasæt. Dog kræver den specialiserede algoritme betydelig domæneviden at designe, er mere beregningsmæssigt krævende og kan være udfordrende at bruge. Målet med cPCA er ikke at erstatte alle disse specialiserede state-of-the-art metoder i hver af deres domæner, men at give en generel metode til at udforske vilkårlige datasæt.

Vi foreslår en konkret og effektiv algoritme for cPCA i denne artikel. Metoden tager som input et måldatasæt {x i} som vi er interesserede i at visualisere eller identificere mønstre inden for. Som en sekundær input tager cPCA et baggrundsdatasæt {y i}, som ikke indeholder de interessante mønstre. cPCA-algoritmen returnerer underrum, der fanger en stor mængde variation i måldataene {x i}, men lidt i baggrunden {y i} (se Fig.[1], Metoder, og Supplementære Metoder for flere detaljer). Dette underrum svarer til funktioner, der indeholder struktur specifik for {x i}. Derfor, når måldataene projiceres på dette underrum, er vi i stand til at visualisere og opdage den yderligere struktur i måldataene i forhold til baggrunden. Analogt til hovedkomponenterne (PC'er) kalder vi de retninger, der findes af cPCA, de kontrastive hovedkomponenter (cPC'er). Vi understreger, at cPCA grundlæggende er en usuperviseret teknik, designet til at løse mønstre i ét datasæt mere klart ved at bruge baggrundsdatasættet som en kontrast. Især søger cPCA ikke at skelne mellem mål- og baggrundsdatasæt; underrummet, der indeholder tendenser, der er beriget i måldatasættet, er ikke nødvendigvis det samme underrum, der er optimalt til klassifikation mellem datasættene.

Forskere har bemærket, at standard PCA ofte er ineffektiv til at opdage undergrupper inden for biologiske data, i det mindste delvist fordi "dominerende hovedkomponenter... korrelerer med artefakter," snarere end med funktioner, der er af interesse for forskeren. Hvordan kan cPCA bruges i disse indstillinger til at opdage de mere betydningsfulde undergrupper? Ved at bruge et baggrundsdatasæt til at annullere den universelle, men uinteressante variation i målet, kan vi søge efter struktur, der er unik for måldatasættet.

Vores første eksperiment bruger et datasæt bestående af proteinudtryksmålinger af mus, der har modtaget chokterapi,. Nogle af musene har udviklet Downs syndrom (DS). For at skabe en usuperviseret læringsopgave, hvor vi har sandhedsinformation til at evaluere metoderne, antager vi, at denne DS-information ikke er kendt af analytikeren og kun bruges til algoritmeevaluering. Vi vil gerne se, om vi kan opdage nogen betydelige forskelle inden for den chokerede musepopulation på en usuperviseret måde (tilstedeværelsen eller fraværet af Downs syndrom er et nøgleeksempel). I Fig. [3a] (top) viser vi resultatet af at anvende PCA på måldatasættet: de transformerede data afslører ikke nogen betydelig klyngedannelse inden for musepopulationen. De største kilder til variation inden for musene kan være naturlige, såsom køn eller alder.

Vi anvender cPCA på dette datasæt ved at bruge en baggrund bestående af proteinudtryksmålinger fra et sæt mus, der ikke har været udsat for chokterapi. De er kontrolmus, der sandsynligvis har lignende naturlig variation som de eksperimentelle mus, men uden de forskelle, der skyldes chokterapien. Med dette datasæt som en baggrund er cPCA i stand til at opløse to forskellige grupper i de transformerede måldata, en svarende til mus, der ikke har Downs syndrom, og en svarende (for det meste) til mus, der har Downs syndrom, som illustreret i Fig. [3a] (bund). Som en sammenligning anvendte vi også 8 andre dimensionsreduktionsteknikker til at identificere retninger, der differentierer mellem mål- og baggrundsdatasæt, hvoraf ingen var i stand til at adskille musene så godt som cPCA (se Supplementær Fig. [4] for detaljer).

Næste analyserer vi et højdimensionelt offentligt datasæt bestående af enkeltcelle RNA-udtryksniveauer af en blanding af knoglemarv mononukleære celler (BMMCs) taget fra en leukæmipatient før stamcelletransplantation og BMMCs fra den samme patient efter stamcelletransplantation. Alle enkeltcelle RNA-Seq data er forbehandlet ved hjælp af lignende metoder som beskrevet af forfatterne. Især før anvendelse af PCA eller cPCA reduceres alle datasæt til 500 gener, som er udvalgt på basis af højeste spredning [varians divideret med gennemsnit] inden for måldataene. Igen udfører vi PCA for at se, om vi visuelt kan opdage de to prøver i de transformerede data. Som vist i Fig. [3b] (øverst til venstre) følger begge celletyper en lignende fordeling i det rum, der spænder over de første to PC'er. Dette er sandsynligvis fordi forskellene mellem prøverne er små, og PC'erne i stedet afspejler heterogeniteten af forskellige slags celler inden for hver prøve eller endda variationer i eksperimentelle betingelser, som kan have en betydelig effekt på enkeltcelle RNA-Seq målinger.

Vi anvender cPCA ved hjælp af et baggrundsdatasæt, der består af RNA-Seq-målinger fra en sund persons BMMC-celler. Vi forventer, at dette baggrundsdatasæt indeholder variationen på grund af den heterogene population af celler samt variationer i eksperimentelle betingelser. Vi håber derfor, at cPCA kan være i stand til at genfinde retninger, der er beriget i måldatasættet, svarende til forskelle før og efter transplantation. Det er faktisk, hvad vi finder, som vist i Fig. [3b] (nederst til venstre).

Vi udvider yderligere vores måldatasæt med BMMC-prøver fra en anden leukæmipatient, igen før og efter stamcelletransplantation. Således er der i alt fire underpopulationer af celler. Anvendelse af PCA på disse data viser, at de fire underpopulationer ikke er adskillelige i det underområde, der spænder over de to øverste hovedkomponenter (PC'er), som vist i Fig. [3b] (øverst til højre). Igen, når cPCA anvendes med det samme baggrundsdatasæt, viser mindst tre af underpopulationerne en meget stærkere adskillelse, som vist i Fig. [3b] (nederst til højre). cPCA-indlejringen antyder også, at celleprøverne fra begge patienter er mere ens efter stamcelletransplantation (cyan og grønne prikker) end før transplantationen (guld og lyserøde prikker), en rimelig hypotese, som kan testes af forskeren. Man kan henvise til Supplementary Fig. [5] for flere detaljer om dette eksperiment. Vi ser, at cPCA kan være et nyttigt værktøj til at udlede forholdet mellem underpopulationer, et emne vi udforsker yderligere næste gang.

I tidligere eksempler har vi set, at cPCA giver brugeren mulighed for at opdage underklasser inden for et måldatasæt, der ikke er mærket på forhånd. Men selv når underklasser er kendt på forhånd, kan dimensionalitetsreduktion være en nyttig måde at visualisere forholdet inden for grupper. For eksempel bruges PCA ofte til at visualisere forholdet mellem etniske populationer baseret på genetiske varianter, fordi projicering af de genetiske varianter på to dimensioner ofte producerer kort, der tilbyder slående visualiseringer af geografiske og historiske tendenser,. Men igen er PCA begrænset til at identificere den mest dominerende struktur; når dette repræsenterer universel eller uinteressant variation, kan cPCA være mere effektiv til at visualisere tendenser.

Datasættet, vi bruger til dette eksempel, består af enkelt-nukleotid-polymorfismer (SNP'er) fra genomerne af individer fra fem stater i Mexico, indsamlet i en tidligere undersøgelse. Mexicansk herkomst er udfordrende at analysere ved hjælp af PCA, da PC'erne normalt ikke afspejler geografisk oprindelse inden for Mexico; i stedet afspejler de andelen af europæisk/indfødt amerikansk arv for hver mexicansk person, hvilket dominerer og skjuler forskelle på grund af geografisk oprindelse inden for Mexico (se Fig. [4a] ). For at overvinde dette problem beskærer populationsgenetikere manuelt SNP'er, fjerner dem, der vides at stamme fra europæisk herkomst, før de anvender PCA. Denne procedure er dog af begrænset anvendelighed, da den kræver kendskab til SNP'ernes oprindelse og at kilden til baggrundsvariation er meget forskellig fra den interessante variation, hvilket ofte ikke er tilfældet.

Som et alternativ bruger vi cPCA med et baggrundsdatasæt, der består af individer fra Mexico og fra Europa. Denne baggrund er domineret af indfødt amerikansk/europæisk variation, hvilket giver os mulighed for at isolere den intra-mexicanske variation i måldatasættet. Resultaterne af anvendelsen af cPCA er vist i Fig. [4b]. Vi finder, at individer fra samme stat i Mexico er indlejret tættere sammen. Desuden er de to grupper, der er mest forskellige, Sonoranerne og Mayaerne fra Yucatan, som også er de mest geografisk fjerne inden for Mexico, mens mexicanere fra de andre tre stater er tæt på hinanden, både geografisk og i indlejringen fanget af cPCA (se Fig. [4c] ). Se også Supplementary Fig. [6] for flere detaljer.

I mange data science-indstillinger er vi interesserede i at visualisere og udforske mønstre, der er beriget i et datasæt i forhold til andre data. Vi har præsenteret cPCA som et generelt værktøj til at udføre sådan kontrastiv udforskning, og vi har illustreret dets anvendelighed i en bred vifte af applikationer. De vigtigste fordele ved cPCA er dets generalitet og brugervenlighed. Beregning af en bestemt cPCA tager stort set den samme tid som beregning af en almindelig PCA. Denne beregningseffektivitet gør cPCA nyttig til interaktiv dataudforskning, hvor hver operation ideelt set skal være næsten øjeblikkelig. Som sådan kan cPCA anvendes i alle indstillinger, hvor PCA anvendes på relaterede datasæt. I Supplementary Note[3]og Supplementary Fig.[8]viser vi, hvordan cPCA kan kerneliseres for at afdække ikke-lineære kontrastmønstre i datasæt.

Den eneste frie parameter for kontrastiv PCA er kontraststyrkenα. I vores standardalgoritme udviklede vi en automatisk ordning baseret på klynger af underområder til at vælge de mest informative værdier afα (se Metoder). Alle de eksperimenter, der er udført for dette papir, bruger de automatisk genereredeα -værdier, og vi mener, at denne standard vil være tilstrækkelig i mange anvendelser af cPCA. Brugeren kan også indtaste specifikke værdier forα , hvis der ønskes en mere detaljeret udforskning.

cPCA, ligesom almindelig PCA og andre metoder til dimensionalitetsreduktion, giver ikkep -værdier eller andre statistiske signifikanskvantificeringer. De mønstre, der opdages gennem cPCA, skal valideres gennem hypotesetestning eller yderligere analyse ved hjælp af relevant domæneviden. Vi har frigivet koden for cPCA som en python-pakke sammen med dokumentation og eksempler.

For ded -dimensionelle måldataxi ∈ Rd og baggrundsdatayi ∈ Rd , ladC X,C Yvære deres tilsvarende empiriske kovariansmatricer. LadR unit d = def v ∈ Rd :v2 = 1 være mængden af enhedsvektorer. For enhver retningv ∈ R unit d , kan variansen, den står for i måldataene og i baggrundsdataene, skrives som: Mål data varians :λX ( v ) =defvTCXv , Baggrunds data varians :λY ( v ) =defvTCYv.Givet en kontrastparameterα ≥ 0, der kvantificerer afvejningen mellem at have høj måldata-varians og lav baggrundsdata-varians, beregner cPCA den kontrastive retningv * ved at optimere 1 v * = argmax v ∈ Runitd λ X(v)- α λ Y(v).  Dette problem kan omskrives somv * = argmax v ∈ Runitd v T CX - α CY v ,  hvilket indebærer, atv * svarer til den første egenvektor af matricenC = def CX - α CY. Derfor kan de kontrastive retninger beregnes effektivt ved hjælp af egenværdidekomposition. Analogt med PCA kalder vi de førende egenvektorer afC de kontrastive hovedkomponenter (cPC'er). Vi bemærker, at cPC'erne er egenvektorer af matricenC og derfor er ortogonale til hinanden. For en fastα beregner vi ( [1] ) og returnerer det underområde, der spænder over de første få (typisk to) cPC'er.

Kontrastparameterenα repræsenterer afvejningen mellem at have den høje måldata-varians og den lave baggrundsdata-varians. Nårα = 0, vælger cPCA de retninger, der kun maksimerer måldata-variansen, og reducerer derfor til PCA anvendt på måldataene {x i}. Nårα øges, bliver retninger med mindre baggrundsdata-varians mere vigtige, og cPC'erne drives mod baggrundsdataens nulrum {y i}. I det begrænsende tilfældeα = ∞ modtager enhver retning, der ikke er i nulrummet af {y i}, en uendelig straf. I dette tilfælde svarer cPCA til først at projicere måldataene på baggrundsdataens nulrum og derefter udføre PCA på de projicerede data.

I stedet for at vælge en enkeltα og returnere dens delrum, beregner cPCA delrummene for en liste afα 'er og returnerer nogle få delrum, der er langt fra hinanden i forhold til hovedvinklen. Ved at projicere dataene på hvert af disse delrum vil forskellige tendenser inden for måldataene blive afsløret, og ved visuelt at undersøge de returnerede spredningsdiagrammer kan brugeren hurtigt skelne det relevante delrum (og den tilsvarende værdi afα ) til sin analyse. Se Supplementary Fig. [1] for et detaljeret eksempel.

Den komplette algoritme for cPCA er beskrevet i Algoritme 2 (Supplementary Methods). Vi sætter typisk listen over potentielle værdier afα til at være 40 værdier logaritmisk fordelt mellem 0,1 og 1000, og dette bruges til alle eksperimenter i artiklen. For at vælge de repræsentative delrum bruger cPCA spektral klyngedannelse til at klynge delrummene, hvor affiniteten er defineret som produktet af cosinus af hovedvinklerne mellem delrummene. Derefter bruges medoidene (repræsentative) af hver klynge som værdierne afα til at generere de spredningsdiagrammer, som brugeren ser.

Valget af baggrundsdatamængden har stor indflydelse på resultatet af cPCA. Generelt bør baggrundsdataene have den struktur, som vi gerne vil fjerne fra måldataene. Sådan struktur svarer normalt til retninger i målet med høj varians, men som ikke er af interesse for analytikeren.

Vi giver nogle generelle eksempler på baggrundsdatamængder, der kan give nyttige kontraster til måldata: (1) En kontrolgruppe {y i} kontrasteret med en syg population {x i} fordi kontrolgruppen indeholder lignende variation på populationsniveau, men ikke den subtile variation på grund af forskellige undertyper af sygdommen. (2) Dataene ved tid nul {y i} brugt til at kontrastere mod data på et senere tidspunkt {x i}. Dette muliggør visualiseringer af de mest fremtrædende ændringer over tid. (3) En homogen gruppe {y i} kontrasteret med en blandet gruppe {x i} fordi begge har variation inden for populationen og måleusikkerhed, men den første har ikke variation mellem populationer. (4) Et præ-behandlingsdatasæt {y i} kontrasteret med post-behandlingsdata {x i} for at fjerne måleusikkerhed, men bevare variationer forårsaget af behandling. (5) Et sæt signalfrie optagelser {y i} eller billeder, der kun indeholder støj, kontrasteret med målinger {x i} der består af både signal og støj.

Det er værd at tilføje, at baggrundsdataene ikke behøver at have nøjagtig den samme kovariansstruktur som det, vi gerne vil fjerne fra målgruppen. Som et eksempel, i eksperimentet vist i Fig. [2] , viser det sig, at vi ikke behøver at bruge en baggrundsdatamængde, der består af billeder af græs. Faktisk opnås lignende resultater, selvom der i stedet for billeder af græs bruges billeder af himlen som baggrundsdatamængde. Da strukturen af kovariansmatricerne er tilstrækkelig ens, fjerner cPCA baggrundsstrukturen fra måldataene. Derudover kræver cPCA ikke, at måldataene og baggrundsdataene har et lignende antal prøver. Da kovariansmatricerne beregnes uafhængigt, kræver cPCA kun, at de empiriske kovariansmatricer er gode estimater af de underliggende populationskovariansmatricer, i det væsentlige det samme krav som PCA.

Her diskuterer vi den geometriske fortolkning af cPCA samt dens statistiske egenskaber. Først er det interessant at overveje, hvilke retninger der er "bedre" til formålet med kontrastanalyse. For en retningv ∈ R unit d , er dens betydning i cPCA fuldt ud bestemt af dens mål-baggrundsvarianspar (λ X(v ),λ Y(v )); det er ønskeligt at have en højere målvarians og en lavere baggrundsvarians. Baseret på denne intuition kan vi yderligere definere en delvis rækkefølge af kontrast for forskellige retninger: for to retningerv1 ogv2 , kan vi sige, atv1 er en bedre kontrastretning, hvis den har en højere målvarians og en lavere baggrundsvarians. I dette tilfælde ville mål-baggrundsvariansparret forv1 ligge på den nedre højre side af det forv2 i plottet af mål-baggrundsvarianspar (λ X(v ),λ Y(v )), f.eks., Fig. [5]. Baseret på denne delvise rækkefølge kan sættet af de mest kontrastive retninger defineres på en lignende måde som definitionen af Pareto-fronten. LadU  være sættet af mål-baggrundsvarianspar for alle retninger, dvs.U = def  ( λX ( v ) ,λY ( v ))v ∈Runitd. Sættet af de mest kontrastive retninger svarer til den nedre højre grænse afU i plottet af mål-baggrundsvarianspar, som vist i Fig.[5]. (For det særlige tilfælde af samtidig diagonaliserbare baggrunds- og målmatricer, se Supplementary Fig.[7].)

Vedrørende cPCA kan vi bevise (se Supplementary Note [2] ), at ved at variereα , er sættet af de øverste cPC'er identisk med sættet af de mest kontrastive retninger. Desuden, for retningenv valgt af cPCA med kontrastparameteren sat tilα , svarer dens varianspar (λ X(v ),λ Y(v )) til tangenspunktet for den nedre højre grænse afU  med en hældning-1/α linje. Som et resultat, ved at variereα fra nul til uendelig, vælger cPCA retninger med varianspar, der bevæger sig fra den nedre venstre ende til den øvre højre ende af den nedre højre grænse afU .

Vi bemærker også, at med hensyn til dataens tilfældighed er konvergenshastigheden for prøve-cPC til populations-cPCO p  dmin ( n,m ) under milde antagelser, hvord er dimensionen ogn ,m er størrelserne af mål- og baggrundsdataene. Denne hastighed ligner den standard konvergenshastighed for prøve-eigenvektoren for en kovariansmatrix. Se Supplementary Note [2].

Vi har frigivet en Python-implementering af kontrastiv PCA på GitHub (https://github.com/abidlabs/contrastive ). GitHub-repositoriet inkluderer også Python-notebooks og datasæt, der gengiver de fleste af figurerne i denne artikel og i Supplementary Information.

Datasæt, der er blevet brugt til at evaluere kontrastiv PCA i denne artikel, er enten tilgængelige fra os eller fra forfatterne af de originale studier. Se venligst GitHub-repositoriet, der er anført i det foregående afsnit, for de datasæt, vi har frigivet.

Fig. 1: Skematisk oversigt over cPCA. For at udføre cPCA, beregn kovariansmatricerne C X, C Yfor mål- og baggrundsdatamængderne. De singulære vektorer af den vægtede forskel af kovariansmatricerne, C X− α · C Y, er de retninger, der returneres af cPCA. Som vist i spredningsdiagrammet til højre, identificerer PCA (på måldataene) den retning, der har den højeste varians i måldataene, mens cPCA identificerer den retning, der har en højere varians i måldataene sammenlignet med baggrundsdataene. Projektering af måldataene på sidstnævnte retning giver mønstre unikke for måldataene og afslører ofte strukturer, der overses af PCA. Specifikt i dette eksempel ville reduktion af måldataenes dimensionalitet ved cPCA afsløre to adskilte klynger

Fig. 2: Kontrastiv PCA på støjende cifre.a, Top: Vi skaber et måldatasæt af 5.000 syntetiske billeder ved tilfældigt at overlejre billeder af håndskrevne cifre 0 og 1 fra MNIST-datasættet oven på billeder af græs taget fra ImageNet-datasættet tilhørende synset græs. Billederne af græs konverteres til gråtoner, ændres til at være 100 × 100, og beskæres derefter tilfældigt til at være samme størrelse som MNIST-cifrene, 28 × 28.b, Top: Her plotter vi resultatet af at indlejre de syntetiske billeder på deres to første hovedkomponenter ved hjælp af standard PCA. Vi ser, at punkterne svarende til billederne med 0'er og billederne med 1'er er svære at skelne.a, Bund: Et baggrundsdatasæt introduceres derefter, der udelukkende består af billeder af græs tilhørende samme synset, men vi bruger billeder, der er forskellige fra dem, der blev brugt til at skabe måldatasættet.b, Bund: Ved at bruge cPCA på mål- og baggrundsdatasættene (med en værdi af kontrastparameteren α sat til 2,0), opstår der to klynger i den lavdimensionelle repræsentation af måldatasættet, en bestående af billeder med cifferet 0 og den anden af billeder med cifferet 1.cVi ser på den relative bidrag af hver pixel til den første hovedkomponent (PC) og den første kontrastive hovedkomponent (cPC). Hvidere pixels er dem, der bærer en mere positiv vægt, mens mørkere angiver de pixels, der bærer negative vægte. PCA har tendens til at fremhæve pixels i periferien af billedet og let nedtone pixels i midten og bunden af billedet, hvilket indikerer, at det meste af variansen skyldes baggrundsfunktioner. På den anden side har cPCA tendens til at opvægte de pixels, der er på placeringen af de håndskrevne 1'ere, negativt vægte pixels på placeringen af de håndskrevne 0'ere og ignorere de fleste andre pixels, hvilket effektivt opdager de funktioner, der er nyttige til at skelne mellem de overlejrede cifre

Fig. 3: Opdagelse af undergrupper i biologiske data. a Vi bruger PCA til at projicere et proteinekspressionsdatasæt af mus med og uden Down Syndrom (DS) på de to første komponenter. Den lavdimensionelle repræsentation af proteinekspressionsmålinger fra mus med og uden DS ses at være distribueret ens (top). Men når vi bruger cPCA til at projicere datasættet på dets to første cPC'er, opdager vi en lavdimensionel repræsentation, der klynger mus med og uden DS separat (bund). b Desuden bruger vi PCA og cPCA til at visualisere et højdimensionelt enkeltcelle RNA-Seq datasæt i to dimensioner. Datasættet består af fire celleprøver fra to leukæmipatienter: en præ-transplantationsprøve fra patient 1, en post-transplantationsprøve fra patient 1, en præ-transplantationsprøve fra patient 2 og en post-transplantationsprøve fra patient 2. b , venstre: Resultaterne ved kun at bruge prøverne fra patient 1, som demonstrerer, at cPCA (bund) mere effektivt adskiller prøverne end PCA (top). Når prøverne fra den anden patient inkluderes, i b , højre, er cPCA (bund) igen mere effektiv end PCA (top) til at adskille prøverne, selvom post-transplantationscellerne fra begge patienter er distribueret ens. Vi viser plots af hver prøve separat i Supplementary Fig.[5] , hvor det er lettere at se overlapningen mellem forskellige prøver

Fig. 4: Forholdet mellem mexicanske afstamningsgrupper. a PCA anvendt på genetiske data fra individer fra 5 mexicanske stater afslører ikke nogen visuelt skelnelige mønstre i de indlejrede data. b cPCA anvendt på det samme datasæt afslører mønstre i dataene: individer fra samme stat er klynget tættere sammen i cPCA-indlejringen. c Desuden afslører fordelingen af punkterne forhold mellem grupperne, der matcher den geografiske placering af de forskellige stater: for eksempel er individer fra geografisk tilstødende stater tilstødende i indlejringen. c Tilpasset fra et kort over Mexico, der oprindeligt er arbejdet af Bruger:Allstrak på Wikipedia, udgivet under en CC-BY-SA-licens, hentet fra https://commons.wikimedia.org/wiki/File:Mexico_Map.svg

Fig. 5: Geometrisk fortolkning af cPCA. Sættet af mål-baggrundsvariansparUer plottet som det blågrønne område for nogle tilfældigt genererede mål- og baggrundsdata. Den nedre-højre grænse, farvet i guld, svarer til sættet af mest kontrastive retningerSλ. De blå trekanter er variansparrene for de cPC'er, der er valgt med α -værdierne 0,92 og 0,29 henholdsvis. Vi bemærker, at de svarer til tangenspunkterne for den gyldne kurve og tangentlinjerne med hældning1α= 1,08, 3,37, henholdsvis

