بررسی الگوهای غنی‌شده در یک مجموعه داده با تحلیل مؤلفه‌های اصلی متضاد

چکیده: تصویربرداری و بررسی داده‌های با ابعاد بالا یک چالش فراگیر در رشته‌های مختلف است. تکنیک‌های پرکاربردی مانند تحلیل مؤلفه‌های اصلی (PCA) به شناسایی روندهای غالب در یک مجموعه داده می‌پردازند. با این حال، در بسیاری از موارد، ما مجموعه داده‌هایی داریم که تحت شرایط مختلف جمع‌آوری شده‌اند، مثلاً یک آزمایش درمانی و یک آزمایش کنترل، و ما به دنبال تصویربرداری و بررسی الگوهایی هستیم که به یک مجموعه داده خاص تعلق دارند. این مقاله روشی به نام تحلیل مؤلفه‌های اصلی متضاد (cPCA) پیشنهاد می‌کند که ساختارهای با ابعاد پایین را که نسبت به داده‌های مقایسه‌ای در یک مجموعه داده غنی شده‌اند، شناسایی می‌کند. در طیف وسیعی از آزمایش‌ها، نشان می‌دهیم که cPCA با یک مجموعه داده پس‌زمینه به ما امکان می‌دهد الگوهای خاص مجموعه داده را که توسط PCA و سایر روش‌های استاندارد از دست رفته‌اند، تصویربرداری کنیم. ما همچنین تفسیر هندسی cPCA و تضمین‌های ریاضی قوی ارائه می‌دهیم. پیاده‌سازی cPCA به صورت عمومی در دسترس است و می‌تواند برای تحلیل داده‌های اکتشافی در بسیاری از کاربردهایی که در حال حاضر از PCA استفاده می‌شود، به کار رود.

تحلیل مؤلفه‌های اصلی (PCA) یکی از پرکاربردترین روش‌ها برای کاوش و تجسم داده‌ها است. PCA داده‌ها را به فضای با ابعاد پایین‌تر می‌افکند و به ویژه به عنوان روشی برای تجسم الگوها، مانند خوشه‌ها، خطوط و نقاط دورافتاده در یک مجموعه داده، قدرتمند است. تعداد زیادی از روش‌های تجسم مرتبط وجود دارد؛ به عنوان مثال، t-SNE و مقیاس‌بندی چندبعدی (MDS) اجازه می‌دهند تا داده‌ها به صورت غیرخطی افکنده شوند و ممکن است الگوهای غیرخطی را بهتر از PCA به تصویر بکشند. با این حال، همه این روش‌ها برای کاوش یک مجموعه داده در هر زمان طراحی شده‌اند. هنگامی که تحلیل‌گر چندین مجموعه داده (یا شرایط مختلف در یک مجموعه داده برای مقایسه) دارد، روش فعلی این است که PCA (یا t-SNE، MDS و غیره) را به صورت جداگانه بر روی هر مجموعه داده انجام دهد و سپس به صورت دستی پروجکشن‌های مختلف را مقایسه کند تا ببیند آیا شباهت‌ها و تفاوت‌های جالبی در میان مجموعه داده‌ها وجود دارد یا خیر,. PCA متضاد (cPCA) برای پر کردن این شکاف در کاوش و تجسم داده‌ها طراحی شده است، با شناسایی خودکار پروجکشن‌هایی که تفاوت‌های جالب‌تری در میان مجموعه داده‌ها نشان می‌دهند. شکل[1]نمای کلی از cPCA ارائه می‌دهد که در ادامه به تفصیل توضیح خواهیم داد.

cPCA با انگیزه‌ای از طیف گسترده‌ای از مشکلات در رشته‌های مختلف ایجاد شده است. برای مثال، ما دو مورد از این مشکلات را در اینجا ذکر می‌کنیم و دیگران را از طریق آزمایش‌ها در ادامه مقاله نشان می‌دهیم. اول، یک مجموعه داده از اندازه‌گیری‌های بیان ژن از افراد با قومیت‌ها و جنسیت‌های مختلف را در نظر بگیرید. این داده‌ها شامل سطوح بیان ژن بیماران سرطانی {x i} است که ما به تحلیل آن‌ها علاقه‌مندیم. ما همچنین داده‌های کنترلی داریم که به سطوح بیان ژن بیماران سالم {y i} از پس‌زمینه جمعیتی مشابه مربوط می‌شود. هدف ما یافتن روندها و تغییرات در بیماران سرطانی است (مثلاً برای شناسایی زیرگروه‌های مولکولی سرطان). اگر مستقیماً PCA را به {x i} اعمال کنیم، ممکن است مؤلفه‌های اصلی بالا به تغییرات جمعیتی افراد مربوط شوند نه به زیرگروه‌های سرطان، زیرا تغییرات ژنتیکی ناشی از اولی احتمالاً بزرگتر از دومی است. ما به این مشکل با توجه به این نکته نزدیک می‌شویم که بیماران سالم نیز شامل تغییرات مرتبط با تفاوت‌های جمعیتی هستند، اما نه تغییرات مربوط به زیرگروه‌های سرطان. بنابراین، می‌توانیم به دنبال مؤلفه‌هایی بگردیم که در آن‌ها {x i} واریانس بالا و {y i} واریانس پایین دارد.

به عنوان یک مثال مرتبط، یک مجموعه داده {x i} را در نظر بگیرید که شامل ارقام دست‌نویس بر روی یک پس‌زمینه پیچیده، مانند تصاویر مختلف از چمن است (به Fig.[2(a), top]نگاه کنید). هدف یک وظیفه یادگیری بدون نظارت معمولی ممکن است خوشه‌بندی داده‌ها باشد، که ارقام مختلف در تصویر را آشکار می‌کند. با این حال، اگر PCA استاندارد را بر روی این تصاویر اعمال کنیم، متوجه می‌شویم که مؤلفه‌های اصلی بالا ویژگی‌های مرتبط با ارقام دست‌نویس را نشان نمی‌دهند، بلکه تغییرات غالب در ویژگی‌های مرتبط با پس‌زمینه تصویر را منعکس می‌کنند (Fig.[2(b)], top). ما نشان می‌دهیم که می‌توان این مسئله را با استفاده از یک مجموعه داده مرجع {y i} که فقط شامل تصاویر چمن است (نه لزوماً همان تصاویری که در {x i} استفاده شده‌اند، اما دارای هم‌پراکنش مشابه بین ویژگی‌ها، همان‌طور که در Fig.[2(a)], bottom نشان داده شده است) و جستجوی زیرفضای با واریانس بالاتر در {x i} نسبت به {y i} اصلاح کرد. با افکندن بر روی این زیرفضا، می‌توانیم تصاویر را بر اساس مقدار رقم دست‌نویس به صورت بصری جدا کنیم (Fig. 2(b), bottom). با مقایسه مؤلفه‌های اصلی کشف شده توسط PCA با آن‌هایی که توسط cPCA کشف شده‌اند، می‌بینیم که cPCA ویژگی‌های مرتبط‌تری را شناسایی می‌کند (Fig.[2(c)])، که به ما اجازه می‌دهد از cPCA برای کاربردهایی مانند انتخاب ویژگی و کاهش نویز استفاده کنیم.

PCA متضاد ابزاری برای یادگیری بدون نظارت است که به طور کارآمدی ابعاد را کاهش می‌دهد تا تجسم و تحلیل داده‌های اکتشافی را ممکن سازد. این cPCA را از یک کلاس بزرگ از روش‌های یادگیری نظارت‌شده که هدف اصلی آن‌ها طبقه‌بندی یا تمایز بین مجموعه داده‌های مختلف است، مانند تحلیل تفکیک خطی (LDA)، تحلیل تفکیک درجه دوم (QDA)، PCA نظارت‌شده و QUADRO جدا می‌کند. این همچنین cPCA را از روش‌هایی که چندین مجموعه داده را ادغام می‌کنند، با هدف شناسایی الگوهای همبسته در میان دو یا چند مجموعه داده، به جای آن‌هایی که به هر مجموعه داده فردی منحصر به فرد هستند، متمایز می‌کند. همچنین یک خانواده غنی از روش‌های بدون نظارت برای کاهش ابعاد به جز PCA وجود دارد. به عنوان مثال، مقیاس‌بندی چندبعدی (MDS) یک افکندن با ابعاد پایین‌تر پیدا می‌کند که فاصله در فضای با ابعاد بالا را حفظ می‌کند؛ جستجوی مؤلفه‌های اصلی یک زیرفضای با رتبه پایین پیدا می‌کند که به نویز کوچک در سطح ورودی و خطاهای پراکنده بزرگ مقاوم است. اما هیچ‌کدام برای استفاده از اطلاعات مرتبط از یک مجموعه داده دوم طراحی نشده‌اند، همان‌طور که cPCA انجام می‌دهد. در مکمل، ما cPCA را با بسیاری از تکنیک‌های ذکر شده قبلی بر روی مجموعه داده‌های نماینده مقایسه کرده‌ایم (به شکل‌های مکمل[3]و[4]نگاه کنید), , ,.

در یک حوزه کاربردی خاص، ممکن است ابزارهای تخصصی در آن حوزه با اهداف مشابه cPCA وجود داشته باشد, ,. به عنوان مثال، در نتایج، ما نشان می‌دهیم که چگونه cPCA که بر روی داده‌های ژنوتیپ اعمال شده است، تبار جغرافیایی در مکزیک را تجسم می‌کند. کاوش خوشه‌های دقیق‌تری از تبارهای ژنتیکی یک مسئله مهم در ژنتیک جمعیت است و محققان اخیراً الگوریتمی برای تجسم خاص این خوشه‌های تبار توسعه داده‌اند. در حالی که cPCA در اینجا عملکرد خوبی دارد، الگوریتم تخصصی ممکن است برای یک مجموعه داده خاص حتی بهتر عمل کند. با این حال، الگوریتم تخصصی نیاز به دانش قابل توجهی از حوزه دارد تا طراحی شود، از نظر محاسباتی پرهزینه‌تر است و می‌تواند چالش‌برانگیز باشد. هدف cPCA جایگزینی همه این روش‌های تخصصی پیشرفته در هر یک از حوزه‌های آن‌ها نیست، بلکه ارائه یک روش عمومی برای کاوش مجموعه داده‌های دلخواه است.

ما یک الگوریتم مشخص و کارآمد برای cPCA در این مقاله پیشنهاد می‌کنیم. این روش به عنوان ورودی یک مجموعه داده هدف {x i} که ما به تجسم یا شناسایی الگوها در آن علاقه‌مندیم، می‌گیرد. به عنوان ورودی ثانویه، cPCA یک مجموعه داده پس‌زمینه {y i} می‌گیرد که شامل الگوهای مورد علاقه نیست. الگوریتم cPCA زیرفضاهایی را برمی‌گرداند که مقدار زیادی از تغییرات در داده‌های هدف {x i} را می‌گیرند، اما در پس‌زمینه {y i} کم است (به Fig.[1]، روش‌ها و روش‌های مکمل برای جزئیات بیشتر نگاه کنید). این زیرفضا به ویژگی‌هایی مربوط می‌شود که ساختار خاصی به {x i} دارند. بنابراین، هنگامی که داده‌های هدف بر روی این زیرفضا افکنده می‌شوند، ما قادر به تجسم و کشف ساختار اضافی در داده‌های هدف نسبت به پس‌زمینه هستیم. مشابه مؤلفه‌های اصلی (PCs)، ما جهت‌هایی که توسط cPCA پیدا می‌شوند را مؤلفه‌های اصلی متضاد (cPCs) می‌نامیم. ما تأکید می‌کنیم که cPCA اساساً یک تکنیک بدون نظارت است که برای حل الگوها در یک مجموعه داده به وضوح بیشتر با استفاده از مجموعه داده پس‌زمینه به عنوان یک تضاد طراحی شده است. به طور خاص، cPCA به دنبال تمایز بین مجموعه داده‌های هدف و پس‌زمینه نیست؛ زیرفضایی که شامل روندهایی است که در مجموعه داده هدف غنی شده‌اند، لزوماً همان زیرفضایی نیست که برای طبقه‌بندی بین مجموعه داده‌ها بهینه است.

محققان متوجه شده‌اند که PCA استاندارد اغلب در کشف زیرگروه‌ها در داده‌های زیستی ناکارآمد است، حداقل تا حدی به این دلیل که "مؤلفه‌های اصلی غالب... با مصنوعات همبسته هستند"، نه با ویژگی‌هایی که برای محقق جالب هستند. چگونه می‌توان از cPCA در این تنظیمات برای شناسایی زیرگروه‌های مهم‌تر استفاده کرد؟ با استفاده از یک مجموعه داده پس‌زمینه برای حذف تغییرات جهانی اما غیرجالب در هدف، می‌توانیم به دنبال ساختاری بگردیم که به مجموعه داده هدف منحصر به فرد است.

اولین آزمایش ما از یک مجموعه داده شامل اندازه‌گیری‌های بیان پروتئین موش‌هایی که تحت درمان شوک قرار گرفته‌اند استفاده می‌کند,. برخی از موش‌ها سندرم داون (DS) را توسعه داده‌اند. برای ایجاد یک وظیفه یادگیری بدون نظارت که در آن اطلاعات حقیقت زمینی برای ارزیابی روش‌ها داریم، فرض می‌کنیم که این اطلاعات DS برای تحلیل‌گر ناشناخته است و فقط برای ارزیابی الگوریتم استفاده می‌شود. ما می‌خواهیم ببینیم آیا می‌توانیم تفاوت‌های قابل توجهی در جمعیت موش‌های شوک‌دیده به صورت بدون نظارت شناسایی کنیم (وجود یا عدم وجود سندرم داون یک مثال کلیدی است). در Fig. [3a] (بالا)، نتیجه اعمال PCA بر روی مجموعه داده هدف را نشان می‌دهیم: داده‌های تبدیل شده هیچ خوشه‌بندی قابل توجهی در جمعیت موش‌ها نشان نمی‌دهند. منابع اصلی تغییرات در موش‌ها ممکن است طبیعی باشند، مانند جنسیت یا سن.

ما cPCA را بر روی این مجموعه داده با استفاده از یک پس‌زمینه که شامل اندازه‌گیری‌های بیان پروتئین از مجموعه‌ای از موش‌هایی است که تحت درمان شوک قرار نگرفته‌اند، اعمال می‌کنیم. آن‌ها موش‌های کنترلی هستند که احتمالاً تغییرات طبیعی مشابهی با موش‌های آزمایشی دارند، اما بدون تفاوت‌هایی که از درمان شوک ناشی می‌شود. با این مجموعه داده به عنوان پس‌زمینه، cPCA قادر است دو گروه مختلف را در داده‌های هدف تبدیل شده حل کند، یکی مربوط به موش‌هایی که سندرم داون ندارند و دیگری (بیشتر) مربوط به موش‌هایی که سندرم داون دارند، همان‌طور که در Fig. [3a] (پایین) نشان داده شده است. به عنوان مقایسه، ما همچنین 8 تکنیک کاهش ابعاد دیگر را برای شناسایی جهت‌هایی که بین مجموعه داده‌های هدف و پس‌زمینه تفاوت قائل می‌شوند، اعمال کردیم، که هیچ‌کدام نتوانستند موش‌ها را به خوبی cPCA جدا کنند (برای جزئیات به شکل مکمل [4] نگاه کنید).

سپس، ما یک مجموعه داده عمومی با ابعاد بالاتر را که شامل سطوح بیان RNA تک‌سلولی از مخلوطی از سلول‌های تک‌هسته‌ای مغز استخوان (BMMCs) گرفته شده از یک بیمار لوسمی قبل از پیوند سلول‌های بنیادی و BMMCs از همان بیمار پس از پیوند سلول‌های بنیادی است، تحلیل می‌کنیم. تمام داده‌های RNA-Seq تک‌سلولی با استفاده از روش‌های مشابهی که توسط نویسندگان توصیف شده است، پیش‌پردازش می‌شوند. به طور خاص، قبل از اعمال PCA یا cPCA، تمام مجموعه داده‌ها به 500 ژن کاهش می‌یابند، که بر اساس بیشترین پراکندگی [واریانس تقسیم بر میانگین] در داده‌های هدف انتخاب می‌شوند. دوباره، ما PCA را انجام می‌دهیم تا ببینیم آیا می‌توانیم به صورت بصری دو نمونه را در داده‌های تبدیل شده کشف کنیم. همان‌طور که در Fig. [3b] (بالا چپ) نشان داده شده است، هر دو نوع سلول در فضای گسترده شده توسط دو مؤلفه اصلی اول توزیع مشابهی دارند. این احتمالاً به این دلیل است که تفاوت‌های بین نمونه‌ها کوچک است و مؤلفه‌های اصلی به جای آن ناهمگنی انواع مختلف سلول‌ها در هر نمونه یا حتی تغییرات در شرایط آزمایشی را منعکس می‌کنند، که می‌تواند تأثیر قابل توجهی بر اندازه‌گیری‌های RNA-Seq تک‌سلولی داشته باشد.

ما از cPCA با استفاده از یک مجموعه داده پس‌زمینه که شامل اندازه‌گیری‌های RNA-Seq از سلول‌های BMMC یک فرد سالم است، استفاده می‌کنیم. ما انتظار داریم که این مجموعه داده پس‌زمینه شامل تغییرات ناشی از جمعیت ناهمگن سلول‌ها و همچنین تغییرات در شرایط آزمایشی باشد. بنابراین، ممکن است امیدوار باشیم که cPCA بتواند جهت‌هایی را که در داده‌های هدف غنی شده‌اند و مربوط به تفاوت‌های قبل و بعد از پیوند هستند، بازیابی کند. در واقع، این همان چیزی است که ما پیدا می‌کنیم، همان‌طور که در Fig. [3b] (پایین چپ) نشان داده شده است.

ما مجموعه داده هدف خود را با نمونه‌های BMMC از یک بیمار دوم مبتلا به لوسمی، دوباره قبل و بعد از پیوند سلول‌های بنیادی، گسترش می‌دهیم. بنابراین، در مجموع چهار زیرجمعیت از سلول‌ها وجود دارد. اعمال PCA بر روی این داده‌ها نشان می‌دهد که چهار زیرجمعیت در زیرفضای گسترده شده توسط دو مؤلفه اصلی (PCs) قابل تفکیک نیستند، همان‌طور که در Fig. [3b] (بالا راست) نشان داده شده است. با این حال، هنگامی که cPCA با همان مجموعه داده پس‌زمینه اعمال می‌شود، حداقل سه زیرجمعیت جدایی بسیار قوی‌تری نشان می‌دهند، همان‌طور که در Fig. [3b] (پایین راست) نشان داده شده است. جاسازی cPCA همچنین نشان می‌دهد که نمونه‌های سلولی از هر دو بیمار پس از پیوند سلول‌های بنیادی (نقاط فیروزه‌ای و سبز) به یکدیگر شبیه‌تر از قبل از پیوند (نقاط طلایی و صورتی) هستند، فرضیه‌ای منطقی که می‌تواند توسط محقق آزمایش شود. برای جزئیات بیشتر این آزمایش می‌توان به Fig. [5] تکمیلی مراجعه کرد. ما می‌بینیم که cPCA می‌تواند ابزاری مفید برای استنباط رابطه بین زیرجمعیت‌ها باشد، موضوعی که در ادامه بیشتر بررسی خواهیم کرد.

در مثال‌های قبلی، دیده‌ایم که cPCA به کاربر اجازه می‌دهد تا زیرکلاس‌هایی را در یک مجموعه داده هدف که به‌طور پیش‌فرض برچسب‌گذاری نشده‌اند، کشف کند. با این حال، حتی زمانی که زیرکلاس‌ها از قبل شناخته شده‌اند، کاهش ابعاد می‌تواند راهی مفید برای تجسم رابطه درون گروه‌ها باشد. به عنوان مثال، PCA اغلب برای تجسم رابطه بین جمعیت‌های قومی بر اساس واریانت‌های ژنتیکی استفاده می‌شود، زیرا پیش‌بینی واریانت‌های ژنتیکی بر روی دو بعد اغلب نقشه‌هایی تولید می‌کند که تجسم‌های چشمگیری از روندهای جغرافیایی و تاریخی ارائه می‌دهند,. اما باز هم، PCA به شناسایی ساختار غالب محدود است؛ زمانی که این ساختار نمایانگر تغییرات جهانی یا غیرجالب است، cPCA می‌تواند در تجسم روندها مؤثرتر باشد.

مجموعه داده‌ای که ما برای این مثال استفاده می‌کنیم شامل پلی‌مورفیسم‌های تک‌نوکلئوتیدی (SNPs) از ژنوم‌های افراد از پنج ایالت مکزیک است که در یک مطالعه قبلی جمع‌آوری شده‌اند. تحلیل نژاد مکزیکی با استفاده از PCA چالش‌برانگیز است زیرا PCs معمولاً منشأ جغرافیایی درون مکزیک را منعکس نمی‌کنند؛ بلکه نسبت میراث اروپایی/بومی آمریکایی هر فرد مکزیکی را منعکس می‌کنند که بر تفاوت‌های ناشی از منشأ جغرافیایی درون مکزیک غالب است و آن‌ها را مبهم می‌کند (به Fig. [4a] مراجعه کنید). برای غلبه بر این مشکل، ژنتیک‌دانان جمعیت به‌صورت دستی SNPها را هرس می‌کنند و آن‌هایی را که می‌دانند از نژاد اروپایی مشتق شده‌اند، قبل از اعمال PCA حذف می‌کنند. با این حال، این روش کاربرد محدودی دارد زیرا نیاز به دانستن منشأ SNPها و اینکه منبع تغییرات پس‌زمینه بسیار متفاوت از تغییرات مورد علاقه باشد، دارد که اغلب این‌گونه نیست.

به عنوان جایگزین، ما از cPCA با یک مجموعه داده پس‌زمینه که شامل افراد از مکزیک و اروپا است، استفاده می‌کنیم. این پس‌زمینه تحت تأثیر تغییرات بومی آمریکایی/اروپایی است و به ما اجازه می‌دهد تا تغییرات درون مکزیکی را در مجموعه داده هدف جدا کنیم. نتایج اعمال cPCA در Fig. [4b] نشان داده شده است. ما می‌یابیم که افراد از همان ایالت در مکزیک به یکدیگر نزدیک‌تر جاسازی شده‌اند. علاوه بر این، دو گروهی که بیشترین تفاوت را دارند، سونوران‌ها و مایان‌ها از یوکاتان هستند که همچنین از نظر جغرافیایی در مکزیک بیشترین فاصله را دارند، در حالی که مکزیکی‌ها از سه ایالت دیگر به یکدیگر نزدیک هستند، هم از نظر جغرافیایی و هم در جاسازی که توسط cPCA ثبت شده است (به Fig. [4c] مراجعه کنید). برای جزئیات بیشتر به Fig. [6] تکمیلی نیز مراجعه کنید.

در بسیاری از تنظیمات علم داده، ما به تجسم و کاوش الگوهایی که در یک مجموعه داده نسبت به داده‌های دیگر غنی شده‌اند، علاقه‌مندیم. ما cPCA را به عنوان یک ابزار عمومی برای انجام چنین کاوش‌های متضاد ارائه کرده‌ایم و مفید بودن آن را در طیف گسترده‌ای از کاربردها نشان داده‌ایم. مزایای اصلی cPCA عمومی بودن و سهولت استفاده آن است. محاسبه یک cPCA خاص اساساً به همان مقدار زمان محاسبه یک PCA معمولی نیاز دارد. این کارایی محاسباتی به cPCA اجازه می‌دهد تا برای کاوش تعاملی داده‌ها مفید باشد، جایی که هر عملیات باید ایده‌آل به‌طور تقریباً فوری باشد. به این ترتیب، در هر تنظیماتی که PCA بر روی مجموعه داده‌های مرتبط اعمال می‌شود، cPCA نیز می‌تواند اعمال شود. در یادداشت تکمیلی[3]و Fig.[8]تکمیلی، نشان می‌دهیم که چگونه cPCA می‌تواند به‌صورت کرنلیزه شده برای کشف الگوهای متضاد غیرخطی در مجموعه داده‌ها استفاده شود.

تنها پارامتر آزاد cPCA قدرت تضادα است. در الگوریتم پیش‌فرض ما، یک طرح خودکار بر اساس خوشه‌بندی زیرفضاها برای انتخاب مقادیر اطلاعاتی‌ترینα توسعه داده‌ایم (به روش‌ها مراجعه کنید). تمام آزمایش‌های انجام شده برای این مقاله از مقادیرα تولید شده به‌طور خودکار استفاده می‌کنند و ما معتقدیم که این پیش‌فرض در بسیاری از کاربردهای cPCA کافی خواهد بود. کاربر همچنین می‌تواند مقادیر خاصی برایα وارد کند اگر کاوش دقیق‌تری مورد نظر باشد.

cPCA، مانند PCA معمولی و سایر روش‌های کاهش ابعاد، مقادیرp یا سایر کمیت‌های اهمیت آماری را ارائه نمی‌دهد. الگوهایی که از طریق cPCA کشف می‌شوند، نیاز به اعتبارسنجی از طریق آزمون فرضیه یا تحلیل اضافی با استفاده از دانش حوزه مرتبط دارند. ما کد cPCA را به عنوان یک بسته پایتون همراه با مستندات و مثال‌ها منتشر کرده‌ایم.

برای داده‌های هدفd -بعدیxi ∈ Rd و داده‌های پس‌زمینهyi ∈ Rd ، بگذاریدC X,C Yماتریس‌های کوواریانس تجربی متناظر آن‌ها باشند. بگذاریدR unit d = def v ∈ Rd :v2 = 1 مجموعه بردارهای واحد باشد. برای هر جهتv ∈ R unit d ، واریانسی که در داده‌های هدف و در داده‌های پس‌زمینه حساب می‌کند، می‌تواند به صورت زیر نوشته شود: واریانس داده‌های هدف :λX ( v ) =defvTCXv , واریانس داده‌های پس‌زمینه :λY ( v ) =defvTCYv.با توجه به یک پارامتر تضادα ≥ 0 که مبادله بین داشتن واریانس هدف بالا و واریانس پس‌زمینه پایین را کمیت می‌کند، cPCA جهت تضادv * را با بهینه‌سازی محاسبه می‌کند 1 v * = argmax v ∈ Runitd λ X(v)- α λ Y(v).  این مسئله می‌تواند به صورت زیر بازنویسی شودv * = argmax v ∈ Runitd v T CX - α CY v ,  که نشان می‌دهدv * متناظر با اولین بردار ویژه ماتریسC = def CX - α CY. بنابراین، جهت‌های تضاد می‌توانند به‌طور کارآمد با استفاده از تجزیه مقدار ویژه محاسبه شوند. مشابه PCA، ما بردارهای ویژه پیشروC را مؤلفه‌های اصلی تضاد (cPCs) می‌نامیم. ما توجه داریم که cPCها بردارهای ویژه ماتریسC هستند و بنابراین به یکدیگر متعامد هستند. برای یکα ثابت، ما ( [1] ) را محاسبه می‌کنیم و زیرفضای گسترده شده توسط چند cPC اول (معمولاً دو) را برمی‌گردانیم.

پارامتر تضادα نمایانگر مبادله بین داشتن واریانس هدف بالا و واریانس پس‌زمینه پایین است. هنگامی کهα = 0، cPCA جهت‌هایی را انتخاب می‌کند که فقط واریانس هدف را به حداکثر می‌رسانند و بنابراین به PCA اعمال شده بر روی داده‌های هدف {x i} کاهش می‌یابد. با افزایشα ، جهت‌هایی با واریانس پس‌زمینه کمتر مهم‌تر می‌شوند و cPCها به سمت فضای تهی داده‌های پس‌زمینه {y i} هدایت می‌شوند. در حالت محدودα = ∞، هر جهتی که در فضای تهی {y i} نباشد، جریمه بی‌نهایت دریافت می‌کند. در این حالت، cPCA متناظر با اولین پیش‌بینی داده‌های هدف بر روی فضای تهی داده‌های پس‌زمینه و سپس انجام PCA بر روی داده‌های پیش‌بینی شده است.

به جای انتخاب یکα واحد و بازگرداندن زیرفضای آن، cPCA زیرفضاهای یک لیست ازα ‌ها را محاسبه می‌کند و چند زیرفضا را که از نظر زاویه اصلی از یکدیگر دور هستند، بازمی‌گرداند. با تصویر کردن داده‌ها بر روی هر یک از این زیرفضاها، روندهای مختلفی در داده‌های هدف آشکار می‌شود و با بررسی بصری نمودارهای پراکندگی که بازگردانده می‌شوند، کاربر می‌تواند به سرعت زیرفضای مرتبط (و مقدار متناظرα ) را برای تحلیل خود تشخیص دهد. برای یک مثال دقیق به شکل تکمیلی Fig. [1] مراجعه کنید.

الگوریتم کامل cPCA در الگوریتم 2 (روش‌های تکمیلی) توصیف شده است. ما معمولاً لیستی از مقادیر بالقوهα را به صورت 40 مقدار به صورت لگاریتمی بین 0.1 و 1000 تنظیم می‌کنیم و این برای تمام آزمایش‌های مقاله استفاده می‌شود. برای انتخاب زیرفضاهای نماینده، cPCA از خوشه‌بندی طیفی برای خوشه‌بندی زیرفضاها استفاده می‌کند، جایی که وابستگی به عنوان حاصل‌ضرب کسینوس زاویه‌های اصلی بین زیرفضاها تعریف می‌شود. سپس مدویدهای (نماینده) هر خوشه به عنوان مقادیرα برای تولید نمودارهای پراکندگی که توسط کاربر دیده می‌شود، استفاده می‌شوند.

انتخاب مجموعه داده‌های پس‌زمینه تأثیر زیادی بر نتیجه cPCA دارد. به طور کلی، داده‌های پس‌زمینه باید ساختاری داشته باشند که ما می‌خواهیم از داده‌های هدف حذف کنیم. چنین ساختاری معمولاً به جهاتی در هدف با واریانس بالا مربوط می‌شود، اما برای تحلیل‌گر جالب نیستند.

ما چند مثال کلی از مجموعه داده‌های پس‌زمینه که ممکن است تضادهای مفیدی با داده‌های هدف ارائه دهند، ارائه می‌دهیم: (1) یک گروه کنترل {y i} که با یک جمعیت بیمار {x i} مقایسه می‌شود زیرا گروه کنترل شامل تغییرات سطح جمعیت مشابه است اما تغییرات ظریف ناشی از زیرگروه‌های مختلف بیماری را ندارد. (2) داده‌ها در زمان صفر {y i} که برای مقایسه با داده‌ها در یک نقطه زمانی بعدی {x i} استفاده می‌شود. این امکان را می‌دهد که تغییرات برجسته در طول زمان به تصویر کشیده شود. (3) یک گروه همگن {y i} که با یک گروه مختلط {x i} مقایسه می‌شود زیرا هر دو دارای تغییرات درون‌جمعیتی و نویز اندازه‌گیری هستند، اما اولی تغییرات بین‌جمعیتی ندارد. (4) یک مجموعه داده پیش‌درمانی {y i} که با داده‌های پس‌درمانی {x i} مقایسه می‌شود تا نویز اندازه‌گیری حذف شود اما تغییرات ناشی از درمان حفظ شود. (5) مجموعه‌ای از ضبط‌های بدون سیگنال {y i} یا تصاویری که فقط نویز دارند، که با اندازه‌گیری‌ها {x i} که شامل هر دو سیگنال و نویز هستند، مقایسه می‌شود.

شایان ذکر است که داده‌های پس‌زمینه نیازی به داشتن ساختار کوواریانس دقیقاً مشابه با آنچه که می‌خواهیم از مجموعه داده هدف حذف کنیم، ندارند. به عنوان مثال، در آزمایشی که در Fig. [2] نشان داده شده است، مشخص شد که نیازی به استفاده از مجموعه داده پس‌زمینه‌ای که شامل تصاویر چمن است، نداریم. در واقع، نتایج مشابهی حتی اگر به جای تصاویر چمن، تصاویر آسمان به عنوان مجموعه داده پس‌زمینه استفاده شود، به دست می‌آید. از آنجا که ساختار ماتریس‌های کوواریانس به اندازه کافی مشابه است، cPCA ساختار پس‌زمینه را از داده‌های هدف حذف می‌کند. علاوه بر این، cPCA نیازی به داشتن تعداد نمونه‌های مشابه در داده‌های هدف و پس‌زمینه ندارد. از آنجا که ماتریس‌های کوواریانس به طور مستقل محاسبه می‌شوند، cPCA فقط نیاز دارد که ماتریس‌های کوواریانس تجربی تخمین‌های خوبی از ماتریس‌های کوواریانس جمعیت زیرین باشند، که اساساً همان نیاز PCA است.

در اینجا، ما تفسیر هندسی cPCA و همچنین ویژگی‌های آماری آن را مورد بحث قرار می‌دهیم. اول، جالب است که در نظر بگیریم کدام جهت‌ها برای هدف تحلیل تضاد بهتر هستند. برای یک جهتv ∈ R unit d ، اهمیت آن در cPCA به طور کامل توسط جفت واریانس هدف-پس‌زمینه آن (λ X(v ),λ Y(v )) تعیین می‌شود؛ مطلوب است که واریانس هدف بالاتر و واریانس پس‌زمینه پایین‌تر باشد. بر اساس این شهود، می‌توانیم یک ترتیب جزئی از تضاد برای جهت‌های مختلف تعریف کنیم: برای دو جهتv1 وv2 ، ممکن است بگوییمv1 یک جهت تضاد بهتر است اگر واریانس هدف بالاتر و واریانس پس‌زمینه پایین‌تری داشته باشد. در این حالت، جفت واریانس هدف-پس‌زمینهv1 در سمت پایین-راست جفتv2 در نمودار جفت‌های واریانس هدف-پس‌زمینه (λ X(v ),λ Y(v )) قرار می‌گیرد، به عنوان مثال، Fig. [5]. بر اساس این ترتیب جزئی، مجموعه‌ای از جهت‌های تضاد بیشتر می‌تواند به روشی مشابه با تعریف مرز پارتو تعریف شود. بگذاریدU  مجموعه جفت‌های واریانس هدف-پس‌زمینه برای همه جهت‌ها باشد، یعنیU = def  ( λX ( v ) ,λY ( v ))v ∈Runitd. مجموعه‌ای از جهت‌های تضاد بیشتر به مرز پایین-راستU در نمودار جفت‌های واریانس هدف-پس‌زمینه مربوط می‌شود، همان‌طور که در Fig.[5]نشان داده شده است. (برای مورد خاص ماتریس‌های پس‌زمینه و هدف که به طور همزمان قابل قطری‌سازی هستند، به شکل تکمیلی Fig.[7]مراجعه کنید.)

در مورد cPCA، می‌توانیم اثبات کنیم (به یادداشت تکمیلی [2] مراجعه کنید) که با تغییرα ، مجموعه cPC‌های برتر با مجموعه‌ای از جهت‌های تضاد بیشتر یکسان است. علاوه بر این، برای جهتv که توسط cPCA با پارامتر تضاد تنظیم شده بهα انتخاب شده است، جفت واریانس آن (λ X(v ),λ Y(v )) به نقطه تماس مرز پایین-راستU  با یک خط شیب-1/α مربوط می‌شود. در نتیجه، با تغییرα از صفر تا بی‌نهایت، cPCA جهت‌هایی با جفت‌های واریانس انتخاب می‌کند که از انتهای پایین-چپ به انتهای بالا-راست مرز پایین-راستU .

همچنین اشاره می‌کنیم که در مورد تصادفی بودن داده‌ها، نرخ همگرایی نمونه cPC به cPC جمعیتO p  dmin ( n,m ) تحت فرض‌های ملایم است، که در آنd بعد وn ,m اندازه‌های داده‌های هدف و پس‌زمینه هستند. این نرخ مشابه نرخ همگرایی استاندارد بردار ویژه نمونه برای یک ماتریس کوواریانس است. به یادداشت تکمیلی [2] مراجعه کنید.

ما یک پیاده‌سازی پایتون از cPCA را در GitHub منتشر کرده‌ایم (https://github.com/abidlabs/contrastive ). مخزن GitHub همچنین شامل نوت‌بوک‌های پایتون و مجموعه داده‌هایی است که بیشتر شکل‌های این مقاله و اطلاعات تکمیلی را بازتولید می‌کنند.

مجموعه داده‌هایی که برای ارزیابی cPCA در این مقاله استفاده شده‌اند، یا از ما یا از نویسندگان مطالعات اصلی در دسترس هستند. لطفاً برای مجموعه داده‌هایی که منتشر کرده‌ایم، به مخزن GitHub ذکر شده در بخش قبلی مراجعه کنید.

Fig. 1: نمای کلی شماتیک از cPCA. برای انجام cPCA، ماتریس‌های کوواریانس C Xو C Yاز داده‌های هدف و پس‌زمینه را محاسبه کنید. بردارهای منفرد تفاوت وزنی ماتریس‌های کوواریانس، C X− α · C Y، جهت‌هایی هستند که توسط cPCA بازگردانده می‌شوند. همان‌طور که در نمودار پراکندگی در سمت راست نشان داده شده است، PCA (بر روی داده‌های هدف) جهتی را شناسایی می‌کند که بیشترین واریانس را در داده‌های هدف دارد، در حالی که cPCA جهتی را شناسایی می‌کند که واریانس بیشتری در داده‌های هدف نسبت به داده‌های پس‌زمینه دارد. تصویر کردن داده‌های هدف بر روی این جهت، الگوهایی منحصر به فرد به داده‌های هدف می‌دهد و اغلب ساختاری را آشکار می‌کند که توسط PCA از دست رفته است. به طور خاص، در این مثال، کاهش ابعاد داده‌های هدف توسط cPCA دو خوشه متمایز را آشکار می‌کند

Fig. 2: PCA متضاد بر روی ارقام نویزی.a, بالا: ما یک مجموعه داده هدف از ۵۰۰۰ تصویر مصنوعی با قرار دادن تصادفی تصاویر ارقام دست‌نویس ۰ و ۱ از مجموعه داده MNIST بر روی تصاویر چمن گرفته شده از مجموعه داده ImageNet که به synset چمن تعلق دارند، ایجاد می‌کنیم. تصاویر چمن به مقیاس خاکستری تبدیل شده، به اندازه ۱۰۰ × ۱۰۰ تغییر اندازه داده شده و سپس به صورت تصادفی به اندازه ارقام MNIST، ۲۸ × ۲۸، برش داده می‌شوند.b, بالا: در اینجا، نتیجه جاسازی تصاویر مصنوعی بر روی دو مؤلفه اصلی اول با استفاده از PCA استاندارد را ترسیم می‌کنیم. می‌بینیم که نقاط مربوط به تصاویر با ۰ و تصاویر با ۱ به سختی قابل تشخیص هستند.a, پایین: سپس یک مجموعه داده پس‌زمینه معرفی می‌شود که تنها شامل تصاویر چمن متعلق به همان synset است، اما از تصاویری استفاده می‌کنیم که با تصاویر استفاده شده برای ایجاد مجموعه داده هدف متفاوت هستند.b, پایین: با استفاده از cPCA بر روی مجموعه داده‌های هدف و پس‌زمینه (با مقدار پارامتر تضاد α تنظیم شده به ۲.۰)، دو خوشه در نمایش کم‌بعد مجموعه داده هدف ظاهر می‌شوند، یکی شامل تصاویر با رقم ۰ و دیگری شامل تصاویر با رقم ۱.cما به مشارکت نسبی هر پیکسل به اولین مؤلفه اصلی (PC) و اولین مؤلفه اصلی تضاد (cPC) نگاه می‌کنیم. پیکسل‌های سفیدتر آن‌هایی هستند که وزن مثبت بیشتری دارند، در حالی که پیکسل‌های تیره‌تر نشان‌دهنده پیکسل‌هایی هستند که وزن منفی دارند. PCA تمایل دارد پیکسل‌های حاشیه تصویر را تأکید کند و پیکسل‌های مرکز و پایین تصویر را کمی کم‌اهمیت کند، که نشان می‌دهد بیشتر واریانس به ویژگی‌های پس‌زمینه مربوط می‌شود. از سوی دیگر، cPCA تمایل دارد پیکسل‌هایی را که در محل ارقام دست‌نویس ۱ هستند، وزن بیشتری دهد، پیکسل‌هایی را که در محل ارقام دست‌نویس ۰ هستند، وزن منفی دهد و بیشتر پیکسل‌های دیگر را نادیده بگیرد، به طور مؤثر ویژگی‌هایی را کشف می‌کند که برای تمایز بین ارقام قرار داده شده مفید هستند

Fig. 3: کشف زیرگروه‌ها در داده‌های زیستی. a ما از PCA برای تصویر کردن یک مجموعه داده بیان پروتئین از موش‌ها با و بدون سندرم داون (DS) بر روی دو مؤلفه اول استفاده می‌کنیم. نمایش کم‌بعد اندازه‌گیری‌های بیان پروتئین از موش‌ها با و بدون DS به طور مشابه توزیع شده‌اند (بالا). اما، هنگامی که از cPCA برای تصویر کردن مجموعه داده بر روی دو cPC اول آن استفاده می‌کنیم، یک نمایش کم‌بعد کشف می‌کنیم که موش‌ها با و بدون DS را به طور جداگانه خوشه‌بندی می‌کند (پایین). b علاوه بر این، ما از PCA و cPCA برای تجسم یک مجموعه داده RNA-Seq تک‌سلولی با ابعاد بالا در دو بعد استفاده می‌کنیم. مجموعه داده شامل چهار نمونه سلولی از دو بیمار مبتلا به لوسمی است: یک نمونه قبل از پیوند از بیمار ۱، یک نمونه بعد از پیوند از بیمار ۱، یک نمونه قبل از پیوند از بیمار ۲، و یک نمونه بعد از پیوند از بیمار ۲. b , چپ: نتایج با استفاده از تنها نمونه‌های بیمار ۱، که نشان می‌دهد cPCA (پایین) نمونه‌ها را مؤثرتر از PCA (بالا) جدا می‌کند. هنگامی که نمونه‌های بیمار دوم نیز شامل می‌شوند، در b , راست، دوباره cPCA (پایین) مؤثرتر از PCA (بالا) در جدا کردن نمونه‌ها است، اگرچه سلول‌های پس از پیوند از هر دو بیمار به طور مشابه توزیع شده‌اند. ما نمودارهای هر نمونه را به طور جداگانه در شکل تکمیلی[5] نشان می‌دهیم، جایی که دیدن همپوشانی بین نمونه‌های مختلف آسان‌تر است

Fig. 4: رابطه بین گروه‌های نژادی مکزیکی. a PCA اعمال شده بر روی داده‌های ژنتیکی از افراد از ۵ ایالت مکزیک هیچ الگوی قابل تشخیص بصری در داده‌های جاسازی شده نشان نمی‌دهد. b cPCA اعمال شده بر روی همان مجموعه داده الگوهایی را در داده‌ها آشکار می‌کند: افراد از همان ایالت در جاسازی cPCA به هم نزدیک‌تر خوشه‌بندی می‌شوند. c علاوه بر این، توزیع نقاط روابط بین گروه‌ها را نشان می‌دهد که با موقعیت جغرافیایی ایالت‌های مختلف مطابقت دارد: به عنوان مثال، افراد از ایالت‌های جغرافیایی مجاور در جاسازی به هم نزدیک هستند. c اقتباس شده از نقشه مکزیک که در اصل کار کاربر:Allstrak در ویکی‌پدیا است، منتشر شده تحت مجوز CC-BY-SA، منبع از https://commons.wikimedia.org/wiki/File:Mexico_Map.svg

Fig. 5: تفسیر هندسی cPCA. مجموعه جفت‌های واریانس هدف-پس‌زمینهUبه عنوان ناحیه فیروزه‌ای برای برخی داده‌های هدف و پس‌زمینه به صورت تصادفی تولید شده ترسیم شده است. مرز پایین-راست، به رنگ طلایی، مربوط به مجموعه جهت‌های متضادترینSλاست. مثلث‌های آبی جفت‌های واریانس برای cPCهایی هستند که با مقادیر α 0.92 و 0.29 به ترتیب انتخاب شده‌اند. ما توجه می‌کنیم که آن‌ها به نقاط تماس منحنی طلایی و خطوط مماس با شیب1α= 1.08, 3.37، به ترتیب مربوط هستند

