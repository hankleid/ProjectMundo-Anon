<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type='text/xsl' href='/ProjectMundo/style/jats-html.xsl'?>
<!DOCTYPE response>
<article article-type="research-article" dtd-version="1.2" specific-use="web-only" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
 <front>
  <journal-meta>
   <journal-id journal-id-type="publisher-id">
    41467
   </journal-id>
   <journal-title-group>
    <journal-title>
     Nature Communications
    </journal-title>
    <abbrev-journal-title abbrev-type="publisher">
     Nat Commun
    </abbrev-journal-title>
   </journal-title-group>
   <issn pub-type="epub">
    2041-1723
   </issn>
   <publisher>
    <publisher-name>
     Nature Publishing Group UK
    </publisher-name>
    <publisher-loc>
     London
    </publisher-loc>
   </publisher>
  </journal-meta>
  <article-meta>
   <article-id pub-id-type="publisher-id">
    s41467-018-04608-8
   </article-id>
   <article-id pub-id-type="manuscript">
    4608
   </article-id>
   <article-id pub-id-type="doi">
    10.1038/s41467-018-04608-8
   </article-id>
   <article-categories>
    <subj-group subj-group-type="heading">
     <subject>
      Article
     </subject>
    </subj-group>
    <subj-group subj-group-type="SubjectPath">
     <subject>
      /631/114
     </subject>
    </subj-group>
    <subj-group subj-group-type="SubjectPath">
     <subject>
      /631/114/1305
     </subject>
    </subj-group>
    <subj-group subj-group-type="SubjectPath">
     <subject>
      /639/705/531
     </subject>
    </subj-group>
    <subj-group subj-group-type="TechniquePath">
     <subject>
      /141
     </subject>
    </subj-group>
    <subj-group subj-group-type="NatureArticleTypeID">
     <subject>
      article
     </subject>
    </subj-group>
   </article-categories>
   <title-group>
    <article-title xml:lang="en">
     Exploring patterns enriched in a dataset with contrastive principal component analysis
    </article-title>
   </title-group>
   <contrib-group>
    <contrib contrib-type="author" equal-contrib="yes" id="Au1">
     <name name-style="western">
      <surname>
       Abid
      </surname>
      <given-names>
       Abubakar
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff1">
      1
     </xref>
     <xref ref-type="author-notes" rid="fn1"/>
    </contrib>
    <contrib contrib-type="author" equal-contrib="yes" id="Au2">
     <name name-style="western">
      <surname>
       Zhang
      </surname>
      <given-names>
       Martin J.
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff1">
      1
     </xref>
     <xref ref-type="author-notes" rid="fn1"/>
    </contrib>
    <contrib contrib-type="author" id="Au3">
     <name name-style="western">
      <surname>
       Bagaria
      </surname>
      <given-names>
       Vivek K.
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff1">
      1
     </xref>
    </contrib>
    <contrib contrib-type="author" corresp="yes" id="Au4">
     <name name-style="western">
      <surname>
       Zou
      </surname>
      <given-names>
       James
      </given-names>
     </name>
     <address>
      <email>
       jamesz@stanford.edu
      </email>
     </address>
     <xref ref-type="aff" rid="Aff2">
      2
     </xref>
     <xref ref-type="aff" rid="Aff3">
      3
     </xref>
     <xref ref-type="corresp" rid="IDs41467018046088_cor4">
      d
     </xref>
    </contrib>
    <aff id="Aff1">
     <label>
      1
     </label>
     <institution-wrap>
      <institution-id institution-id-type="ISNI">
       0000000419368956
      </institution-id>
      <institution-id institution-id-type="GRID">
       grid.168010.e
      </institution-id>
      <institution content-type="org-division">
       Department of Electrical Engineering
      </institution>
      <institution content-type="org-name">
       Stanford University
      </institution>
     </institution-wrap>
     <addr-line content-type="street">
      450 Serra Mall
     </addr-line>
     <addr-line content-type="postcode">
      94305
     </addr-line>
     <addr-line content-type="city">
      Stanford
     </addr-line>
     <addr-line content-type="state">
      CA
     </addr-line>
     <country country="US">
      USA
     </country>
    </aff>
    <aff id="Aff2">
     <label>
      2
     </label>
     <institution-wrap>
      <institution-id institution-id-type="ISNI">
       0000000419368956
      </institution-id>
      <institution-id institution-id-type="GRID">
       grid.168010.e
      </institution-id>
      <institution content-type="org-division">
       Department of Biomedical Data Science
      </institution>
      <institution content-type="org-name">
       Stanford University
      </institution>
     </institution-wrap>
     <addr-line content-type="street">
      450 Serra Mall
     </addr-line>
     <addr-line content-type="postcode">
      94305
     </addr-line>
     <addr-line content-type="city">
      Stanford
     </addr-line>
     <addr-line content-type="state">
      CA
     </addr-line>
     <country country="US">
      USA
     </country>
    </aff>
    <aff id="Aff3">
     <label>
      3
     </label>
     <institution-wrap>
      <institution content-type="org-name">
       Chan-Zuckerberg Biohub
      </institution>
     </institution-wrap>
     <addr-line content-type="street">
      499 Illinois St.
     </addr-line>
     <addr-line content-type="postcode">
      94158
     </addr-line>
     <addr-line content-type="city">
      San Francisco
     </addr-line>
     <addr-line content-type="state">
      CA
     </addr-line>
     <country country="US">
      USA
     </country>
    </aff>
   </contrib-group>
   <author-notes>
    <fn fn-type="equal" id="fn1">
     <p>
      These authors contributed equally: Abubakar Abid, Martin J. Zhang.
     </p>
    </fn>
    <corresp id="IDs41467018046088_cor4">
     <label>
      d
     </label>
     <email>
      jamesz@stanford.edu
     </email>
    </corresp>
   </author-notes>
   <pub-date date-type="pub" publication-format="electronic">
    <day>
     30
    </day>
    <month>
     5
    </month>
    <year>
     2018
    </year>
   </pub-date>
   <pub-date date-type="collection" publication-format="electronic">
    <month>
     12
    </month>
    <year>
     2018
    </year>
   </pub-date>
   <volume>
    9
   </volume>
   <issue seq="2133">
    1
   </issue>
   <elocation-id>
    2134
   </elocation-id>
   <history>
    <date date-type="registration">
     <day>
      14
     </day>
     <month>
      5
     </month>
     <year>
      2018
     </year>
    </date>
    <date date-type="received">
     <day>
      5
     </day>
     <month>
      12
     </month>
     <year>
      2017
     </year>
    </date>
    <date date-type="accepted">
     <day>
      25
     </day>
     <month>
      4
     </month>
     <year>
      2018
     </year>
    </date>
    <date date-type="online">
     <day>
      30
     </day>
     <month>
      5
     </month>
     <year>
      2018
     </year>
    </date>
   </history>
   <permissions>
    <copyright-statement content-type="compact">
     © The Author(s) 2018
    </copyright-statement>
    <copyright-year>
     2018
    </copyright-year>
    <copyright-holder>
     The Author(s)
    </copyright-holder>
    <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/">
     <license-p>
      <bold>
       Open Access
      </bold>
      This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit
      <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">
       http://creativecommons.org/licenses/by/4.0/
      </ext-link>
      .
     </license-p>
    </license>
   </permissions>
   <abstract id="Abs1" xml:lang="en">
    <title>
     Abstract
    </title>
    <p id="Par1">
     Visualization and exploration of high-dimensional data is a ubiquitous challenge across disciplines. Widely used techniques such as principal component analysis (PCA) aim to identify dominant trends in one dataset. However, in many settings we have datasets collected under different conditions, e.g., a treatment and a control experiment, and we are interested in visualizing and exploring patterns that are specific to one dataset. This paper proposes a method, contrastive principal component analysis (cPCA), which identifies low-dimensional structures that are enriched in a dataset relative to comparison data. In a wide variety of experiments, we demonstrate that cPCA with a background dataset enables us to visualize dataset-specific patterns missed by PCA and other standard methods. We further provide a geometric interpretation of cPCA and strong mathematical guarantees. An implementation of cPCA is publicly available, and can be used for exploratory data analysis in many applications where PCA is currently used.
    </p>
   </abstract>
   <abstract abstract-type="ShortSummary" id="Abs2" specific-use="web-only" xml:lang="en">
    <p id="Par2">
     Dimensionality reduction and visualization methods lack a principled way of comparing multiple datasets. Here, Abid et al. introduce contrastive PCA, which identifies low-dimensional structures enriched in one dataset compared to another and enables visualization of dataset-specific patterns.
    </p>
   </abstract>
   <custom-meta-group>
    <custom-meta>
     <meta-name>
      publisher-imprint-name
     </meta-name>
     <meta-value>
      Nature Research
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      volume-issue-count
     </meta-name>
     <meta-value>
      1
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-article-count
     </meta-name>
     <meta-value>
      0
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-toc-levels
     </meta-name>
     <meta-value>
      0
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-pricelist-year
     </meta-name>
     <meta-value>
      2018
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-copyright-holder
     </meta-name>
     <meta-value>
      The Author(s)
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-copyright-year
     </meta-name>
     <meta-value>
      2018
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-contains-esm
     </meta-name>
     <meta-value>
      Yes
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-numbering-style
     </meta-name>
     <meta-value>
      Unnumbered
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-registration-date-year
     </meta-name>
     <meta-value>
      2018
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-registration-date-month
     </meta-name>
     <meta-value>
      5
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-registration-date-day
     </meta-name>
     <meta-value>
      14
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-toc-levels
     </meta-name>
     <meta-value>
      0
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      toc-levels
     </meta-name>
     <meta-value>
      0
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      volume-type
     </meta-name>
     <meta-value>
      Regular
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-product
     </meta-name>
     <meta-value>
      NonStandardArchiveJournal
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      numbering-style
     </meta-name>
     <meta-value>
      Unnumbered
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-grants-type
     </meta-name>
     <meta-value>
      OpenChoice
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      metadata-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      abstract-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      bodypdf-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      bodyhtml-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      bibliography-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      esm-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      online-first
     </meta-name>
     <meta-value>
      false
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      pdf-file-reference
     </meta-name>
     <meta-value>
      BodyRef/PDF/41467_2018_Article_4608.pdf
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      pdf-type
     </meta-name>
     <meta-value>
      Typeset
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      target-type
     </meta-name>
     <meta-value>
      OnlinePDF
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-type
     </meta-name>
     <meta-value>
      Regular
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-type
     </meta-name>
     <meta-value>
      OriginalPaper
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-primary
     </meta-name>
     <meta-value>
      Science, Humanities and Social Sciences, multidisciplinary
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-secondary
     </meta-name>
     <meta-value>
      Science, Humanities and Social Sciences, multidisciplinary
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-secondary
     </meta-name>
     <meta-value>
      Science, multidisciplinary
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-collection
     </meta-name>
     <meta-value>
      Science (multidisciplinary)
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      open-access
     </meta-name>
     <meta-value>
      true
     </meta-value>
    </custom-meta>
   </custom-meta-group>
  </article-meta>
 </front>
 <body>
  <sec id="Sec1" sec-type="introduction">
   <title>
    Introduction
   </title>
   <p id="Par3">
    Principal component analysis (PCA) is one of the most widely used methods for data exploration and visualization
    <sup>
     <xref ref-type="bibr" rid="CR1">
      1
     </xref>
    </sup>
    . PCA projects the data onto a low-dimensional space and is especially powerful as an approach to visualize patterns, such as clusters, clines, and outliers in a dataset
    <sup>
     <xref ref-type="bibr" rid="CR2">
      2
     </xref>
    </sup>
    . There is a large number of related visualization methods; for example, t-SNE
    <sup>
     <xref ref-type="bibr" rid="CR3">
      3
     </xref>
    </sup>
    and multi-dimensional scaling (MDS)
    <sup>
     <xref ref-type="bibr" rid="CR4">
      4
     </xref>
    </sup>
    allow for nonlinear data projections and may better capture nonlinear patterns than PCA. Yet, all of these methods are designed to explore one dataset at a time. When the analyst has multiple datasets (or multiple conditions in one dataset to compare), then the current state-of-practice is to perform PCA (or t-SNE, MDS, etc.) on each dataset separately, and then manually compare the various projections to explore if there are interesting similarities and differences across datasets
    <sup>
     <xref ref-type="bibr" rid="CR5">
      5
     </xref>
     ,
     <xref ref-type="bibr" rid="CR6">
      6
     </xref>
    </sup>
    . Contrastive PCA (cPCA) is designed to fill in this gap in data exploration and visualization by automatically identifying the projections that exhibit the most interesting differences across datasets. Figure
    <xref ref-type="fig" rid="Fig1">
     1
    </xref>
    provides an overview of cPCA that we explain in more detail ahead.
    <fig id="Fig1" position="float">
     <label>
      Fig. 1
     </label>
     <caption xml:lang="en">
      <p>
       Schematic Overview of cPCA. To perform cPCA, compute the covariance matrices
       <italic>
        C
       </italic>
       <sub>
        <italic>
         X
        </italic>
       </sub>
       ,
       <italic>
        C
       </italic>
       <sub>
        <italic>
         Y
        </italic>
       </sub>
       of the target and background datasets. The singular vectors of the weighted difference of the covariance matrices,
       <italic>
        C
       </italic>
       <sub>
        <italic>
         X
        </italic>
       </sub>
       −
       <italic>
        α
       </italic>
       ·
       <italic>
        C
       </italic>
       <sub>
        <italic>
         Y
        </italic>
       </sub>
       , are the directions returned by cPCA. As shown in the scatter plot on the right, PCA (on the target data) identifies the direction that has the highest variance in the target data, while cPCA identifies the direction that has a higher variance in the target data as compared to the background data. Projecting the target data onto the latter direction gives patterns unique to the target data and often reveals structure that is missed by PCA. Specifically, in this example, reducing the dimensionality of the target data by cPCA would reveal two distinct clusters
      </p>
     </caption>
     <graphic mime-subtype="JPEG" specific-use="web" xlink:href="/ProjectMundo/MediaObjects/10X1038_s41467-018-04608-8/41467_2018_4608_Fig1_HTML.jpg"/>
    </fig>
   </p>
   <p id="Par4">
    cPCA is motivated by a broad range of problems across disciplines. For illustration, we mention two such problems here and demonstrate others through experiments later in the paper. First, consider a dataset of gene-expression measurements from individuals of different ethnicities and sexes. This data includes gene-expression levels of cancer patients {
    <bold>
     x
    </bold>
    <sub>
     <italic>
      i
     </italic>
    </sub>
    }, which we are interested in analyzing. We also have control data, which corresponds to the gene-expression levels of healthy patients {
    <bold>
     y
    </bold>
    <sub>
     <italic>
      i
     </italic>
    </sub>
    } from a similar demographic background. Our goal is to find trends and variations within cancer patients (e.g., to identify molecular subtypes of cancer). If we directly apply PCA to {
    <bold>
     x
    </bold>
    <sub>
     <italic>
      i
     </italic>
    </sub>
    }, however, the top principal components may correspond to the demographic variations of the individuals instead of the subtypes of cancers because the genetic variations due to the former are likely to be larger than that of the latter
    <sup>
     <xref ref-type="bibr" rid="CR7">
      7
     </xref>
    </sup>
    . We approach this problem by noting that the healthy patients also contain the variation associated with demographic differences, but not the variation corresponding to subtypes of cancers. Thus, we can search for components in which {
    <bold>
     x
    </bold>
    <sub>
     <italic>
      i
     </italic>
    </sub>
    } has high variance but {
    <bold>
     y
    </bold>
    <sub>
     <italic>
      i
     </italic>
    </sub>
    } has low variance.
   </p>
   <p id="Par5">
    As a related example, consider a dataset {
    <bold>
     x
    </bold>
    <sub>
     <italic>
      i
     </italic>
    </sub>
    } that consists of handwritten digits on a complex background, such as different images of grass (see Fig.
    <xref ref-type="fig" rid="Fig2">
     2(a), top
    </xref>
    ). The goal of a typical unsupervised learning task may be to cluster the data, revealing the different digits in the image. However, if we apply standard PCA on these images, we find that the top principal components do not represent features related to the handwritten digits, but reflect the dominant variation in features related to the image background (Fig.
    <xref ref-type="fig" rid="Fig2">
     2(b)
    </xref>
    , top). We show that it is possible to correct for this by using a reference dataset {
    <bold>
     y
    </bold>
    <sub>
     <italic>
      i
     </italic>
    </sub>
    } that consists solely of images of the grass (not necessarily the same images used in {
    <bold>
     x
    </bold>
    <sub>
     <italic>
      i
     </italic>
    </sub>
    } but having similar covariance between features, as shown in Fig.
    <xref ref-type="fig" rid="Fig2">
     2(a)
    </xref>
    , bottom), and looking for the subspace of higher variance in {
    <bold>
     x
    </bold>
    <sub>
     <italic>
      i
     </italic>
    </sub>
    } compared to {
    <bold>
     y
    </bold>
    <sub>
     <italic>
      i
     </italic>
    </sub>
    }. By projecting onto this subspace, we can actually visually separate the images based on the value of the handwritten digit (Fig. 2(b), bottom). By comparing the principal components discovered by PCA with those discovered by cPCA, we see that cPCA identifies more relevant features (Fig.
    <xref ref-type="fig" rid="Fig2">
     2(c)
    </xref>
    ), which allows us to use cPCA for such applications as feature selection and denoising
    <sup>
     <xref ref-type="bibr" rid="CR8">
      8
     </xref>
    </sup>
    .
    <fig id="Fig2" position="float">
     <label>
      Fig. 2
     </label>
     <caption xml:lang="en">
      <p>
       Contrastive PCA on Noisy Digits.
       <bold>
        a
       </bold>
       , Top: We create a target dataset of 5,000 synthetic images by randomly superimposing images of handwritten digits 0 and 1 from MNIST dataset
       <sup>
        <xref ref-type="bibr" rid="CR32">
         32
        </xref>
       </sup>
       on top of images of grass taken from ImageNet dataset
       <sup>
        <xref ref-type="bibr" rid="CR33">
         33
        </xref>
       </sup>
       belonging to the synset grass. The images of grass are converted to grayscale, resized to be 100 × 100, and then randomly cropped to be the same size as the MNIST digits, 28 × 28.
       <bold>
        b
       </bold>
       , Top: Here, we plot the result of embedding the synthetic images onto their first two principal components using standard PCA. We see that the points corresponding to the images with 0’s and images with 1’s are hard to distinguish.
       <bold>
        a
       </bold>
       , Bottom: A background dataset is then introduced consisting solely of images of grass belonging to the same synset, but we use images that are different than those used to create the target dataset.
       <bold>
        b
       </bold>
       , Bottom: Using cPCA on the target and background datasets (with a value of the contrast parameter
       <italic>
        α
       </italic>
       set to 2.0), two clusters emerge in the lower-dimensional representation of the target dataset, one consisting of images with the digit 0 and the other of images with the digit 1.
       <bold>
        c
       </bold>
       We look at the relative contribution of each pixel to the first principal component (PC) and first contrastive principal component (cPC). Whiter pixels are those that carry a more positive weight, while darker denotes those pixels that carry negative weights. PCA tends to emphasize pixels in the periphery of the image and slightly de-emphasize pixels in the center and bottom of the image, indicating that most of the variance is due to background features. On the other hand, cPCA tends to upweight the pixels that are at the location of the handwritten 1’s, negatively weight pixels at the location of handwritten 0’s, and neglect most other pixels, effectively discovering those features useful for discriminating between the superimposed digits
      </p>
     </caption>
     <graphic mime-subtype="JPEG" specific-use="web" xlink:href="/ProjectMundo/MediaObjects/10X1038_s41467-018-04608-8/41467_2018_4608_Fig2_HTML.jpg"/>
    </fig>
   </p>
   <p id="Par6">
    Contrastive PCA is a tool for unsupervised learning, which efficiently reduces dimensionality to enable visualization and exploratory data analysis. This separates cPCA from a large class of supervised learning methods whose primary goal is to classify or discriminate between various datasets, such as linear discriminant analysis (LDA)
    <sup>
     <xref ref-type="bibr" rid="CR9">
      9
     </xref>
    </sup>
    , quadratic discriminant analysis (QDA)
    <sup>
     <xref ref-type="bibr" rid="CR10">
      10
     </xref>
    </sup>
    , supervised PCA
    <sup>
     <xref ref-type="bibr" rid="CR11">
      11
     </xref>
    </sup>
    , and QUADRO
    <sup>
     <xref ref-type="bibr" rid="CR12">
      12
     </xref>
    </sup>
    . This also dinstinguishes cPCA from methods that integrate multiple datasets
    <sup>
     <xref ref-type="bibr" rid="CR13">
      13
     </xref>
     ,
     <xref ref-type="bibr" rid="CR14">
      14
     </xref>
     ,
     <xref ref-type="bibr" rid="CR15">
      15
     </xref>
     –
     <xref ref-type="bibr" rid="CR16">
      16
     </xref>
    </sup>
    , with the goal of identifying correlated patterns among two or more datasets, rather than those unique to each individual dataset. There is also a rich family of unsupervised methods for dimension reduction besides PCA. For example, multi-dimensional scaling (MDS)
    <sup>
     <xref ref-type="bibr" rid="CR4">
      4
     </xref>
    </sup>
    finds a low-dimensional embedding that preserves the distance in the high-dimensional space; principal component pursuit
    <sup>
     <xref ref-type="bibr" rid="CR17">
      17
     </xref>
    </sup>
    finds a low-rank subspace that is robust to small entry-wise noise and gross sparse errors. But none are designed to utilize relevant information from a second dataset, as cPCA does. In the supplement, we have compared cPCA to many of the previously-mentioned techniques on representative datasets (see Supplementary Figs.
    <xref ref-type="supplementary-material" rid="MOESM1">
     3
    </xref>
    and
    <xref ref-type="supplementary-material" rid="MOESM1">
     4
    </xref>
    ).
   </p>
   <p id="Par7">
    In a specific application domain, there may be specialized tools in that domain with similar goals as cPCA
    <sup>
     <xref ref-type="bibr" rid="CR18">
      18
     </xref>
     ,
     <xref ref-type="bibr" rid="CR19">
      19
     </xref>
     –
     <xref ref-type="bibr" rid="CR20">
      20
     </xref>
    </sup>
    . For example, in the results, we show how cPCA applied on genotype data visualizes geographical ancestry within Mexico. Exploring fine-grained clusters of genetic ancestries is an important problem in population genetics, and researchers have recently developed an algorithm to specifically visualize such ancestry clusters
    <sup>
     <xref ref-type="bibr" rid="CR18">
      18
     </xref>
    </sup>
    . While cPCA performs well here, the expert-crafted algorithm might perform even better for a specific dataset. However, the specialized algorithm requires substantial domain knowledge to design, is more computationally expensive, and can be challenging to use. The goal of cPCA is not to replace all these specialized state-of-the-art methods in each of their domains, but to provide a general method for exploring arbitrary datasets.
   </p>
   <p id="Par8">
    We propose a concrete and efficient algorithm for cPCA in this paper. The method takes as input a target dataset {
    <bold>
     x
    </bold>
    <sub>
     <italic>
      i
     </italic>
    </sub>
    } that we are interested in visualizing or identifying patterns within. As a secondary input, cPCA takes a background dataset {
    <bold>
     y
    </bold>
    <sub>
     <italic>
      i
     </italic>
    </sub>
    }, which does not contain the patterns of interest. The cPCA algorithm returns subspaces which capture a large amount of variation in the target data {
    <bold>
     x
    </bold>
    <sub>
     <italic>
      i
     </italic>
    </sub>
    }, but little in the background {
    <bold>
     y
    </bold>
    <sub>
     <italic>
      i
     </italic>
    </sub>
    } (see Fig.
    <xref ref-type="fig" rid="Fig1">
     1
    </xref>
    , Methods, and Supplementary Methods for more details). This subspace corresponds to features containing structure specific to {
    <bold>
     x
    </bold>
    <sub>
     <italic>
      i
     </italic>
    </sub>
    }. Hence, when the target data is projected onto this subspace, we are able to visualize and discover the additional structure in the target data relative to the background. Analogous to the principal components (PCs), we call the directions found by cPCA the contrastive principal components (cPCs). We emphasize that cPCA is fundamentally an unsupervised technique, designed to resolve patterns in one dataset more clearly by using the background dataset as a contrast. In particular, cPCA does not seek to discriminate between the target and background datasets; the subspace that contains trends that are enriched in the target dataset is not necessarily the same subspace that is optimal for classification between the datasets.
   </p>
  </sec>
  <sec id="Sec2" sec-type="results">
   <title>
    Results
   </title>
   <sec id="Sec3">
    <title>
     Subgroup discovery in protein expression data
    </title>
    <p id="Par9">
     Researchers have noted that standard PCA is often ineffective at discovering subgroups within biological data, at least in part because “dominant principal components…correlate with artifacts,”
     <sup>
      <xref ref-type="bibr" rid="CR21">
       21
      </xref>
     </sup>
     rather than with features that are of interest to the researcher. How can cPCA be used in these settings to detect the more significant subgroups? By using a background dataset to cancel out the universal but uninteresting variation in the target, we can search for structure that is unique to the target dataset.
    </p>
    <p id="Par10">
     Our first experiment uses a dataset consisting of protein expression measurements of mice that have received shock therapy
     <sup>
      <xref ref-type="bibr" rid="CR22">
       22
      </xref>
      ,
      <xref ref-type="bibr" rid="CR23">
       23
      </xref>
     </sup>
     . Some of the mice have developed Down Syndrome (DS). To create an unsupervised learning task where we have ground truth information to evaluate the methods, we assume this DS information is not known to the analyst and only use it for algorithm evaluation. We would like to see if we detect any significant differences within the shocked mice population in an unsupervised manner (the presence or absence of Down Syndrome being a key example). In Fig.
     <xref ref-type="fig" rid="Fig3">
      3a
     </xref>
     (top), we show the result of applying PCA to the target dataset: the transformed data does not reveal any significant clustering within the population of mice. The major sources of variation within mice may be natural, such as sex or age.
     <fig id="Fig3" position="float">
      <label>
       Fig. 3
      </label>
      <caption xml:lang="en">
       <p>
        Discovering subgroups in biological data.
        <bold>
         a
        </bold>
        We use PCA to project a protein expression dataset of mice with and without Down Syndrome (DS) onto the first two components. The lower-dimensional representation of protein expression measurements from mice with and without DS are seen to be distributed similarly (top). But, when we use cPCA to project the dataset onto its first two cPCs, we discover a lower-dimensional representation that clusters mice with and without DS separately (bottom).
        <bold>
         b
        </bold>
        Furthermore, we use PCA and cPCA to visualize a high-dimensional single-cell RNA-Seq dataset in two dimensions. The dataset consists of four cell samples from two leukemia patients: a pre-transplant sample from patient 1, a post-transplant sample from patient 1, a pre-transplant sample from patient 2, and a post-transplant sample from patient 2.
        <bold>
         b
        </bold>
        , left: The results using only the samples from patient 1, which demonstrate that cPCA (bottom) more effectively separates the samples than PCA (top). When the samples from the second patient are included, in
        <bold>
         b
        </bold>
        , right, again cPCA (bottom) is more effective than PCA (top) at separating the samples, although the post-transplant cells from both patients are similarly-distributed. We show plots of each sample separately in Supplementary Fig.
        <xref ref-type="supplementary-material" rid="MOESM1">
         5
        </xref>
        , where it is easier to see the overlap between different samples
       </p>
      </caption>
      <graphic mime-subtype="JPEG" specific-use="web" xlink:href="/ProjectMundo/MediaObjects/10X1038_s41467-018-04608-8/41467_2018_4608_Fig3_HTML.jpg"/>
     </fig>
    </p>
    <p id="Par11">
     We apply cPCA to this dataset using a background consists of protein expression measurements from a set of mice that have not been exposed to shock therapy. They are control mice that likely have similar natural variation as the experimental mice, but without the differences that result from the shock therapy. With this dataset as a background, cPCA is able to resolve two different groups in the transformed target data, one corresponding to mice that do not have Down Syndrome and one corresponding (mostly) to mice that have Down Syndrome, as illustrated in Fig.
     <xref ref-type="fig" rid="Fig3">
      3a
     </xref>
     (bottom). As a comparison, we also applied 8 other dimensionality reduction techniques to identify directions that differentiate between the target and background datasets, none of which were able to separate the mice as well as cPCA (see Supplementary Fig.
     <xref ref-type="supplementary-material" rid="MOESM1">
      4
     </xref>
     for details).
    </p>
   </sec>
   <sec id="Sec4">
    <title>
     Subgroup discovery in single-cell RNA-Seq data
    </title>
    <p id="Par12">
     Next, we analyze a higher-dimensional public dataset consisting of single-cell RNA expression levels of a mixture of bone marrow mononuclear cells (BMMCs) taken from a leukemia patient before stem-cell transplant and BMMCs from the same patient after stem-cell transplant
     <sup>
      <xref ref-type="bibr" rid="CR24">
       24
      </xref>
     </sup>
     . All single-cell RNA-Seq data is preprocessed using similar methods as described by the authors. In particular, before applying PCA or cPCA, all datasets are reduced to 500 genes, which are selected on the basis of highest dispersion [variance divided by mean] within the target data. Again, we perform PCA to see if we can visually discover the two samples in the transformed data. As shown in Fig.
     <xref ref-type="fig" rid="Fig3">
      3b
     </xref>
     (top left), both cell types follow a similar distribution in the space spanned by the first two PCs. This is likely because the differences between the samples is small and the PCs instead reflect the heterogeneity of various kinds of cells within each sample or even variations in experimental conditions, which can have a significant effect on single-cell RNA-Seq measurements
     <sup>
      <xref ref-type="bibr" rid="CR25">
       25
      </xref>
     </sup>
     .
    </p>
    <p id="Par13">
     We apply cPCA using a background dataset that consists of RNA-Seq measurements from a healthy individual’s BMMC cells. We expect that this background dataset to contain the variation due to the heterogeneous population of cells as well as variations in experimental conditions. We may hope, then, that cPCA might be able to recover directions that are enriched in the target data, corresponding to pre- and post-transplant differences. Indeed, that is what we find, as shown in Fig.
     <xref ref-type="fig" rid="Fig3">
      3b
     </xref>
     (bottom left).
    </p>
    <p id="Par14">
     We further augment our target dataset with BMMC samples from a second leukemia patient, again before and after stem-cell transplant. Thus, there are a total of four subpopulations of cells. Application of PCA on this data shows that the four subpopulations are not separable in the subspace spanned by the top two principal components (PCs), as shown in Fig.
     <xref ref-type="fig" rid="Fig3">
      3b
     </xref>
     (top right). Again, however, when cPCA is applied with the same background dataset, at least three of the subpopulations show much stronger separation, as shown in Fig.
     <xref ref-type="fig" rid="Fig3">
      3b
     </xref>
     (bottom right). The cPCA embedding also suggests that the cell samples from both patients are more similar to each other after stem-cell transplant (cyan and green dots) than before the transplant (gold and pink dots), a reasonable hypothesis which can be tested by the investigator. One may refer to Supplementary Fig.
     <xref ref-type="supplementary-material" rid="MOESM1">
      5
     </xref>
     for more details of this experiment. We see that cPCA can be a useful tool to infer the relationship between subpopulations, a topic we explore further next.
    </p>
   </sec>
   <sec id="Sec5">
    <title>
     Relationship between ancestral groups in Mexico
    </title>
    <p id="Par15">
     In previous examples, we have seen that cPCA allows the user to discover subclasses within a target dataset that are not labeled a priori. However, even when subclasses are known ahead of time, dimensionality reduction can be a useful way to visualize the relationship within groups. For example, PCA is often used to visualize the relationship between ethnic populations based on genetic variants, because projecting the genetic variants onto two dimensions often produces maps that offer striking visualizations of geographic and historic trends
     <sup>
      <xref ref-type="bibr" rid="CR26">
       26
      </xref>
      ,
      <xref ref-type="bibr" rid="CR27">
       27
      </xref>
     </sup>
     . But again, PCA is limited to identifying the most dominant structure; when this represents universal or uninteresting variation, cPCA can be more effective at visualizing trends.
    </p>
    <p id="Par16">
     The dataset that we use for this example consists of single nucleotide polymorphisms (SNPs) from the genomes of individuals from five states in Mexico, collected in a previous study
     <sup>
      <xref ref-type="bibr" rid="CR28">
       28
      </xref>
     </sup>
     . Mexican ancestry is challenging to analyze using PCA since the PCs usually do not reflect geographic origin within Mexico; instead, they reflect the proportion of European/Native American heritage of each Mexican individual, which dominates and obscures differences due to geographic origin within Mexico (see Fig.
     <xref ref-type="fig" rid="Fig4">
      4a
     </xref>
     ). To overcome this problem, population geneticists manually prune SNPs, removing those known to derive from Europeans ancestry, before applying PCA. However, this procedure is of limited applicability since it requires knowing the origin of the SNPs and that the source of background variation to be very different from the variation of interest, which are often not the case.
     <fig id="Fig4" position="float">
      <label>
       Fig. 4
      </label>
      <caption xml:lang="en">
       <p>
        Relationship between Mexican ancestry groups.
        <bold>
         a
        </bold>
        PCA applied to genetic data from individuals from 5 Mexican states does not reveal any visually discernible patterns in the embedded data.
        <bold>
         b
        </bold>
        cPCA applied to the same dataset reveals patterns in the data: individuals from the same state are clustered closer together in the cPCA embedding.
        <bold>
         c
        </bold>
        Furthermore, the distribution of the points reveals relationships between the groups that matches the geographic location of the different states: for example, individuals from geographically adjacent states are adjacent in the embedding.
        <bold>
         c
        </bold>
        Adapted from a map of Mexico that is originally the work of User:Allstrak at Wikipedia, published under a CC-BY-SA license, sourced from
        <ext-link ext-link-type="uri" xlink:href="https://commons.wikimedia.org/wiki/File:Mexico_Map.svg">
         https://commons.wikimedia.org/wiki/File:Mexico_Map.svg
        </ext-link>
       </p>
      </caption>
      <graphic mime-subtype="JPEG" specific-use="web" xlink:href="/ProjectMundo/MediaObjects/10X1038_s41467-018-04608-8/41467_2018_4608_Fig4_HTML.jpg"/>
     </fig>
    </p>
    <p id="Par17">
     As an alternative, we use cPCA with a background dataset that consists of individuals from Mexico and from Europe. This background is dominated by Native American/European variation, allowing us to isolate the intra-Mexican variation in the target dataset. The results of applying cPCA are shown in Fig.
     <xref ref-type="fig" rid="Fig4">
      4b
     </xref>
     . We find that individuals from the same state in Mexico are embedded closer together. Furthermore, the two groups that are the most divergent are the Sonorans and the Mayans from Yucatan, which are also the most geographically distant within Mexico, while Mexicans from the other three states are close to each other, both geographically as well as in the embedding captured by cPCA (see Fig.
     <xref ref-type="fig" rid="Fig4">
      4c
     </xref>
     ). See also Supplementary Fig.
     <xref ref-type="supplementary-material" rid="MOESM1">
      6
     </xref>
     for more details.
    </p>
   </sec>
  </sec>
  <sec id="Sec6" sec-type="discussion">
   <title>
    Discussion
   </title>
   <p id="Par18">
    In many data science settings, we are interested in visualizing and exploring patterns that are enriched in one dataset relative to other data. We have presented cPCA as a general tool for performing such contrastive exploration, and we have illustrated its usefulness in a diverse range of applications. The main advantages of cPCA are its generality and ease of use. Computing a particular cPCA takes essentially the same amount of time as computing a regular PCA. This computational efficiency enables cPCA to be useful for interactive data exploration, where each operation should ideally be almost immediate. As such, any settings where PCA is applied on related datasets, cPCA can also be applied. In the Supplementary Note
    <xref ref-type="supplementary-material" rid="MOESM1">
     3
    </xref>
    and Supplementary Fig.
    <xref ref-type="supplementary-material" rid="MOESM1">
     8
    </xref>
    , we show how cPCA can be kernelized to uncover nonlinear contrastive patterns in datasets.
   </p>
   <p id="Par19">
    The only free parameter of contrastive PCA is the contrast strength
    <italic>
     α
    </italic>
    . In our default algorithm, we developed an automatic scheme based on clusterings of subspaces for selecting the most informative values of
    <italic>
     α
    </italic>
    (see Methods). All of the experiments performed for this paper use the automatically generated
    <italic>
     α
    </italic>
    values, and we believe this default will be sufficient in many applications of cPCA. The user may also input specific values for
    <italic>
     α
    </italic>
    if more fine-grained exploration is desired.
   </p>
   <p id="Par20">
    cPCA, like regular PCA and other dimensionality reduction methods, does not give
    <italic>
     p
    </italic>
    -values or other statistical significance quantifications. The patterns discovered through cPCA need to be validated through hypothesis testing or additional analysis using relevant domain knowledge. We have released the code for cPCA as a python package along with documentation and examples.
   </p>
  </sec>
  <sec id="Sec7" sec-type="materials|methods">
   <title>
    Methods
   </title>
   <sec id="Sec8">
    <title>
     Description of the Algorithm
    </title>
    <p id="Par21">
     For the
     <italic>
      d
     </italic>
     -dimensional target data
     <inline-formula id="IEq1">
      <alternatives>
       <math id="IEq1_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mfenced close="}" open="{" separators="">
         <mrow>
          <msub>
           <mrow>
            <mi mathvariant="bold">
             x
            </mi>
           </mrow>
           <mrow>
            <mi>
             i
            </mi>
           </mrow>
          </msub>
          <mo>
           ∈
          </mo>
          <msup>
           <mrow>
            <mi mathvariant="double-struck">
             R
            </mi>
           </mrow>
           <mrow>
            <mi>
             d
            </mi>
           </mrow>
          </msup>
         </mrow>
        </mfenced>
       </math>
       <tex-math id="IEq1_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{ {{\bf{x}}_i \in {\Bbb R}^d} \right\}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_IEq1.gif"/>
      </alternatives>
     </inline-formula>
     and background data
     <inline-formula id="IEq2">
      <alternatives>
       <math id="IEq2_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mfenced close="}" open="{" separators="">
         <mrow>
          <msub>
           <mrow>
            <mi mathvariant="bold">
             y
            </mi>
           </mrow>
           <mrow>
            <mi>
             i
            </mi>
           </mrow>
          </msub>
          <mo>
           ∈
          </mo>
          <msup>
           <mrow>
            <mi mathvariant="double-struck">
             R
            </mi>
           </mrow>
           <mrow>
            <mi>
             d
            </mi>
           </mrow>
          </msup>
         </mrow>
        </mfenced>
       </math>
       <tex-math id="IEq2_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{ {{\bf{y}}_i \in {\Bbb R}^d} \right\}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_IEq2.gif"/>
      </alternatives>
     </inline-formula>
     , let
     <italic>
      C
     </italic>
     <sub>
      <italic>
       X
      </italic>
     </sub>
     ,
     <italic>
      C
     </italic>
     <sub>
      <italic>
       Y
      </italic>
     </sub>
     be their corresponding empirical covariance matrices. Let
     <inline-formula id="IEq3">
      <alternatives>
       <math id="IEq3_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi mathvariant="double-struck">
           R
          </mi>
         </mrow>
         <mrow>
          <mi mathvariant="normal">
           unit
          </mi>
         </mrow>
         <mrow>
          <mi>
           d
          </mi>
         </mrow>
        </msubsup>
        <mover>
         <mrow>
          <mo>
           =
          </mo>
         </mrow>
         <mrow>
          <mi mathvariant="normal">
           def
          </mi>
         </mrow>
        </mover>
        <mfenced close="}" open="{" separators="">
         <mrow>
          <mi mathvariant="bold">
           v
          </mi>
          <mo>
           ∈
          </mo>
          <msup>
           <mrow>
            <mi mathvariant="double-struck">
             R
            </mi>
           </mrow>
           <mrow>
            <mi>
             d
            </mi>
           </mrow>
          </msup>
          <mo>
           :
          </mo>
          <msub>
           <mrow>
            <mfenced close="∥" open="∥" separators="">
             <mrow>
              <mi mathvariant="bold">
               v
              </mi>
             </mrow>
            </mfenced>
           </mrow>
           <mrow>
            <mn>
             2
            </mn>
           </mrow>
          </msub>
          <mo>
           =
          </mo>
          <mn>
           1
          </mn>
         </mrow>
        </mfenced>
       </math>
       <tex-math id="IEq3_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\Bbb R}_{{\mathrm{unit}}}^d\mathop { = }\limits^{{\kern 1pt} {\mathrm{def}}} \left\{ {{\bf{v}} \in {\Bbb R}^d:\left\| {\bf{v}} \right\|_2 = 1} \right\}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_IEq3.gif"/>
      </alternatives>
     </inline-formula>
     be the set of unit vectors. For any direction
     <inline-formula id="IEq4">
      <alternatives>
       <math id="IEq4_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi mathvariant="bold">
         v
        </mi>
        <mo>
         ∈
        </mo>
        <msubsup>
         <mrow>
          <mi mathvariant="double-struck">
           R
          </mi>
         </mrow>
         <mrow>
          <mi mathvariant="normal">
           unit
          </mi>
         </mrow>
         <mrow>
          <mi>
           d
          </mi>
         </mrow>
        </msubsup>
       </math>
       <tex-math id="IEq4_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bf{v}} \in {\Bbb R}_{{\mathrm{unit}}}^d$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_IEq4.gif"/>
      </alternatives>
     </inline-formula>
     , the variance it accounts for in the target data and in the background data can be written as:
     <disp-formula id="Equa">
      <alternatives>
       <math display="block" id="Equa_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mtable>
         <mtr>
          <mtd columnalign="left">
           <mi mathvariant="normal">
            Target
           </mi>
           <mi mathvariant="normal">
            data
           </mi>
           <mi mathvariant="normal">
            variance :
           </mi>
           <mspace width="1em"/>
           <msub>
            <mrow>
             <mi>
              λ
             </mi>
            </mrow>
            <mrow>
             <mi>
              X
             </mi>
            </mrow>
           </msub>
           <mrow>
            <mo>
             (
            </mo>
            <mrow>
             <mi mathvariant="bold">
              v
             </mi>
            </mrow>
            <mo>
             )
            </mo>
           </mrow>
           <mover>
            <mrow>
             <mo>
              =
             </mo>
            </mrow>
            <mrow>
             <mi mathvariant="normal">
              def
             </mi>
            </mrow>
           </mover>
           <msup>
            <mrow>
             <mi mathvariant="bold">
              v
             </mi>
            </mrow>
            <mrow>
             <mi>
              T
             </mi>
            </mrow>
           </msup>
           <msub>
            <mrow>
             <mi>
              C
             </mi>
            </mrow>
            <mrow>
             <mi>
              X
             </mi>
            </mrow>
           </msub>
           <mi mathvariant="bold">
            v
           </mi>
           <mo>
            ,
           </mo>
          </mtd>
         </mtr>
         <mtr>
          <mtd columnalign="left">
           <mi mathvariant="normal">
            Background
           </mi>
           <mi mathvariant="normal">
            data
           </mi>
           <mi mathvariant="normal">
            variance :
           </mi>
           <mspace width="1em"/>
           <msub>
            <mrow>
             <mi>
              λ
             </mi>
            </mrow>
            <mrow>
             <mi>
              Y
             </mi>
            </mrow>
           </msub>
           <mrow>
            <mo>
             (
            </mo>
            <mrow>
             <mi mathvariant="bold">
              v
             </mi>
            </mrow>
            <mo>
             )
            </mo>
           </mrow>
           <mover>
            <mrow>
             <mo>
              =
             </mo>
            </mrow>
            <mrow>
             <mi mathvariant="normal">
              def
             </mi>
            </mrow>
           </mover>
           <msup>
            <mrow>
             <mi mathvariant="bold">
              v
             </mi>
            </mrow>
            <mrow>
             <mi>
              T
             </mi>
            </mrow>
           </msup>
           <msub>
            <mrow>
             <mi>
              C
             </mi>
            </mrow>
            <mrow>
             <mi>
              Y
             </mi>
            </mrow>
           </msub>
           <mi mathvariant="bold">
            v
           </mi>
           <mo>
            .
           </mo>
          </mtd>
         </mtr>
        </mtable>
       </math>
       <tex-math id="Equa_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{l}{\mathrm{Target}}{\kern 1pt} {\mathrm{data}}{\kern 1pt} {\mathrm{variance:}}\quad \lambda _X({\mathbf{v}})\mathop { = }\limits^{{\mathrm{def}}} {\bf{v}}^TC_X{\bf{v}},\\ {\mathrm{Background}}{\kern 1pt} {\mathrm{data}}{\kern 1pt} {\mathrm{variance:}}\quad \lambda _Y({\bf{v}})\mathop { = }\limits^{{\mathrm{def}}} {\bf{v}}^TC_Y{\bf{v}}.\end{array}$$\end{document}
       </tex-math>
       <graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_Equa.gif"/>
      </alternatives>
     </disp-formula>
     Given a contrast parameter
     <italic>
      α
     </italic>
     ≥ 0 that quantifies the trade-off between having high target variance and low background variance, cPCA computes the contrastive direction
     <bold>
      v
     </bold>
     * by optimizing
     <disp-formula id="Equ1">
      <label>
       1
      </label>
      <alternatives>
       <math display="block" id="Equ1_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi mathvariant="bold">
           v
          </mi>
         </mrow>
         <mrow>
          <mo>
           *
          </mo>
         </mrow>
        </msup>
        <mo>
         =
        </mo>
        <msub>
         <mrow>
          <mi mathvariant="normal">
           argmax
          </mi>
         </mrow>
         <mrow>
          <mi mathvariant="bold">
           v
          </mi>
          <mo>
           ∈
          </mo>
          <msubsup>
           <mrow>
            <mi mathvariant="double-struck">
             R
            </mi>
           </mrow>
           <mrow>
            <mi mathvariant="normal">
             unit
            </mi>
           </mrow>
           <mrow>
            <mi>
             d
            </mi>
           </mrow>
          </msubsup>
         </mrow>
        </msub>
        <msub>
         <mrow>
          <mi>
           λ
          </mi>
         </mrow>
         <mrow>
          <mi>
           X
          </mi>
         </mrow>
        </msub>
        <mrow>
         <mo>
          (
         </mo>
         <mrow>
          <mi mathvariant="bold">
           v
          </mi>
         </mrow>
         <mo>
          )
         </mo>
        </mrow>
        <mo>
         -
        </mo>
        <mi>
         α
        </mi>
        <msub>
         <mrow>
          <mi>
           λ
          </mi>
         </mrow>
         <mrow>
          <mi>
           Y
          </mi>
         </mrow>
        </msub>
        <mrow>
         <mo>
          (
         </mo>
         <mrow>
          <mi mathvariant="bold">
           v
          </mi>
         </mrow>
         <mo>
          )
         </mo>
        </mrow>
        <mo>
         .
        </mo>
       </math>
       <tex-math id="Equ1_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bf{v}}^ \ast = {\mathrm{argmax}}_{{\bf{v}} \in {\Bbb R}_{{\mathrm{unit}}}^d}\lambda _X({\bf{v}}) - \alpha \lambda _Y({\bf{v}}).$$\end{document}
       </tex-math>
       <graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_Equ1.gif"/>
      </alternatives>
     </disp-formula>
     This problem can be rewritten as
     <disp-formula id="Equb">
      <alternatives>
       <math display="block" id="Equb_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi mathvariant="bold">
           v
          </mi>
         </mrow>
         <mrow>
          <mo>
           *
          </mo>
         </mrow>
        </msup>
        <mo>
         =
        </mo>
        <msub>
         <mrow>
          <mi mathvariant="normal">
           argmax
          </mi>
         </mrow>
         <mrow>
          <mi mathvariant="bold">
           v
          </mi>
          <mo>
           ∈
          </mo>
          <msubsup>
           <mrow>
            <mi mathvariant="double-struck">
             R
            </mi>
           </mrow>
           <mrow>
            <mi mathvariant="normal">
             unit
            </mi>
           </mrow>
           <mrow>
            <mi>
             d
            </mi>
           </mrow>
          </msubsup>
         </mrow>
        </msub>
        <msup>
         <mrow>
          <mi mathvariant="bold">
           v
          </mi>
         </mrow>
         <mrow>
          <mi>
           T
          </mi>
         </mrow>
        </msup>
        <mfenced close=")" open="(" separators="">
         <mrow>
          <msub>
           <mrow>
            <mi>
             C
            </mi>
           </mrow>
           <mrow>
            <mi>
             X
            </mi>
           </mrow>
          </msub>
          <mo>
           -
          </mo>
          <mi>
           α
          </mi>
          <msub>
           <mrow>
            <mi>
             C
            </mi>
           </mrow>
           <mrow>
            <mi>
             Y
            </mi>
           </mrow>
          </msub>
         </mrow>
        </mfenced>
        <mi mathvariant="bold">
         v
        </mi>
        <mo>
         ,
        </mo>
       </math>
       <tex-math id="Equb_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bf{v}}^ \ast = {\mathrm{argmax}}_{{\mathbf{v}} \in {\Bbb R}_{{\mathrm{unit}}}^d}{\bf{v}}^T\left( {C_X - \alpha C_Y} \right){\bf{v}},$$\end{document}
       </tex-math>
       <graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_Equb.gif"/>
      </alternatives>
     </disp-formula>
     which implies that
     <bold>
      v
     </bold>
     * corresponds to the first eigenvector of the matrix
     <inline-formula id="IEq5">
      <alternatives>
       <math id="IEq5_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         C
        </mi>
        <mover>
         <mrow>
          <mo>
           =
          </mo>
         </mrow>
         <mrow>
          <mi mathvariant="normal">
           def
          </mi>
         </mrow>
        </mover>
        <mfenced close=")" open="(" separators="">
         <mrow>
          <msub>
           <mrow>
            <mi>
             C
            </mi>
           </mrow>
           <mrow>
            <mi>
             X
            </mi>
           </mrow>
          </msub>
          <mo>
           -
          </mo>
          <mi>
           α
          </mi>
          <msub>
           <mrow>
            <mi>
             C
            </mi>
           </mrow>
           <mrow>
            <mi>
             Y
            </mi>
           </mrow>
          </msub>
         </mrow>
        </mfenced>
       </math>
       <tex-math id="IEq5_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C\mathop { = }\limits^{{\mathrm{def}}{\kern 1pt} } \left( {C_X - \alpha C_Y} \right)$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_IEq5.gif"/>
      </alternatives>
     </inline-formula>
     . Hence the contrastive directions can be efficiently computed using eigenvalue decomposition. Analogous to PCA, we call the leading eigenvectors of
     <italic>
      C
     </italic>
     the contrastive principal components (cPCs). We note the cPCs are eigenvectors of the matrix
     <italic>
      C
     </italic>
     and are hence orthogonal to each other. For a fixed
     <italic>
      α
     </italic>
     , we compute (
     <xref ref-type="disp-formula" rid="Equ1">
      1
     </xref>
     ) and return the subspace spanned by the first few (typically two) cPCs.
    </p>
    <p id="Par22">
     The contrast parameter
     <italic>
      α
     </italic>
     represents the trade-off between having the high target variance and the low background variance. When
     <italic>
      α
     </italic>
     = 0, cPCA selects the directions that only maximize the target variance, and hence reduces to PCA applied on the target data {
     <bold>
      x
     </bold>
     <sub>
      <italic>
       i
      </italic>
     </sub>
     }. As
     <italic>
      α
     </italic>
     increases, directions with smaller background variance become more important and the cPCs are driven towards the null space of the background data {
     <bold>
      y
     </bold>
     <sub>
      <italic>
       i
      </italic>
     </sub>
     }. In the limiting case
     <italic>
      α
     </italic>
     = ∞, any direction not in the null space of {
     <bold>
      y
     </bold>
     <sub>
      <italic>
       i
      </italic>
     </sub>
     } receives an infinite penalty. In this case, cPCA corresponds to first projecting the target data onto the null space of the background data, and then performing PCA on the projected data.
    </p>
    <p id="Par23">
     Instead of choosing a single
     <italic>
      α
     </italic>
     and returning its subspace, cPCA computes the subspaces of a list of
     <italic>
      α
     </italic>
     ’s and returns a few subspaces that are far away from each other in terms of the principal angle
     <sup>
      <xref ref-type="bibr" rid="CR29">
       29
      </xref>
     </sup>
     . Projecting the data onto each of these subspaces will reveal different trends within the target data, and by visually examining the scatterplots that are returned, the user can quickly discern the relevant subspace (and corresponding value of
     <italic>
      α
     </italic>
     ) for his or her analysis. See Supplementary Fig.
     <xref ref-type="supplementary-material" rid="MOESM1">
      1
     </xref>
     for a detailed example.
    </p>
    <p id="Par24">
     The complete algorithm of cPCA is described in Algorithm 2 (Supplementary Methods). We typically set the list of potential values of
     <italic>
      α
     </italic>
     to be 40 values logarithmically spaced between 0.1 and 1000 and this is used for all experiments in the paper. To select the representative subspaces, cPCA uses spectral clustering
     <sup>
      <xref ref-type="bibr" rid="CR30">
       30
      </xref>
     </sup>
     to cluster the subspaces, where the affinity is defined as the product of the cosine of the principal angles between the subspaces. Then the medoids (representative) of each cluster are used as the values of
     <italic>
      α
     </italic>
     to generate the scatterplots seen by the user.
    </p>
   </sec>
   <sec id="Sec9">
    <title>
     Choosing the background dataset
    </title>
    <p id="Par25">
     The choice of the background dataset has a large influence on the result of cPCA. In general, the background data should have the structure that we would like to remove from the target data. Such structure usually corresponds to directions in the target with high variance, but that are not of interest to the analyst.
    </p>
    <p id="Par26">
     We provide a few general examples of background datasets that may provide useful contrasts to target data: (1) A control group {
     <bold>
      y
     </bold>
     <sub>
      <italic>
       i
      </italic>
     </sub>
     } contrasted with a diseased population {
     <bold>
      x
     </bold>
     <sub>
      <italic>
       i
      </italic>
     </sub>
     } because the control group contains similar population-level variation but not the subtle variation due to different subtypes of the disease. (2) The data at time zero {
     <bold>
      y
     </bold>
     <sub>
      <italic>
       i
      </italic>
     </sub>
     } used to contrast against data at a later time point {
     <bold>
      x
     </bold>
     <sub>
      <italic>
       i
      </italic>
     </sub>
     }. This enables visualizations of the most salient changes over time. (3) A homogeneous group {
     <bold>
      y
     </bold>
     <sub>
      <italic>
       i
      </italic>
     </sub>
     } contrasted with a mixed group {
     <bold>
      x
     </bold>
     <sub>
      <italic>
       i
      </italic>
     </sub>
     } because both have intra-population variation and measurement noise, but the former does not have inter-population variation. (4) A pre-treatment dataset {
     <bold>
      y
     </bold>
     <sub>
      <italic>
       i
      </italic>
     </sub>
     } contrasted with post-treatment data {
     <bold>
      x
     </bold>
     <sub>
      <italic>
       i
      </italic>
     </sub>
     } to remove measurement noise but preserve variations caused by treatment. (5) A set of signal-free recordings {
     <bold>
      y
     </bold>
     <sub>
      <italic>
       i
      </italic>
     </sub>
     } or images that contain only noise, contrasted with measurements {
     <bold>
      x
     </bold>
     <sub>
      <italic>
       i
      </italic>
     </sub>
     } that consist of both signal and noise.
    </p>
    <p id="Par27">
     It is worth adding that the background data does not need to have exactly the same covariance structure as what we would like to remove from the target dataset. As an example, in the experiment shown in Fig.
     <xref ref-type="fig" rid="Fig2">
      2
     </xref>
     , it turns out that we do not need to use a background dataset that consists of images of grass. In fact, similar results are obtained even if instead of images of grass, images of the sky are used as the background dataset. As the structure of the covariance matrices are similar enough, cPCA removes the background structure from the target data. In addition, cPCA does not require the target data and the background data to have a similar number of samples. Since the covariance matrices are computed independently, cPCA only requires that the empirical covariance matrices be good estimates of the underlying population covariance matrices, essentially the same requirement as PCA.
    </p>
   </sec>
   <sec id="Sec10">
    <title>
     Theoretical guarantees of cPCA
    </title>
    <p id="Par28">
     Here, we discuss the geometric interpretation of cPCA as well as its statistical properties. First, it is interesting to consider which directions are “better” for the purpose of contrastive analysis. For a direction
     <inline-formula id="IEq6">
      <alternatives>
       <math id="IEq6_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi mathvariant="bold">
         v
        </mi>
        <mo>
         ∈
        </mo>
        <msubsup>
         <mrow>
          <mi mathvariant="double-struck">
           R
          </mi>
         </mrow>
         <mrow>
          <mi mathvariant="normal">
           unit
          </mi>
         </mrow>
         <mrow>
          <mi>
           d
          </mi>
         </mrow>
        </msubsup>
       </math>
       <tex-math id="IEq6_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bf{v}} \in {\Bbb R}_{{\mathrm{unit}}}^d$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_IEq6.gif"/>
      </alternatives>
     </inline-formula>
     , its significance in cPCA is fully determined by its target–background variance pair (
     <italic>
      λ
     </italic>
     <sub>
      <italic>
       X
      </italic>
     </sub>
     (
     <bold>
      v
     </bold>
     ),
     <italic>
      λ
     </italic>
     <sub>
      <italic>
       Y
      </italic>
     </sub>
     (
     <bold>
      v
     </bold>
     )); it is desirable to have a higher target variance and a lower background variance. Based on this intuition, we can further define a partial order of contrastiveness for various directions: for two directions
     <bold>
      v
     </bold>
     <sub>
      1
     </sub>
     and
     <bold>
      v
     </bold>
     <sub>
      2
     </sub>
     , we might say
     <bold>
      v
     </bold>
     <sub>
      1
     </sub>
     is a better contrastive direction if it has a higher target variance and a lower background variance. In this case, the target–background variance pair of
     <bold>
      v
     </bold>
     <sub>
      1
     </sub>
     would lie on the lower-right side of that of
     <bold>
      v
     </bold>
     <sub>
      2
     </sub>
     in the plot of target–background variance pairs (
     <italic>
      λ
     </italic>
     <sub>
      <italic>
       X
      </italic>
     </sub>
     (
     <bold>
      v
     </bold>
     ),
     <italic>
      λ
     </italic>
     <sub>
      <italic>
       Y
      </italic>
     </sub>
     (
     <bold>
      v
     </bold>
     )), e.g., Fig.
     <xref ref-type="fig" rid="Fig5">
      5
     </xref>
     . Based on this partial order, the set of most contrastive directions can be defined in a similar fashion as the definition of the Pareto frontier
     <sup>
      <xref ref-type="bibr" rid="CR31">
       31
      </xref>
     </sup>
     . Let
     <inline-formula id="IEq7">
      <alternatives>
       <math id="IEq7_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi mathvariant="script">
         U
        </mi>
       </math>
       <tex-math id="IEq7_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\cal U}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_IEq7.gif"/>
      </alternatives>
     </inline-formula>
     be the set of target–background variance pairs for all directions, i.e.
     <inline-formula id="IEq8">
      <alternatives>
       <math id="IEq8_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi mathvariant="script">
         U
        </mi>
        <mover>
         <mrow>
          <mo>
           =
          </mo>
         </mrow>
         <mrow>
          <mi mathvariant="normal">
           def
          </mi>
         </mrow>
        </mover>
        <msub>
         <mrow>
          <mfenced close="}" open="{" separators="">
           <mrow>
            <mrow>
             <mo>
              (
             </mo>
             <mrow>
              <msub>
               <mrow>
                <mi>
                 λ
                </mi>
               </mrow>
               <mrow>
                <mi>
                 X
                </mi>
               </mrow>
              </msub>
              <mrow>
               <mo>
                (
               </mo>
               <mrow>
                <mi mathvariant="bold">
                 v
                </mi>
               </mrow>
               <mo>
                )
               </mo>
              </mrow>
              <mo>
               ,
              </mo>
              <msub>
               <mrow>
                <mi>
                 λ
                </mi>
               </mrow>
               <mrow>
                <mi>
                 Y
                </mi>
               </mrow>
              </msub>
              <mrow>
               <mo>
                (
               </mo>
               <mrow>
                <mi mathvariant="bold">
                 v
                </mi>
               </mrow>
               <mo>
                )
               </mo>
              </mrow>
             </mrow>
             <mo>
              )
             </mo>
            </mrow>
           </mrow>
          </mfenced>
         </mrow>
         <mrow>
          <mi mathvariant="bold">
           v
          </mi>
          <mo>
           ∈
          </mo>
          <msubsup>
           <mrow>
            <mi mathvariant="double-struck">
             R
            </mi>
           </mrow>
           <mrow>
            <mi mathvariant="normal">
             unit
            </mi>
           </mrow>
           <mrow>
            <mi>
             d
            </mi>
           </mrow>
          </msubsup>
         </mrow>
        </msub>
       </math>
       <tex-math id="IEq8_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\cal U}\mathop { = }\limits^{{\mathrm{def}}{\kern 1pt} } \left\{ {(\lambda _X({\bf{v}}),\lambda _Y({\bf{v}}))} \right\}_{{\bf{v}} \in {\Bbb R}_{{\mathrm{unit}}}^d}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_IEq8.gif"/>
      </alternatives>
     </inline-formula>
     . The set of most contrastive directions corresponds to the lower-right boundary of
     <inline-formula id="IEq9">
      <alternatives>
       <math id="IEq9_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi mathvariant="script">
         U
        </mi>
       </math>
       <tex-math id="IEq9_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\cal U}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_IEq9.gif"/>
      </alternatives>
     </inline-formula>
     in the plot of target–background variance pairs, as shown in Fig.
     <xref ref-type="fig" rid="Fig5">
      5
     </xref>
     . (For the particular case of simultaneously diagonalizable background and target matrices, see Supplementary Fig.
     <xref ref-type="supplementary-material" rid="MOESM1">
      7
     </xref>
     .)
     <fig id="Fig5" position="float">
      <label>
       Fig. 5
      </label>
      <caption xml:lang="en">
       <p>
        Geometric Interpretation of cPCA. The set of target–background variance pairs
        <inline-formula id="IEq10">
         <alternatives>
          <math id="IEq10_Math" xmlns="http://www.w3.org/1998/Math/MathML">
           <mi mathvariant="script">
            U
           </mi>
          </math>
          <tex-math id="IEq10_TeX">
           \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\cal U}$$\end{document}
          </tex-math>
          <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_IEq10.gif"/>
         </alternatives>
        </inline-formula>
        is plotted as the teal region for some randomly generated target and background data. The lower-right boundary, as colored in gold, corresponds to the set of most contrastive directions
        <inline-formula id="IEq11">
         <alternatives>
          <math id="IEq11_Math" xmlns="http://www.w3.org/1998/Math/MathML">
           <msub>
            <mrow>
             <mi mathvariant="script">
              S
             </mi>
            </mrow>
            <mrow>
             <mi>
              λ
             </mi>
            </mrow>
           </msub>
          </math>
          <tex-math id="IEq11_TeX">
           \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\cal S}_\lambda$$\end{document}
          </tex-math>
          <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_IEq11.gif"/>
         </alternatives>
        </inline-formula>
        . The blue triangles are the variance pairs for the cPCs selected with
        <italic>
         α
        </italic>
        values 0.92 and 0.29 respectively. We note that they correspond to the points of tangency of the gold curve and the tangent lines with slope
        <inline-formula id="IEq12">
         <alternatives>
          <math id="IEq12_Math" xmlns="http://www.w3.org/1998/Math/MathML">
           <mfrac>
            <mrow>
             <mn>
              1
             </mn>
            </mrow>
            <mrow>
             <mi>
              α
             </mi>
            </mrow>
           </mfrac>
          </math>
          <tex-math id="IEq12_TeX">
           \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{1}{\alpha }$$\end{document}
          </tex-math>
          <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_IEq12.gif"/>
         </alternatives>
        </inline-formula>
        = 1.08, 3.37, respectively
       </p>
      </caption>
      <graphic mime-subtype="JPEG" specific-use="web" xlink:href="/ProjectMundo/MediaObjects/10X1038_s41467-018-04608-8/41467_2018_4608_Fig5_HTML.jpg"/>
     </fig>
    </p>
    <p id="Par29">
     Regarding cPCA, we can prove (see Supplementary Note
     <xref ref-type="supplementary-material" rid="MOESM1">
      2
     </xref>
     ) that by varying
     <italic>
      α
     </italic>
     , the set of top cPC’s is identical to the set of most contrastive directions. Moreover, for the direction
     <bold>
      v
     </bold>
     selected by cPCA with the contrast parameter set to
     <italic>
      α
     </italic>
     , its variance pair (
     <italic>
      λ
     </italic>
     <sub>
      <italic>
       X
      </italic>
     </sub>
     (
     <bold>
      v
     </bold>
     ),
     <italic>
      λ
     </italic>
     <sub>
      <italic>
       Y
      </italic>
     </sub>
     (
     <bold>
      v
     </bold>
     )) corresponds to the point of tangency of the lower-right boundary of
     <inline-formula id="IEq13">
      <alternatives>
       <math id="IEq13_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi mathvariant="script">
         U
        </mi>
       </math>
       <tex-math id="IEq13_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\cal U}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_IEq13.gif"/>
      </alternatives>
     </inline-formula>
     with a slope-1/
     <italic>
      α
     </italic>
     line. As a result, by varying
     <italic>
      α
     </italic>
     from zero to infinity, cPCA selects directions with variance pairs traveling from the lower-left end to the upper-right end of the lower-right boundary of
     <inline-formula id="IEq14">
      <alternatives>
       <math id="IEq14_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi mathvariant="script">
         U
        </mi>
       </math>
       <tex-math id="IEq14_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\cal U}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_IEq14.gif"/>
      </alternatives>
     </inline-formula>
     .
    </p>
    <p id="Par30">
     We also remark that regarding the randomness of the data, the convergence rate of the sample cPC to the population cPC is
     <inline-formula id="IEq15">
      <alternatives>
       <math id="IEq15_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mrow>
          <mi>
           O
          </mi>
         </mrow>
         <mrow>
          <mi>
           p
          </mi>
         </mrow>
        </msub>
        <mfenced close=")" open="(" separators="">
         <mrow>
          <msqrt>
           <mfrac>
            <mrow>
             <mi>
              d
             </mi>
            </mrow>
            <mrow>
             <mi mathvariant="normal">
              min
             </mi>
             <mrow>
              <mo>
               (
              </mo>
              <mrow>
               <mi>
                n
               </mi>
               <mo>
                ,
               </mo>
               <mi>
                m
               </mi>
              </mrow>
              <mo>
               )
              </mo>
             </mrow>
            </mrow>
           </mfrac>
          </msqrt>
         </mrow>
        </mfenced>
       </math>
       <tex-math id="IEq15_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$O_p\left( {\sqrt {\frac{d}{{{\mathrm{min}}(n,m)}}} } \right)$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41467_2018_4608_Article_IEq15.gif"/>
      </alternatives>
     </inline-formula>
     under mild assumptions, where
     <italic>
      d
     </italic>
     is the dimension and
     <italic>
      n
     </italic>
     ,
     <italic>
      m
     </italic>
     are the sizes of the target and the background data. This rate is similar to the standard convergence rate of the sample eigenvector for a covariance matrix. See Supplementary Note
     <xref ref-type="supplementary-material" rid="MOESM1">
      2
     </xref>
     .
    </p>
   </sec>
   <sec id="Sec11">
    <title>
     Code availability
    </title>
    <p id="Par31">
     We have released a Python implementation of contrastive PCA on GitHub (
     <ext-link ext-link-type="uri" xlink:href="https://github.com/abidlabs/contrastive">
      https://github.com/abidlabs/contrastive
     </ext-link>
     ). The GitHub repository also includes Python notebooks and datasets that reproduce most of the figures in this paper and in the Supplementary Information.
    </p>
   </sec>
   <sec id="Sec12">
    <title>
     Data availability
    </title>
    <p id="Par32">
     Datasets that have been used to evaluate contrastive PCA in this paper are either available from us or from the authors of the original studies. Please see the GitHub repository listed in the previous section for the datasets that we have released.
    </p>
   </sec>
  </sec>
 </body>
 <back>
  <ack>
   <title>
    Acknowledgements
   </title>
   <p>
    We thank Alex Ioannidis for the assistance in carrying out the experiments on the relationship between acestral groups in Mexico. We thank Professor David Tse for providing helpful suggestions and financially supporting M.Z. and V.B. We thank our colleagues Amirata Ghorbani, Xinkun Nie, and Ruishan Liu for helpful comments in the development of this technique. A.A. and M.Z. are partially supported by Stanford Graduate Fellowship. J.Z. is supported by a Chan-Zuckerberg Investigator grant and by National Science Foundation grant CRII 1657155.
   </p>
  </ack>
  <sec sec-type="author-contribution">
   <title>
    Author contributions
   </title>
   <p>
    J.Z. proposed the original notion of contrastive PCA and supervised the research. A.A., M.Z., and V.B. designed the algorithm. A.A. implemented the algorithm and carried out the empirical experiments. M.Z. and V.B. proved the theoretical results. A.A. and M.Z. wrote the manuscript. All of the authors reviewed the manuscript.
   </p>
  </sec>
  <sec sec-type="ethics-statement">
   <sec id="FPar1" sec-type="COI-statement">
    <title>
     Competing interests
    </title>
    <p id="Par33">
     The authors declare no competing interests.
    </p>
   </sec>
  </sec>
  <ref-list id="Bib1">
   <title>
    References
   </title>
   <ref-list>
    <ref id="CR1">
     <label>
      1.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Hotelling
        </surname>
        <given-names>
         H
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Analysis of a complex of statistical variables into principal components
      </article-title>
      <source>
       J. Educ. Psychol.
      </source>
      <year>
       1933
      </year>
      <volume>
       24
      </volume>
      <fpage>
       417
      </fpage>
      <pub-id pub-id-type="doi">
       10.1037/h0071325
      </pub-id>
      <pub-id assigning-authority="Zentralblatt MATH" pub-id-type="other">
       59.1182.04
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR2">
     <label>
      2.
     </label>
     <mixed-citation publication-type="other">
      Jolliffe, I. T (ed.).
      <italic>
       Principal Component Analysis
      </italic>
      , 115–128 (Springer, New York, NY, 1986).
     </mixed-citation>
    </ref>
    <ref id="CR3">
     <label>
      3.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Maaten
        </surname>
        <given-names>
         L
        </given-names>
       </name>
       <name>
        <surname>
         Hinton
        </surname>
        <given-names>
         G
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Visualizing data using t-sne
      </article-title>
      <source>
       J. Mach. Learn. Res.
      </source>
      <year>
       2008
      </year>
      <volume>
       9
      </volume>
      <fpage>
       2579
      </fpage>
      <lpage>
       2605
      </lpage>
      <pub-id assigning-authority="Zentralblatt MATH" pub-id-type="other">
       1225.68219
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR4">
     <label>
      4.
     </label>
     <mixed-citation publication-type="other">
      Cox, M. A. &amp; Cox, T. F.
      <italic>
       Multidimensional Scaling. Handbook of Data Visualization
      </italic>
      315–347 (Springer, Berlin, 2008).
     </mixed-citation>
    </ref>
    <ref id="CR5">
     <label>
      5.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Chen
        </surname>
        <given-names>
         W
        </given-names>
       </name>
       <name>
        <surname>
         Ma
        </surname>
        <given-names>
         H
        </given-names>
       </name>
       <name>
        <surname>
         Yu
        </surname>
        <given-names>
         D
        </given-names>
       </name>
       <name>
        <surname>
         Zhang
        </surname>
        <given-names>
         H
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       SVD-based technique for interference cancellation and noise reduction in NMR measurement of time-dependent magnetic fields
      </article-title>
      <source>
       Sensors
      </source>
      <year>
       2016
      </year>
      <volume>
       16
      </volume>
      <fpage>
       323
      </fpage>
      <pub-id pub-id-type="doi">
       10.3390/s16030323
      </pub-id>
      <pub-id pub-id-type="pmcid">
       4813898
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR6">
     <label>
      6.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Zhou
        </surname>
        <given-names>
         F
        </given-names>
       </name>
       <name>
        <surname>
         Wu
        </surname>
        <given-names>
         R
        </given-names>
       </name>
       <name>
        <surname>
         Xing
        </surname>
        <given-names>
         M
        </given-names>
       </name>
       <name>
        <surname>
         Bao
        </surname>
        <given-names>
         Z
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Eigensubspace-based filtering with application in narrow-band interference suppression for sar
      </article-title>
      <source>
       IEEE Geosci. Remote Sens. Lett.
      </source>
      <year>
       2007
      </year>
      <volume>
       4
      </volume>
      <fpage>
       75
      </fpage>
      <lpage>
       79
      </lpage>
      <pub-id assigning-authority="NASA Astrophysics Data System" pub-id-type="other">
       2007IGRSL...4...75Z
      </pub-id>
      <pub-id pub-id-type="doi">
       10.1109/LGRS.2006.887033
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR7">
     <label>
      7.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Garte
        </surname>
        <given-names>
         S
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       The role of ethnicity in cancer susceptibility gene polymorphisms: the example of CYP1A1
      </article-title>
      <source>
       Carcinogenesis
      </source>
      <year>
       1998
      </year>
      <volume>
       19
      </volume>
      <fpage>
       1329
      </fpage>
      <lpage>
       1332
      </lpage>
      <pub-id pub-id-type="doi">
       10.1093/carcin/19.8.1329
      </pub-id>
      <pub-id pub-id-type="pmid">
       9744524
      </pub-id>
      <pub-id assigning-authority="ChemPort ( Chemical Abstract Service )" pub-id-type="other">
       1:CAS:528:DyaK1cXls1elt7g%3D
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR8">
     <label>
      8.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Wold
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <name>
        <surname>
         Esbensen
        </surname>
        <given-names>
         K
        </given-names>
       </name>
       <name>
        <surname>
         Geladi
        </surname>
        <given-names>
         P
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Principal component analysis
      </article-title>
      <source>
       Chemom. Intell. Lab. Syst.
      </source>
      <year>
       1987
      </year>
      <volume>
       2
      </volume>
      <fpage>
       37
      </fpage>
      <lpage>
       52
      </lpage>
      <pub-id pub-id-type="doi">
       10.1016/0169-7439(87)80084-9
      </pub-id>
      <pub-id assigning-authority="ChemPort ( Chemical Abstract Service )" pub-id-type="other">
       1:CAS:528:DyaL1cXjtVyjsw%3D%3D
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR9">
     <label>
      9.
     </label>
     <mixed-citation publication-type="other">
      Izenman, A. J.
      <italic>
       Modern Multivariate Statistical Techniques
      </italic>
      237–280 (Springer, New York, 2013).
     </mixed-citation>
    </ref>
    <ref id="CR10">
     <label>
      10.
     </label>
     <mixed-citation publication-type="other">
      Mika, S., Ratsch, G., Weston, J., Scholkopf, B. &amp; Mullers, K.-R. Fisher discriminant analysis with kernels. In
      <italic>
       Proc. of the 1999 IEEE Signal Processing Society Workshop Neural Networks for Signal Processing IX, 1999
      </italic>
      , 41–48 (IEEE, Beijing, 1999).
     </mixed-citation>
    </ref>
    <ref id="CR11">
     <label>
      11.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Barshan
        </surname>
        <given-names>
         E
        </given-names>
       </name>
       <name>
        <surname>
         Ghodsi
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <name>
        <surname>
         Azimifar
        </surname>
        <given-names>
         Z
        </given-names>
       </name>
       <name>
        <surname>
         Jahromi
        </surname>
        <given-names>
         MZ
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Supervised principal component analysis: visualization, classification and regression on subspaces and submanifolds
      </article-title>
      <source>
       Pattern Recognit.
      </source>
      <year>
       2011
      </year>
      <volume>
       44
      </volume>
      <fpage>
       1357
      </fpage>
      <lpage>
       1371
      </lpage>
      <pub-id pub-id-type="doi">
       10.1016/j.patcog.2010.12.015
      </pub-id>
      <pub-id assigning-authority="Zentralblatt MATH" pub-id-type="other">
       1214.62067
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR12">
     <label>
      12.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Fan
        </surname>
        <given-names>
         J
        </given-names>
       </name>
       <name>
        <surname>
         Ke
        </surname>
        <given-names>
         ZT
        </given-names>
       </name>
       <name>
        <surname>
         Liu
        </surname>
        <given-names>
         H
        </given-names>
       </name>
       <name>
        <surname>
         Xia
        </surname>
        <given-names>
         L
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Quadro: a supervised dimension reduction method via rayleigh quotient optimization
      </article-title>
      <source>
       Ann. Stat.
      </source>
      <year>
       2015
      </year>
      <volume>
       43
      </volume>
      <fpage>
       1498
      </fpage>
      <pub-id assigning-authority="American Mathematical Society" pub-id-type="other">
       3357869
      </pub-id>
      <pub-id pub-id-type="doi">
       10.1214/14-AOS1307
      </pub-id>
      <pub-id pub-id-type="pmid">
       26778864
      </pub-id>
      <pub-id pub-id-type="pmcid">
       4712455
      </pub-id>
      <pub-id assigning-authority="Zentralblatt MATH" pub-id-type="other">
       1317.62054
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR13">
     <label>
      13.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Meng
        </surname>
        <given-names>
         C
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Dimension reduction techniques for the integrative analysis of multi-omics data
      </article-title>
      <source>
       Brief. Bioinformatics
      </source>
      <year>
       2016
      </year>
      <volume>
       17
      </volume>
      <fpage>
       628
      </fpage>
      <lpage>
       641
      </lpage>
      <pub-id pub-id-type="doi">
       10.1093/bib/bbv108
      </pub-id>
      <pub-id pub-id-type="pmid">
       26969681
      </pub-id>
      <pub-id pub-id-type="pmcid">
       4945831
      </pub-id>
      <pub-id assigning-authority="ChemPort ( Chemical Abstract Service )" pub-id-type="other">
       1:CAS:528:DC%2BC1cXlsVSqs7o%3D
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR14">
     <label>
      14.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Rohart
        </surname>
        <given-names>
         F
        </given-names>
       </name>
       <name>
        <surname>
         Gautier
        </surname>
        <given-names>
         B
        </given-names>
       </name>
       <name>
        <surname>
         Singh
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <name>
        <surname>
         Le Cao
        </surname>
        <given-names>
         KA
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       mixomics: An r package for omics feature selection and multiple data integration
      </article-title>
      <source>
       PLoS Comput. Biol.
      </source>
      <year>
       2017
      </year>
      <volume>
       13
      </volume>
      <fpage>
       e1005752
      </fpage>
      <pub-id pub-id-type="doi">
       10.1371/journal.pcbi.1005752
      </pub-id>
      <pub-id pub-id-type="pmid">
       29099853
      </pub-id>
      <pub-id pub-id-type="pmcid">
       5687754
      </pub-id>
      <pub-id assigning-authority="ChemPort ( Chemical Abstract Service )" pub-id-type="other">
       1:CAS:528:DC%2BC1cXht1Wjs73K
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR15">
     <label>
      15.
     </label>
     <mixed-citation publication-type="other">
      Garali, I. et al. A strategy for multimodal data integration: application to biomarkers identification in spinocerebellar ataxia.
      <italic>
       Brief. Bioinform.
      </italic>
      <bold>
       bbx060
      </bold>
      , 1–14 (2017).
     </mixed-citation>
    </ref>
    <ref id="CR16">
     <label>
      16.
     </label>
     <mixed-citation publication-type="other">
      Stein-O’Brien, G. L. et al. Enter the matrix: Interpreting unsupervised feature learning with matrix decomposition to discover hidden knowledge in high-throughput omics data. Preprint at
      <italic>
       bioRxiv
      </italic>
      <ext-link ext-link-type="doi" xlink:href="10.1101/196915">
       https://doi.org/10.1101/196915
      </ext-link>
      (2017).
     </mixed-citation>
    </ref>
    <ref id="CR17">
     <label>
      17.
     </label>
     <mixed-citation publication-type="other">
      Zhou, Z., Li, X., Wright, J., Candes, E. &amp; Ma, Y. Stable principal component pursuit. In
      <italic>
       IEEE International Symposium on Information Theory Proceedings (ISIT), 2010
      </italic>
      1518–1522 (IEEE, Austin, TX, 2010).
     </mixed-citation>
    </ref>
    <ref id="CR18">
     <label>
      18.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Moreno-Estrada
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       The genetics of Mexico recapitulates native american substructure and affects biomedical traits
      </article-title>
      <source>
       Science
      </source>
      <year>
       2014
      </year>
      <volume>
       344
      </volume>
      <fpage>
       1280
      </fpage>
      <lpage>
       1285
      </lpage>
      <pub-id assigning-authority="NASA Astrophysics Data System" pub-id-type="other">
       2014Sci...344.1280M
      </pub-id>
      <pub-id pub-id-type="doi">
       10.1126/science.1251688
      </pub-id>
      <pub-id pub-id-type="pmid">
       24926019
      </pub-id>
      <pub-id pub-id-type="pmcid">
       4156478
      </pub-id>
      <pub-id assigning-authority="ChemPort ( Chemical Abstract Service )" pub-id-type="other">
       1:CAS:528:DC%2BC2cXpsVGnsbc%3D
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR19">
     <label>
      19.
     </label>
     <mixed-citation publication-type="other">
      Zou, J. Y., Hsu, D. J., Parkes, D. C. &amp; Adams, R. P. Contrastive learning using spectral methods. In
      <italic>
       Advances in Neural Information Processing Systems
      </italic>
      2238–2246 (NIPS, Lake Tahoe, 2013).
     </mixed-citation>
    </ref>
    <ref id="CR20">
     <label>
      20.
     </label>
     <mixed-citation publication-type="other">
      Ge, R. &amp; Zou, J. Rich component analysis. In
      <italic>
       Proc. International Conference on Machine Learning
      </italic>
      1502–1510 (ICML, New York, 2016).
     </mixed-citation>
    </ref>
    <ref id="CR21">
     <label>
      21.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Ringner
        </surname>
        <given-names>
         M
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       What is principal component analysis?
      </article-title>
      <source>
       Nat. Biotechnol.
      </source>
      <year>
       2008
      </year>
      <volume>
       26
      </volume>
      <fpage>
       303
      </fpage>
      <pub-id pub-id-type="doi">
       10.1038/nbt0308-303
      </pub-id>
      <pub-id pub-id-type="pmid">
       18327243
      </pub-id>
      <pub-id assigning-authority="ChemPort ( Chemical Abstract Service )" pub-id-type="other">
       1:CAS:528:DC%2BD1cXjsVGitrg%3D
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR22">
     <label>
      22.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Ahmed
        </surname>
        <given-names>
         MM
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Protein dynamics associated with failed and rescued learning in the ts65dn mouse model of down syndrome
      </article-title>
      <source>
       PLoS ONE
      </source>
      <year>
       2015
      </year>
      <volume>
       10
      </volume>
      <fpage>
       e0119491
      </fpage>
      <pub-id pub-id-type="doi">
       10.1371/journal.pone.0119491
      </pub-id>
      <pub-id pub-id-type="pmid">
       25793384
      </pub-id>
      <pub-id pub-id-type="pmcid">
       4368539
      </pub-id>
      <pub-id assigning-authority="ChemPort ( Chemical Abstract Service )" pub-id-type="other">
       1:CAS:528:DC%2BC2MXhslaqt77P
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR23">
     <label>
      23.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Higuera
        </surname>
        <given-names>
         C
        </given-names>
       </name>
       <name>
        <surname>
         Gardiner
        </surname>
        <given-names>
         KJ
        </given-names>
       </name>
       <name>
        <surname>
         Cios
        </surname>
        <given-names>
         KJ
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Self-organizing feature maps identify proteins critical to learning in a mouse model of down syndrome
      </article-title>
      <source>
       PLoS ONE
      </source>
      <year>
       2015
      </year>
      <volume>
       10
      </volume>
      <fpage>
       e0129126
      </fpage>
      <pub-id pub-id-type="doi">
       10.1371/journal.pone.0129126
      </pub-id>
      <pub-id pub-id-type="pmid">
       26111164
      </pub-id>
      <pub-id pub-id-type="pmcid">
       4482027
      </pub-id>
      <pub-id assigning-authority="ChemPort ( Chemical Abstract Service )" pub-id-type="other">
       1:CAS:528:DC%2BC28XosVCrurg%3D
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR24">
     <label>
      24.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Zheng
        </surname>
        <given-names>
         GXY
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Massively parallel digital transcriptional profiling of single cells
      </article-title>
      <source>
       Nat. Commun.
      </source>
      <year>
       2017
      </year>
      <volume>
       8
      </volume>
      <pub-id assigning-authority="NASA Astrophysics Data System" pub-id-type="other">
       2017NatCo...814049Z
      </pub-id>
      <pub-id pub-id-type="doi">
       10.1038/ncomms14049
      </pub-id>
      <pub-id pub-id-type="pmid">
       28091601
      </pub-id>
      <pub-id pub-id-type="pmcid">
       5241818
      </pub-id>
      <pub-id assigning-authority="ChemPort ( Chemical Abstract Service )" pub-id-type="other">
       1:CAS:528:DC%2BC2sXht1WlsLo%3D
      </pub-id>
      <elocation-id>
       14049
      </elocation-id>
     </mixed-citation>
    </ref>
    <ref id="CR25">
     <label>
      25.
     </label>
     <mixed-citation publication-type="other">
      Bhargava, V., Head, S. R., Ordoukhanian, P., Mercola, M. &amp; Subramaniam, S. Technical variations in low-input RNA-seq methodologies.
      <italic>
       Sci. Rep
      </italic>
      .
      <bold>
       4
      </bold>
      , 3678 (2014).
     </mixed-citation>
    </ref>
    <ref id="CR26">
     <label>
      26.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Cavalli-Sforza
        </surname>
        <given-names>
         LL
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       The DNA revolution in population genetics
      </article-title>
      <source>
       Trends Genet.
      </source>
      <year>
       1998
      </year>
      <volume>
       14
      </volume>
      <fpage>
       60
      </fpage>
      <lpage>
       65
      </lpage>
      <pub-id pub-id-type="doi">
       10.1016/S0168-9525(97)01327-9
      </pub-id>
      <pub-id pub-id-type="pmid">
       9520599
      </pub-id>
      <pub-id assigning-authority="ChemPort ( Chemical Abstract Service )" pub-id-type="other">
       1:CAS:528:DyaK1cXhsFKhsbg%3D
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR27">
     <label>
      27.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Novembre
        </surname>
        <given-names>
         J
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Genes mirror geography within Europe
      </article-title>
      <source>
       Nature
      </source>
      <year>
       2008
      </year>
      <volume>
       456
      </volume>
      <fpage>
       98
      </fpage>
      <lpage>
       101
      </lpage>
      <pub-id assigning-authority="NASA Astrophysics Data System" pub-id-type="other">
       2008Natur.456...98N
      </pub-id>
      <pub-id pub-id-type="doi">
       10.1038/nature07331
      </pub-id>
      <pub-id pub-id-type="pmid">
       18758442
      </pub-id>
      <pub-id pub-id-type="pmcid">
       2735096
      </pub-id>
      <pub-id assigning-authority="ChemPort ( Chemical Abstract Service )" pub-id-type="other">
       1:CAS:528:DC%2BD1cXhtlCjtrjM
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR28">
     <label>
      28.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Silva-Zolezzi
        </surname>
        <given-names>
         I
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Analysis of genomic diversity in Mexican mestizo populations to develop genomic medicine in Mexico
      </article-title>
      <source>
       Proc. Natl. Acad. Sci. USA
      </source>
      <year>
       2009
      </year>
      <volume>
       106
      </volume>
      <fpage>
       8611
      </fpage>
      <lpage>
       8616
      </lpage>
      <pub-id assigning-authority="NASA Astrophysics Data System" pub-id-type="other">
       2009PNAS..106.8611S
      </pub-id>
      <pub-id pub-id-type="doi">
       10.1073/pnas.0903045106
      </pub-id>
      <pub-id pub-id-type="pmid">
       19433783
      </pub-id>
      <pub-id pub-id-type="pmcid">
       2680428
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR29">
     <label>
      29.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Miao
        </surname>
        <given-names>
         J
        </given-names>
       </name>
       <name>
        <surname>
         Ben-Israel
        </surname>
        <given-names>
         A
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       On principal angles between subspaces in Rn
      </article-title>
      <source>
       Linear Algebra Appl.
      </source>
      <year>
       1992
      </year>
      <volume>
       171
      </volume>
      <fpage>
       81
      </fpage>
      <lpage>
       98
      </lpage>
      <pub-id assigning-authority="American Mathematical Society" pub-id-type="other">
       1165446
      </pub-id>
      <pub-id pub-id-type="doi">
       10.1016/0024-3795(92)90251-5
      </pub-id>
      <pub-id assigning-authority="Zentralblatt MATH" pub-id-type="other">
       0779.15003
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR30">
     <label>
      30.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Ng
        </surname>
        <given-names>
         AY
        </given-names>
       </name>
       <name>
        <surname>
         Jordan
        </surname>
        <given-names>
         MI
        </given-names>
       </name>
       <name>
        <surname>
         Weiss
        </surname>
        <given-names>
         Y
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       On spectral clustering: analysis and an algorithm
      </article-title>
      <source>
       Adv. Neural. Inf. Process. Syst.
      </source>
      <year>
       2002
      </year>
      <volume>
       14
      </volume>
      <fpage>
       849
      </fpage>
      <lpage>
       856
      </lpage>
     </mixed-citation>
    </ref>
    <ref id="CR31">
     <label>
      31.
     </label>
     <mixed-citation publication-type="other">
      Fudenberg, D. D. &amp; Tirole, J.
      <italic>
       Game Theory
      </italic>
      (MIT Press, Cambridge, MA, 1991).
     </mixed-citation>
    </ref>
    <ref id="CR32">
     <label>
      32.
     </label>
     <mixed-citation publication-type="other">
      LeCun, Y., Cortes, C. &amp; Burges, C. J. Mnist handwritten digit database.
      <italic>
       AT&amp;T Labs
      </italic>
      .
      <bold>
       2
      </bold>
      ,
      <ext-link ext-link-type="uri" xlink:href="http://yann.lecun.com/exdb/mnist">
       http://yann.lecun.com/exdb/mnist
      </ext-link>
      (2010).
     </mixed-citation>
    </ref>
    <ref id="CR33">
     <label>
      33.
     </label>
     <mixed-citation publication-type="other">
      Deng, J. et al. Imagenet: a large-scale hierarchical image database. In
      <italic>
       IEEE Conference on
      </italic>
      <italic>
       Computer Vision and Pattern Recognition, 2009. CVPR 2009
      </italic>
      , 248–255 (IEEE, Washington, DC, 2009).
     </mixed-citation>
    </ref>
   </ref-list>
  </ref-list>
  <app-group>
   <app id="App1" specific-use="web-only">
    <sec id="Sec14">
     <title>
      Electronic supplementary material
     </title>
     <p id="Par34">
      <supplementary-material content-type="local-data" id="MOESM1" xlink:title="Electronic supplementary material">
       <media mime-subtype="pdf" mimetype="application" xlink:href="MediaObjects/41467_2018_4608_MOESM1_ESM.pdf">
        <caption xml:lang="en">
         <p>
          Supplementary Information
         </p>
        </caption>
       </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM2" xlink:title="Electronic supplementary material">
       <media mime-subtype="pdf" mimetype="application" xlink:href="MediaObjects/41467_2018_4608_MOESM2_ESM.pdf">
        <caption xml:lang="en">
         <p>
          Peer Review File
         </p>
        </caption>
       </media>
      </supplementary-material>
     </p>
    </sec>
   </app>
  </app-group>
  <notes notes-type="ESMHint">
   <title>
    Electronic supplementary material
   </title>
   <p>
    <bold>
     Supplementary Information
    </bold>
    accompanies this paper at
    <ext-link ext-link-type="doi" xlink:href="10.1038/s41467-018-04608-8">
     https://doi.org/10.1038/s41467-018-04608-8
    </ext-link>
    .
   </p>
  </notes>
  <notes notes-type="Misc">
   <p>
    <bold>
     Publisher's note:
    </bold>
    Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
   </p>
  </notes>
 </back>
</article>
