<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type='text/xsl' href='/ProjectMundo-Anon/style/jats-html.xsl'?>
<!DOCTYPE response>
<article article-type="research-article" dtd-version="1.2" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
 <front>
  <journal-meta>
   <journal-id journal-id-type="publisher-id">
    41746
   </journal-id>
   <journal-id journal-id-type="doi">
    10.1038/41746.2398-6352
   </journal-id>
   <journal-title-group>
    <journal-title>
     npj Digital Medicine
    </journal-title>
    <abbrev-journal-title abbrev-type="publisher">
     npj Digit. Med.
    </abbrev-journal-title>
   </journal-title-group>
   <issn pub-type="epub">
    2398-6352
   </issn>
   <publisher>
    <publisher-name>
     Nature Publishing Group UK
    </publisher-name>
    <publisher-loc>
     London
    </publisher-loc>
   </publisher>
  </journal-meta>
  <article-meta>
   <article-id pub-id-type="publisher-id">
    s41746-019-0216-8
   </article-id>
   <article-id pub-id-type="manuscript">
    216
   </article-id>
   <article-id pub-id-type="doi">
    10.1038/s41746-019-0216-8
   </article-id>
   <article-categories>
    <subj-group subj-group-type="heading">
     <subject>
      Article
     </subject>
    </subj-group>
    <subj-group subj-group-type="SubjectPath">
     <subject>
      /692/699/75
     </subject>
    </subj-group>
    <subj-group subj-group-type="SubjectPath">
     <subject>
      /631/114/1305
     </subject>
    </subj-group>
    <subj-group subj-group-type="SubjectPath">
     <subject>
      /631/114/1564
     </subject>
    </subj-group>
    <subj-group subj-group-type="NatureArticleTypeID">
     <subject>
      article
     </subject>
    </subj-group>
   </article-categories>
   <title-group>
    <article-title xml:lang="en">
     تفسیر یادگیری عمیق از اکوکاردیوگرام‌ها
    </article-title>
   </title-group>
   <contrib-group>
    <contrib contrib-type="author" equal-contrib="yes" id="Au1">
     <name name-style="western">
      <surname>
       Ghorbani
      </surname>
      <given-names>
       Amirata
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff1">
      1
     </xref>
     <xref ref-type="author-notes" rid="fn1"/>
    </contrib>
    <contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="Au2">
     <contrib-id contrib-id-type="orcid">
      http://orcid.org/0000-0002-3813-7518
     </contrib-id>
     <name name-style="western">
      <surname>
       Ouyang
      </surname>
      <given-names>
       David
      </given-names>
     </name>
     <address>
      <email>
       ouyangd@stanford.edu
      </email>
     </address>
     <xref ref-type="aff" rid="Aff2">
      2
     </xref>
     <xref ref-type="corresp" rid="IDs4174601902168_cor2">
      b
     </xref>
     <xref ref-type="author-notes" rid="fn1"/>
    </contrib>
    <contrib contrib-type="author" id="Au3">
     <name name-style="western">
      <surname>
       Abid
      </surname>
      <given-names>
       Abubakar
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff1">
      1
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au4">
     <name name-style="western">
      <surname>
       He
      </surname>
      <given-names>
       Bryan
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff3">
      3
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au5">
     <name name-style="western">
      <surname>
       Chen
      </surname>
      <given-names>
       Jonathan H.
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff2">
      2
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au6">
     <name name-style="western">
      <surname>
       Harrington
      </surname>
      <given-names>
       Robert A.
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff2">
      2
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au7">
     <name name-style="western">
      <surname>
       Liang
      </surname>
      <given-names>
       David H.
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff2">
      2
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au8">
     <contrib-id contrib-id-type="orcid">
      http://orcid.org/0000-0001-9418-9577
     </contrib-id>
     <name name-style="western">
      <surname>
       Ashley
      </surname>
      <given-names>
       Euan A.
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff2">
      2
     </xref>
    </contrib>
    <contrib contrib-type="author" corresp="yes" id="Au9">
     <contrib-id contrib-id-type="orcid">
      http://orcid.org/0000-0001-8880-4764
     </contrib-id>
     <name name-style="western">
      <surname>
       Zou
      </surname>
      <given-names>
       James Y.
      </given-names>
     </name>
     <address>
      <email>
       jamesz@stanford.edu
      </email>
     </address>
     <xref ref-type="aff" rid="Aff1">
      1
     </xref>
     <xref ref-type="aff" rid="Aff3">
      3
     </xref>
     <xref ref-type="aff" rid="Aff4">
      4
     </xref>
     <xref ref-type="aff" rid="Aff5">
      5
     </xref>
     <xref ref-type="corresp" rid="IDs4174601902168_cor9">
      j
     </xref>
    </contrib>
    <aff id="Aff1">
     <label>
      1
     </label>
     <institution-wrap>
      <institution-id institution-id-type="GRID">
       grid.168010.e
      </institution-id>
      <institution-id institution-id-type="ISNI">
       0000000419368956
      </institution-id>
      <institution content-type="org-division">
       Department of Electrical Engineering
      </institution>
      <institution content-type="org-name">
       Stanford University
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      Stanford
     </addr-line>
     <addr-line content-type="state">
      CA
     </addr-line>
     <country country="US">
      USA
     </country>
    </aff>
    <aff id="Aff2">
     <label>
      2
     </label>
     <institution-wrap>
      <institution-id institution-id-type="GRID">
       grid.168010.e
      </institution-id>
      <institution-id institution-id-type="ISNI">
       0000000419368956
      </institution-id>
      <institution content-type="org-division">
       Department of Medicine
      </institution>
      <institution content-type="org-name">
       Stanford University
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      Stanford
     </addr-line>
     <addr-line content-type="state">
      CA
     </addr-line>
     <country country="US">
      USA
     </country>
    </aff>
    <aff id="Aff3">
     <label>
      3
     </label>
     <institution-wrap>
      <institution-id institution-id-type="GRID">
       grid.168010.e
      </institution-id>
      <institution-id institution-id-type="ISNI">
       0000000419368956
      </institution-id>
      <institution content-type="org-division">
       Department of Computer Science
      </institution>
      <institution content-type="org-name">
       Stanford University
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      Stanford
     </addr-line>
     <addr-line content-type="state">
      CA
     </addr-line>
     <country country="US">
      USA
     </country>
    </aff>
    <aff id="Aff4">
     <label>
      4
     </label>
     <institution-wrap>
      <institution-id institution-id-type="GRID">
       grid.168010.e
      </institution-id>
      <institution-id institution-id-type="ISNI">
       0000000419368956
      </institution-id>
      <institution content-type="org-division">
       Department of Biomedical Data Science
      </institution>
      <institution content-type="org-name">
       Stanford University
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      Stanford
     </addr-line>
     <addr-line content-type="state">
      CA
     </addr-line>
     <country country="US">
      USA
     </country>
    </aff>
    <aff id="Aff5">
     <label>
      5
     </label>
     <institution-wrap>
      <institution-id institution-id-type="GRID">
       grid.499295.a
      </institution-id>
      <institution content-type="org-name">
       Chan-Zuckerberg Biohub
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      San Francisco
     </addr-line>
     <addr-line content-type="state">
      CA
     </addr-line>
     <country country="US">
      USA
     </country>
    </aff>
   </contrib-group>
   <author-notes>
    <fn fn-type="equal" id="fn1">
     <p>
      These authors contributed equally: Amirata Ghorbani, David Ouyang
     </p>
    </fn>
    <corresp id="IDs4174601902168_cor2">
     <label>
      b
     </label>
     <email>
      ouyangd@stanford.edu
     </email>
    </corresp>
    <corresp id="IDs4174601902168_cor9">
     <label>
      j
     </label>
     <email>
      jamesz@stanford.edu
     </email>
    </corresp>
   </author-notes>
   <pub-date date-type="pub" publication-format="electronic">
    <day>
     24
    </day>
    <month>
     1
    </month>
    <year>
     2020
    </year>
   </pub-date>
   <pub-date date-type="collection" publication-format="electronic">
    <month>
     12
    </month>
    <year>
     2020
    </year>
   </pub-date>
   <volume>
    3
   </volume>
   <issue seq="10">
    1
   </issue>
   <elocation-id>
    10
   </elocation-id>
   <history>
    <date date-type="registration">
     <day>
      21
     </day>
     <month>
      12
     </month>
     <year>
      2019
     </year>
    </date>
    <date date-type="received">
     <day>
      25
     </day>
     <month>
      6
     </month>
     <year>
      2019
     </year>
    </date>
    <date date-type="accepted">
     <day>
      19
     </day>
     <month>
      12
     </month>
     <year>
      2019
     </year>
    </date>
    <date date-type="online">
     <day>
      24
     </day>
     <month>
      1
     </month>
     <year>
      2020
     </year>
    </date>
   </history>
   <permissions>
    <copyright-statement content-type="compact">
     © The Author(s) 2020
    </copyright-statement>
    <copyright-year>
     2020
    </copyright-year>
    <copyright-holder>
     The Author(s)
    </copyright-holder>
    <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/">
     <license-p>
      <bold>
       Open Access
      </bold>
      This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit
      <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">
       http://creativecommons.org/licenses/by/4.0/
      </ext-link>
      .
     </license-p>
    </license>
   </permissions>
   <abstract id="Abs1" xml:lang="en">
    <title>
     چکیده
    </title>
    <p id="Par1">
     اکوکاردیوگرافی از فناوری اولتراسوند برای ثبت تصاویر با وضوح زمانی و مکانی بالا از قلب و ساختارهای اطراف آن استفاده می‌کند و رایج‌ترین روش تصویربرداری در پزشکی قلب و عروق است. با استفاده از شبکه‌های عصبی پیچشی بر روی یک مجموعه داده بزرگ جدید، نشان می‌دهیم که یادگیری عمیق اعمال شده بر اکوکاردیوگرافی می‌تواند ساختارهای محلی قلب را شناسایی کند، عملکرد قلب را تخمین بزند و فنوتیپ‌های سیستمیک را که خطر قلبی عروقی را تغییر می‌دهند اما به راحتی برای تفسیر انسانی قابل شناسایی نیستند، پیش‌بینی کند. مدل یادگیری عمیق ما، EchoNet، به‌طور دقیق حضور لیدهای پیس‌میکر (AUC = 0.89)، بزرگ شدن دهلیز چپ (AUC = 0.86)، هیپرتروفی بطن چپ (AUC = 0.75)، حجم‌های انتهای سیستولیک و دیاستولیک بطن چپ (
     <inline-formula id="IEq1">
      <alternatives>
       <math id="IEq1_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           R
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
       </math>
       <tex-math id="IEq1_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}^{2}$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq1.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     = 0.74 و
     <inline-formula id="IEq2">
      <alternatives>
       <math id="IEq2_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           R
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
       </math>
       <tex-math id="IEq2_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}^{2}$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq2.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     = 0.70)، و کسر جهشی (
     <inline-formula id="IEq3">
      <alternatives>
       <math id="IEq3_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           R
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
       </math>
       <tex-math id="IEq3_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}^{2}$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq3.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     = 0.50)، و همچنین فنوتیپ‌های سیستمیک پیش‌بینی شده از سن (
     <inline-formula id="IEq4">
      <alternatives>
       <math id="IEq4_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           R
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
       </math>
       <tex-math id="IEq4_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}^{2}$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq4.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     = 0.46)، جنسیت (AUC = 0.88)، وزن (
     <inline-formula id="IEq5">
      <alternatives>
       <math id="IEq5_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           R
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
       </math>
       <tex-math id="IEq5_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}^{2}$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq5.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     = 0.56)، و قد (
     <inline-formula id="IEq6">
      <alternatives>
       <math id="IEq6_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           R
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
       </math>
       <tex-math id="IEq6_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}^{2}$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq6.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     = 0.33). تحلیل تفسیر تأیید می‌کند که EchoNet توجه مناسبی به ساختارهای کلیدی قلبی در هنگام انجام وظایف قابل توضیح انسانی نشان می‌دهد و مناطق مورد علاقه فرضیه‌ساز را در هنگام پیش‌بینی فنوتیپ‌های سیستمیک دشوار برای تفسیر انسانی برجسته می‌کند. یادگیری ماشینی بر روی تصاویر اکوکاردیوگرافی می‌تواند وظایف تکراری در جریان کار بالینی را ساده کند، تفسیر اولیه در مناطقی با کمبود متخصصان قلب واجد شرایط ارائه دهد و فنوتیپ‌هایی را که برای ارزیابی انسانی چالش‌برانگیز هستند، پیش‌بینی کند.
    </p>
   </abstract>
   <kwd-group kwd-group-type="hierarchical" vocab="FoR" vocab-identifier="ANZSRC 2008">
    <kwd content-type="term" vocab-term-identifier="11">
     Medical and Health Sciences
    </kwd>
    <nested-kwd>
     <kwd content-type="term" vocab-term-identifier="1102">
      Cardiorespiratory Medicine and Haematology
     </kwd>
    </nested-kwd>
   </kwd-group>
   <funding-group>
    <award-group>
     <funding-source>
      <institution-wrap>
       <institution>
        American College of Cardiology/ Merck Research Fellowship
       </institution>
      </institution-wrap>
     </funding-source>
    </award-group>
   </funding-group>
   <custom-meta-group>
    <custom-meta>
     <meta-name>
      publisher-imprint-name
     </meta-name>
     <meta-value>
      Nature Research
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      volume-issue-count
     </meta-name>
     <meta-value>
      1
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-article-count
     </meta-name>
     <meta-value>
      161
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-toc-levels
     </meta-name>
     <meta-value>
      0
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-pricelist-year
     </meta-name>
     <meta-value>
      2020
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-copyright-holder
     </meta-name>
     <meta-value>
      The Author(s)
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-copyright-year
     </meta-name>
     <meta-value>
      2020
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-contains-esm
     </meta-name>
     <meta-value>
      Yes
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-numbering-style
     </meta-name>
     <meta-value>
      Unnumbered
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-registration-date-year
     </meta-name>
     <meta-value>
      2019
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-registration-date-month
     </meta-name>
     <meta-value>
      12
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-registration-date-day
     </meta-name>
     <meta-value>
      21
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-toc-levels
     </meta-name>
     <meta-value>
      0
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      toc-levels
     </meta-name>
     <meta-value>
      0
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      volume-type
     </meta-name>
     <meta-value>
      Regular
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-product
     </meta-name>
     <meta-value>
      NonStandardArchiveJournal
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      numbering-style
     </meta-name>
     <meta-value>
      Unnumbered
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-grants-type
     </meta-name>
     <meta-value>
      OpenChoice
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      metadata-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      abstract-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      bodypdf-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      bodyhtml-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      bibliography-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      esm-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      online-first
     </meta-name>
     <meta-value>
      false
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      pdf-file-reference
     </meta-name>
     <meta-value>
      BodyRef/PDF/41746_2019_Article_216.pdf
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      pdf-type
     </meta-name>
     <meta-value>
      Typeset
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      target-type
     </meta-name>
     <meta-value>
      OnlinePDF
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-type
     </meta-name>
     <meta-value>
      Regular
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-type
     </meta-name>
     <meta-value>
      OriginalPaper
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-primary
     </meta-name>
     <meta-value>
      Medicine &amp; Public Health
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-secondary
     </meta-name>
     <meta-value>
      Medicine/Public Health, general
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-secondary
     </meta-name>
     <meta-value>
      Biomedicine, general
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-secondary
     </meta-name>
     <meta-value>
      Biotechnology
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-collection
     </meta-name>
     <meta-value>
      Medicine
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      open-access
     </meta-name>
     <meta-value>
      true
     </meta-value>
    </custom-meta>
   </custom-meta-group>
  </article-meta>
 </front>
 <body>
  <sec id="Sec1" sec-type="introduction">
   <title>
    مقدمه
   </title>
   <p id="Par2">
    بیماری‌های قلبی‌عروقی تأثیر قابل‌توجهی بر سلامت کلی، رفاه و امید به زندگی دارند. علاوه بر اینکه علت اصلی مرگ و میر در مردان و زنان است، بیماری‌های قلبی‌عروقی مسئول ۱۷٪ از هزینه‌های ملی بهداشت در ایالات متحده هستند. حتی با وجود اینکه بار بیماری‌های قلبی‌عروقی با پیر شدن جمعیت انتظار می‌رود افزایش یابد، همچنان تفاوت‌های نژادی، اجتماعی-اقتصادی و جغرافیایی قابل‌توجهی در دسترسی به مراقبت و نتایج بیماری وجود دارد. تنوع در دسترسی و کیفیت تصویربرداری قلبی‌عروقی با تفاوت‌های نتایج مرتبط بوده است. فرض بر این است که تفسیر خودکار تصاویر می‌تواند مراقبت‌های قلبی‌عروقی را در دسترس‌تر و دقیق‌تر کند و برخی از تفاوت‌ها در مراقبت‌های قلبی‌عروقی را کاهش دهد. کاربرد یادگیری ماشین در قلب‌شناسی هنوز در مراحل ابتدایی است، اما علاقه زیادی به استفاده از روش‌های مبتنی بر شبکه‌های عصبی در تصویربرداری قلبی‌عروقی وجود دارد
    <sup>
     <xref ref-type="bibr" rid="CR1">
      1
     </xref>
     <xref ref-type="bibr" rid="CR1">
      1
     </xref>
     <xref ref-type="bibr" rid="CR2">
      2
     </xref>
     ,
     <xref ref-type="bibr" rid="CR3">
      3
     </xref>
     <xref ref-type="bibr" rid="CR3">
      3
     </xref>
     ,
     <xref ref-type="bibr" rid="CR4">
      4
     </xref>
     <xref ref-type="bibr" rid="CR5">
      5
     </xref>
     ,
     <xref ref-type="bibr" rid="CR6">
      6
     </xref>
    </sup>
    .
   </p>
   <p id="Par3">
    یادگیری ماشین بسیاری از حوزه‌ها را متحول کرده است، از پردازش تصویر و سیستم‌های تشخیص صدا تا عملکرد فوق‌العاده انسانی در بازی‌های استراتژی پیچیده. بسیاری از پیشرفت‌های بزرگ اخیر در یادگیری ماشین از الگوریتم‌های بینایی کامپیوتری و پردازش داده‌های تصویری با یادگیری عمیق ناشی می‌شود. پیشرفت‌های اخیر در یادگیری ماشین نشان می‌دهد که یادگیری عمیق می‌تواند ویژگی‌های قابل‌شناسایی انسانی و همچنین فنوتیپ‌هایی که توسط کارشناسان انسانی شناسایی نشده‌اند را تشخیص دهد. تلاش‌ها برای اعمال یادگیری ماشین در سایر روش‌های تصویربرداری پزشکی نشان داده‌اند که در تشخیص به کمک کامپیوتر امیدبخش هستند. تصویربرداری به ظاهر نامرتبط از سیستم‌های اندام فردی، مانند تصاویر شبکیه چشم، می‌تواند فنوتیپ‌های سیستمیک و عوامل خطر قلبی‌عروقی را پیش‌بینی کند. علاوه بر این، الگوریتم‌های یادگیری عمیق در طبقه‌بندی و طبقه‌بندی خطر بیماری عملکرد خوبی دارند. چندین مثال پزشکی اخیر خارج از قلب‌شناسی نشان می‌دهد که الگوریتم‌های شبکه عصبی پیچشی (CNN) می‌توانند با کارشناسان انسانی در شناسایی و طبقه‌بندی بیماری‌ها برابری یا حتی برتری داشته باشند
    <sup>
     <xref ref-type="bibr" rid="CR7">
      7
     </xref>
     <xref ref-type="bibr" rid="CR8">
      8
     </xref>
     ,
     <xref ref-type="bibr" rid="CR9">
      9
     </xref>
     ,
     <xref ref-type="bibr" rid="CR10">
      10
     </xref>
     ,
     <xref ref-type="bibr" rid="CR11">
      11
     </xref>
     <xref ref-type="bibr" rid="CR12">
      12
     </xref>
     ,
     <xref ref-type="bibr" rid="CR13">
      13
     </xref>
     <xref ref-type="bibr" rid="CR12">
      12
     </xref>
     ,
     <xref ref-type="bibr" rid="CR13">
      13
     </xref>
     ,
     <xref ref-type="bibr" rid="CR14">
      14
     </xref>
     ,
     <xref ref-type="bibr" rid="CR15">
      15
     </xref>
     ,
     <xref ref-type="bibr" rid="CR16">
      16
     </xref>
     <xref ref-type="bibr" rid="CR12">
      12
     </xref>
     <xref ref-type="bibr" rid="CR14">
      14
     </xref>
     ,
     <xref ref-type="bibr" rid="CR16">
      16
     </xref>
     <xref ref-type="bibr" rid="CR13">
      13
     </xref>
     ,
     <xref ref-type="bibr" rid="CR14">
      14
     </xref>
    </sup>
    .
   </p>
   <p id="Par4">
    اکوکاردیوگرافی رویکردی منحصربه‌فرد و مناسب برای کاربرد یادگیری عمیق در قلب‌شناسی است. به عنوان تکنیک تصویربرداری که به راحتی در دسترس و به طور گسترده‌ای برای ارزیابی عملکرد و ساختار قلب استفاده می‌شود، اکوکاردیوگرافی ترکیبی از تصویربرداری سریع و عدم وجود تابش یونیزان است که به عنوان ستون فقرات تصویربرداری قلبی‌عروقی عمل می‌کند. اکوکاردیوگرافی هم به عنوان یک روش غربالگری برای بیماران سالم و بدون علائم و هم برای تشخیص و مدیریت بیماران با بیماری‌های پیچیده قلبی‌عروقی به طور مکرر استفاده می‌شود. برای نشانه‌هایی از کاردیومیوپاتی‌ها تا بیماری‌های دریچه‌ای قلب، اکوکاردیوگرافی هم ضروری و هم کافی برای تشخیص بسیاری از بیماری‌های قلبی‌عروقی است. با وجود اهمیت آن در فنوتیپ‌سازی بالینی، در تفسیر انسانی تصاویر اکوکاردیوگرام تفاوت‌هایی وجود دارد که می‌تواند بر مراقبت بالینی تأثیر بگذارد. دستورالعمل‌های آموزشی رسمی برای قلب‌شناسان ارزش تجربه در تفسیر تصاویر اکوکاردیوگرام را به رسمیت می‌شناسند و آموزش پایه قلب‌شناسی ممکن است برای تفسیر اکوکاردیوگرام‌ها در بالاترین سطح کافی نباشد
    <sup>
     <xref ref-type="bibr" rid="CR4">
      4
     </xref>
     ,
     <xref ref-type="bibr" rid="CR17">
      17
     </xref>
     <xref ref-type="bibr" rid="CR17">
      17
     </xref>
     <xref ref-type="bibr" rid="CR18">
      18
     </xref>
     ,
     <xref ref-type="bibr" rid="CR19">
      19
     </xref>
     ,
     <xref ref-type="bibr" rid="CR20">
      20
     </xref>
     <xref ref-type="bibr" rid="CR21">
      21
     </xref>
    </sup>
    .
   </p>
   <p id="Par5">
    با توجه به اهمیت تصویربرداری در مراقبت‌های قلبی‌عروقی، یک خط لوله خودکار برای تفسیر تصویربرداری قلبی‌عروقی می‌تواند بهبود طبقه‌بندی خطر قبل از عمل، مدیریت خطر قلبی‌عروقی بیماران با بیماری‌های انکولوژیک تحت شیمی‌درمانی و کمک به تشخیص بیماری‌های قلبی‌عروقی را فراهم کند. در حالی که سایر کارها که یادگیری ماشین را به تصویربرداری پزشکی اعمال می‌کنند نیاز به بازنشانی تصاویر توسط کارشناسان انسانی داشتند، جریان کاری بالینی برای اکوکاردیوگرافی به طور ذاتی شامل بسیاری از اندازه‌گیری‌ها و محاسبات است و اغلب از طریق سیستم‌های گزارش‌دهی ساختاریافته گزارش می‌شود. توانایی استفاده از نشانه‌گذاری‌ها و تفسیرهای قبلی از گزارش‌های بالینی می‌تواند به طور قابل‌توجهی پذیرش یادگیری ماشین در تصویربرداری پزشکی را تسریع کند. با توجه به در دسترس بودن گزارش‌های بالینی قبلاً نشانه‌گذاری شده، تراکم اطلاعات در مجموعه داده‌های تصویری و ویدیویی و بسیاری از معماری‌های یادگیری ماشین موجود که قبلاً به مجموعه داده‌های تصویری اعمال شده‌اند، اکوکاردیوگرافی یک کاربرد با تأثیر بالا و بسیار قابل‌پیگیری از یادگیری ماشین در تصویربرداری پزشکی است
    <sup>
     <xref ref-type="bibr" rid="CR1">
      1
     </xref>
     ,
     <xref ref-type="bibr" rid="CR22">
      22
     </xref>
     ,
     <xref ref-type="bibr" rid="CR23">
      23
     </xref>
    </sup>
    .
   </p>
   <p id="Par6">
    ادبیات فعلی قبلاً نشان داده است که شناسایی نماهای استاندارد اکوکاردیوگرام از مجموعه داده‌های بدون برچسب ممکن است. کارهای قبلی از CNNهایی که بر روی تصاویر و ویدیوهای اکوکاردیوگرافی آموزش دیده‌اند برای انجام تقسیم‌بندی به منظور شناسایی ساختارهای قلبی و استخراج عملکرد قلب استفاده کرده‌اند. در این مطالعه، ما تحلیل‌های قبلی را گسترش می‌دهیم تا نشان دهیم که EchoNet، مدل یادگیری عمیق ما با استفاده از تصاویر اکوکاردیوگرافی، می‌تواند به طور قابل‌اعتمادی ساختارها و آناتومی محلی قلب را شناسایی کند، اندازه‌گیری‌های حجمی و معیارهای عملکرد قلب را تخمین بزند و فنوتیپ‌های انسانی سیستمیک که خطر قلبی‌عروقی را تغییر می‌دهند پیش‌بینی کند. علاوه بر این، ما اولین کاربرد چارچوب‌های تفسیر را برای درک مدل‌های یادگیری عمیق از تصاویر اکوکاردیوگرام نشان می‌دهیم. ویژگی‌های قابل‌شناسایی انسانی، مانند حضور لیدهای پیس‌میکر و دفیبریلاتور، هیپرتروفی بطن چپ و اندازه غیرطبیعی حفره دهلیز چپ که توسط CNN ما شناسایی شده‌اند، با استفاده از چارچوب‌های تفسیر برای برجسته کردن مناطق مورد علاقه مرتبط تأیید شدند. به بهترین دانش ما، ما اولین مدل یادگیری عمیق را توسعه می‌دهیم که می‌تواند به طور مستقیم سن، جنسیت، وزن و قد را از تصاویر اکوکاردیوگرام پیش‌بینی کند و از روش‌های تفسیر برای درک چگونگی پیش‌بینی این فنوتیپ‌های سیستمیک که برای مفسران انسانی دشوار است استفاده کند
    <sup>
     <xref ref-type="bibr" rid="CR5">
      5
     </xref>
     ,
     <xref ref-type="bibr" rid="CR6">
      6
     </xref>
     ,
     <xref ref-type="bibr" rid="CR24">
      24
     </xref>
    </sup>
    .
   </p>
  </sec>
  <sec id="Sec2" sec-type="results">
   <title>
    نتایج
   </title>
   <p id="Par7">
    ما یک مدل CNN را بر روی مجموعه داده‌ای از بیش از ۲.۶ میلیون تصویر اکوکاردیوگرام از ۲۸۵۰ بیمار آموزش دادیم تا ساختارهای محلی قلبی را شناسایی کند، عملکرد قلبی را تخمین بزند و عوامل خطر سیستمیک را پیش‌بینی کند (Fig.
    <xref ref-type="fig" rid="Fig1">
     1
    </xref>
    ). تصاویر اکوکاردیوگرام، گزارش‌ها و اندازه‌گیری‌ها از یک آزمایشگاه اکوکاردیوگرافی معتبر در یک مرکز پزشکی دانشگاهی بزرگ به دست آمدند (Table
    <xref ref-type="table" rid="Tab1">
     1
    </xref>
    ). اکوکاردیوگرافی ساختارهای قلبی را از زوایا و هندسه‌های مختلف تجسم می‌کند، بنابراین تصاویر بر اساس نمای قلبی طبقه‌بندی شدند تا مجموعه داده ورودی همگن شود. تصاویر اکوکاردیوگرام از ویدیوهای اکوکاردیوگرام نمونه‌برداری شدند، با حذف شناسایی از پیش‌پردازش شدند و برای حذف اطلاعات خارج از بخش اسکن برش داده شدند. این تصاویر پردازش‌شده برای آموزش EchoNet در وظیفه طبقه‌بندی یا پیش‌بینی پزشکی مربوطه استفاده شدند.
    <fig id="Fig1" position="float">
     <label>
      Fig. 1
     </label>
     <caption xml:lang="en">
      <title>
       خط لوله یادگیری ماشین EchoNet برای پیش‌بینی نتایج.
      </title>
      <p>
       <bold>
        a
       </bold>
       جریان کار EchoNet برای انتخاب تصویر، پاکسازی و آموزش مدل.
       <bold>
        b
       </bold>
       مقایسه عملکرد مدل با نماهای مختلف قلب به عنوان ورودی.
       <bold>
        c
       </bold>
       مثال‌هایی از افزایش داده. قاب اصلی به عنوان افزایش داده چرخانده می‌شود (از چپ به راست) و شدت آن افزایش می‌یابد (از بالا به پایین).
      </p>
     </caption>
     <graphic href="/ProjectMundo-Anon/MediaObjects/10X1038_s41746-019-0216-8/41746_2019_216_Fig1_HTML.png" mime-subtype="PNG" specific-use="web"/>
    </fig>
    <table-wrap id="Tab1">
     <label>
      Table 1
     </label>
     <caption xml:lang="en">
      <p>
       ویژگی‌های پایه بیماران در مجموعه داده‌های آموزشی و آزمایشی.
      </p>
     </caption>
     <table frame="hsides" rules="groups">
      <thead>
       <tr>
        <th>
         <p>
          ویژگی‌ها
         </p>
        </th>
        <th colspan="2">
         <p>
          داده‌های کامل
         </p>
        </th>
        <th colspan="2">
         <p>
          داده‌های نمای A4C
         </p>
        </th>
       </tr>
       <tr>
        <th/>
        <th>
         <p>
          داده‌های آموزشی
         </p>
        </th>
        <th>
         <p>
          داده‌های آزمایشی
         </p>
        </th>
        <th>
         <p>
          داده‌های آموزشی
         </p>
        </th>
        <th>
         <p>
          داده‌های آزمایشی
         </p>
        </th>
       </tr>
      </thead>
      <tbody>
       <tr>
        <td>
         <p>
          تعداد بیماران
         </p>
        </td>
        <td>
         <p>
          ۲۸۵۰
         </p>
        </td>
        <td>
         <p>
          ۳۷۳
         </p>
        </td>
        <td>
         <p>
          ۲۵۴۶
         </p>
        </td>
        <td>
         <p>
          ۳۳۷
         </p>
        </td>
       </tr>
       <tr>
        <td>
         <p>
          تعداد تصاویر
         </p>
        </td>
        <td>
         <p>
          ۱,۶۲۴,۷۸۰
         </p>
        </td>
        <td>
         <p>
          ۱۶۹,۸۸۰
         </p>
        </td>
        <td>
         <p>
          ۱۷۲,۰۸۰
         </p>
        </td>
        <td>
         <p>
          ۲۱,۵۴۰
         </p>
        </td>
       </tr>
       <tr>
        <td>
         <p>
          جنسیت (% مرد)
         </p>
        </td>
        <td>
         <p>
          ۵۲.۴٪
         </p>
        </td>
        <td>
         <p>
          ۵۲.۸٪
         </p>
        </td>
        <td>
         <p>
          ۵۲.۲٪
         </p>
        </td>
        <td>
         <p>
          ۵۳.۷٪
         </p>
        </td>
       </tr>
       <tr>
        <td>
         <p>
          سن: میانگین، سال (انحراف معیار)
         </p>
        </td>
        <td>
         <p>
          ۶۱.۳ (۱۷.۲)
         </p>
        </td>
        <td>
         <p>
          ۶۲.۸ (۱۶.۸)
         </p>
        </td>
        <td>
         <p>
          ۶۱.۱ (۱۷.۱)
         </p>
        </td>
        <td>
         <p>
          ۶۳.۲ (۱۶.۹)
         </p>
        </td>
       </tr>
       <tr>
        <td>
         <p>
          وزن: میانگین، کیلوگرم (انحراف معیار)
         </p>
        </td>
        <td>
         <p>
          ۷۸.۸ (۲۲.۷)
         </p>
        </td>
        <td>
         <p>
          ۷۸.۹ (۲۰.۸)
         </p>
        </td>
        <td>
         <p>
          ۷۸.۰ (۲۱.۷)
         </p>
        </td>
        <td>
         <p>
          ۷۸.۵ (۲۰.۲)
         </p>
        </td>
       </tr>
       <tr>
        <td>
         <p>
          قد: میانگین، متر (انحراف معیار)
         </p>
        </td>
        <td>
         <p>
          ۱.۶۹ (۰.۱۱)
         </p>
        </td>
        <td>
         <p>
          ۱.۶۹ (۰.۱۱)
         </p>
        </td>
        <td>
         <p>
          ۱.۶۹ (۰.۱۲)
         </p>
        </td>
        <td>
         <p>
          ۱.۶۹ (۰.۱۱)
         </p>
        </td>
       </tr>
       <tr>
        <td>
         <p>
          شاخص توده بدنی: میانگین (انحراف معیار)
         </p>
        </td>
        <td>
         <p>
          ۲۷.۳ (۶.۷)
         </p>
        </td>
        <td>
         <p>
          ۲۷.۵ (۶.۵)
         </p>
        </td>
        <td>
         <p>
          ۲۷.۱ (۶.۵)
         </p>
        </td>
        <td>
         <p>
          ۲۷.۳ (۶.۱)
         </p>
        </td>
       </tr>
       <tr>
        <td>
         <p>
          سیم‌کشی پیس‌میکر یا دفیبریلاتور (% موجود)
         </p>
        </td>
        <td>
         <p>
          ۱۳.۲
         </p>
        </td>
        <td>
         <p>
          ۱۴.۷
         </p>
        </td>
        <td>
         <p>
          ۱۳.۱
         </p>
        </td>
        <td>
         <p>
          ۱۵.۱
         </p>
        </td>
       </tr>
       <tr>
        <td>
         <p>
          بزرگ‌شدگی شدید دهلیز چپ (% موجود)
         </p>
        </td>
        <td>
         <p>
          ۱۷.۲
         </p>
        </td>
        <td>
         <p>
          ۲۰.۳
         </p>
        </td>
        <td>
         <p>
          ۱۸.۰
         </p>
        </td>
        <td>
         <p>
          ۲۱.۹
         </p>
        </td>
       </tr>
       <tr>
        <td>
         <p>
          هیپرتروفی بطن چپ (% موجود)
         </p>
        </td>
        <td>
         <p>
          ۳۳.۳
         </p>
        </td>
        <td>
         <p>
          ۳۸.۰
         </p>
        </td>
        <td>
         <p>
          ۳۲.۷
         </p>
        </td>
        <td>
         <p>
          ۳۷.۹
         </p>
        </td>
       </tr>
       <tr>
        <td>
         <p>
          حجم انتهای دیاستولیک، میلی‌لیتر: میانگین (انحراف معیار)
         </p>
        </td>
        <td>
         <p>
          ۹۴.۳ (۴۷.۲)
         </p>
        </td>
        <td>
         <p>
          ۹۴.۶ (۱۳.۰)
         </p>
        </td>
        <td>
         <p>
          ۹۵.۱ (۴۸.۲)
         </p>
        </td>
        <td>
         <p>
          ۹۶.۹ (۴۸.۰)
         </p>
        </td>
       </tr>
       <tr>
        <td>
         <p>
          حجم انتهای سیستولیک، میلی‌لیتر: میانگین (انحراف معیار)
         </p>
        </td>
        <td>
         <p>
          ۴۵.۶ (۳۸.۳)
         </p>
        </td>
        <td>
         <p>
          ۴۶.۲ (۳۶.۱)
         </p>
        </td>
        <td>
         <p>
          ۴۶.۰ (۳۹.۳)
         </p>
        </td>
        <td>
         <p>
          ۴۷.۰ (۳۶.۶)
         </p>
        </td>
       </tr>
       <tr>
        <td>
         <p>
          کسر جهشی: میانگین (انحراف معیار)
         </p>
        </td>
        <td>
         <p>
          ۵۵.۲ (۱۲.۳)
         </p>
        </td>
        <td>
         <p>
          ۵۴.۷ (۱۳.۰)
         </p>
        </td>
        <td>
         <p>
          ۵۵.۱ (۱۲.۲)
         </p>
        </td>
        <td>
         <p>
          ۵۴.۸ (۱۳.۱)
         </p>
        </td>
       </tr>
      </tbody>
     </table>
    </table-wrap>
   </p>
   <sec id="Sec3">
    <title>
     پیش‌بینی ساختارهای آناتومیک و ویژگی‌های محلی
    </title>
    <p id="Par8">
     بخشی استاندارد از جریان کاری بالینی تفسیر اکوکاردیوگرافی شناسایی ساختارهای محلی قلبی و توصیف مکان، اندازه و شکل آن است. ساختارهای محلی قلبی می‌توانند در ویژگی‌های تصویری تفاوت قابل‌توجهی داشته باشند، از اکوهای روشن ساختارهای فلزی داخل قلبی تا مناطق تاریک که نشان‌دهنده حوضچه‌های خون در حفره‌های قلبی هستند. به عنوان اولین وظیفه ما، EchoNet را بر روی سه وظیفه طبقه‌بندی که به طور مکرر توسط قلب‌شناسان ارزیابی می‌شوند و به شناسایی ویژگی‌های محلی متکی هستند آموزش دادیم (Fig.
     <xref ref-type="fig" rid="Fig2">
      2
     </xref>
     ). برچسب‌های حضور دستگاه‌های داخل قلبی (مانند کاتترها، لیدهای پیس‌میکر و دفیبریلاتور)، گشادشدگی شدید دهلیز چپ و هیپرتروفی بطن چپ از گزارش تفسیر شده توسط پزشک استخراج شدند و برای آموزش EchoNet بر روی تصاویر ورودی بدون برچسب از نمای چهار حفره‌ای راسی استفاده شدند. حضور یک لید پیس‌میکر با دقت بالا پیش‌بینی شد (AUC برابر ۰.۸۹، امتیاز F1 برابر ۰.۷۳)، به دنبال آن شناسایی یک دهلیز چپ به شدت گشاد شده (AUC برابر ۰.۸۵، امتیاز F1 برابر ۰.۶۸) و هیپرتروفی بطن چپ (AUC برابر ۰.۷۵، امتیاز F1 برابر ۰.۵۷) قرار گرفت. عملکرد مشابهی در پیش‌بینی طول محور اصلی دهلیز راست و تخمین حجم دهلیز چپ به دست آمد. نمودارهای پراکندگی در مواد تکمیلی نشان داده شده‌اند. برای درک پیش‌بینی‌های مدل، از روش‌های نقشه حساسیت مبتنی بر گرادیان برای شناسایی مناطق مورد علاقه برای تفسیر استفاده کردیم و نشان دادیم که EchoNet مناطق مرتبطی را که به ترتیب با دستگاه‌های داخل قلبی، دهلیز چپ و بطن چپ مطابقت دارند برجسته می‌کند. استحکام پیش‌بینی مدل‌ها همچنین با دستکاری مستقیم تصاویر ورودی، از جمله انسداد ویژگی‌های قابل‌شناسایی انسانی، بررسی شد تا تأیید شود که EchoNet با تمرکز بر مناطق مورد علاقه زیست‌شناختی قابل‌قبول به پیش‌بینی‌های خود می‌رسد. به عنوان مثال، در فریم‌های Fig.
     <xref ref-type="fig" rid="Fig2">
      2
     </xref>
     با لید پیس‌میکر، هنگامی که ما به صورت دستی لید را در فریم ماسک می‌کنیم، EchoNet پیش‌بینی خود را به عدم وجود پیس‌میکر تغییر می‌دهد
     <sup>
      <xref ref-type="bibr" rid="CR25">
       25
      </xref>
      <xref ref-type="bibr" rid="CR26">
       26
      </xref>
     </sup>
     .
     <fig id="Fig2" position="float">
      <label>
       Fig. 2
      </label>
      <caption xml:lang="en">
       <title>
        عملکرد و تفسیر EchoNet برای سه تفسیر بالینی از ساختارها و ویژگی‌های محلی.
       </title>
       <p>
        برای هر وظیفه، نمونه‌های مثبت نماینده در کنار مناطق مورد علاقه از مدل مربوطه نشان داده می‌شوند. نواحی سایه‌دار نشان‌دهنده
        <inline-formula id="IEq7">
         <alternatives>
          <math id="IEq7_Math" xmlns="http://www.w3.org/1998/Math/MathML">
           <mrow>
            <mn>
             95
            </mn>
            <mi>
             %
            </mi>
           </mrow>
          </math>
          <tex-math id="IEq7_TeX">
           \documentclass[12pt]{minimal}
                            \usepackage{amsmath}
                            \usepackage{wasysym}
                            \usepackage{amsfonts}
                            \usepackage{amssymb}
                            \usepackage{amsbsy}
                            \usepackage{mathrsfs}
                            \usepackage{upgreek}
                            \setlength{\oddsidemargin}{-69pt}
                            \begin{document}$$95 \%$$\end{document}
          </tex-math>
          <inline-graphic href="41746_2019_216_Article_IEq7.gif" mime-subtype="GIF" specific-use="web"/>
         </alternatives>
        </inline-formula>
        فواصل اطمینان هستند.
       </p>
      </caption>
      <graphic href="/ProjectMundo-Anon/MediaObjects/10X1038_s41746-019-0216-8/41746_2019_216_Fig2_HTML.png" mime-subtype="PNG" specific-use="web"/>
     </fig>
    </p>
   </sec>
   <sec id="Sec4">
    <title>
     پیش‌بینی عملکرد قلبی
    </title>
    <p id="Par9">
     کمی‌سازی عملکرد قلب یک ارزیابی حیاتی است که توسط اکوکاردیوگرافی انجام می‌شود. با این حال، تفسیر انسانی آن دارای تغییرات قابل توجهی است. کسر جهشی، معیاری از تغییر حجم در بطن چپ با هر ضربان قلب، یک معیار کلیدی از عملکرد قلب است، اما اندازه‌گیری آن به ردیابی دستی و زمان‌بر نواحی و حجم‌های بطن چپ در زمان‌های مختلف در طول چرخه قلبی وابسته است. ما EchoNet را برای پیش‌بینی حجم انتهای سیستولی بطن چپ (ESV)، حجم انتهای دیاستولی (EDV) و کسر جهشی از تصاویر نمای چهار حفره‌ای آپیکال نمونه‌برداری شده آموزش دادیم (Fig.
     <xref ref-type="fig" rid="Fig3">
      3
     </xref>
     ). ESV و EDV بطن چپ به‌طور دقیق پیش‌بینی شدند. برای پیش‌بینی ESV، یک
     <inline-formula id="IEq8">
      <alternatives>
       <math id="IEq8_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           R
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
       </math>
       <tex-math id="IEq8_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$${R}^{2}$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq8.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     امتیاز 0.74 و خطای مطلق میانگین (MAE) 13.3 mL به‌دست آمد در مقابل MAE 25.4 mL اگر از پیش‌بینی میانگین استفاده کنیم که ESV هر بیمار را به‌عنوان مقدار متوسط ESCV بیماران پیش‌بینی کنیم. نتیجه برای پیش‌بینی EDV یک
     <inline-formula id="IEq9">
      <alternatives>
       <math id="IEq9_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           R
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
       </math>
       <tex-math id="IEq9_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$${R}^{2}$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq9.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     امتیاز 0.70 و MAE 20.5 mL (پیش‌بینی میانگین MAE = 35.4 mL) بود. به‌طور معمول، کسر جهشی از نسبت این دو اندازه‌گیری حجمی محاسبه می‌شود، اما کسر جهشی محاسبه‌شده از حجم‌های پیش‌بینی‌شده کمتر دقیق بود (Fig.
     <xref ref-type="fig" rid="Fig3">
      3
     </xref>
     c) نسبت به EchoNet که مستقیماً بر روی کسر جهشی آموزش دیده بود (Fig.
     <xref ref-type="fig" rid="Fig3">
      3
     </xref>
     d). ما عملکرد نسبی یک مدل یادگیری عمیق را که از یک جریان کاری استاندارد انسانی برای ارزیابی ESV و EDV استفاده می‌کند و سپس کسر جهشی را از دو اندازه‌گیری حجمی محاسبه می‌کند در مقابل پیش‌بینی مستقیم "انتهای به انتها" یادگیری عمیق از کسر جهشی نشان می‌دهیم و نشان می‌دهیم که مدل پیش‌بینی "انتهای به انتها" یادگیری عمیق عملکرد بهتری داشت. با استفاده از EchoNet آموزش‌دیده، یک
     <inline-formula id="IEq10">
      <alternatives>
       <math id="IEq10_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           R
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
       </math>
       <tex-math id="IEq10_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$${R}^{2}$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq10.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     امتیاز 0.50 و MAE
     <inline-formula id="IEq11">
      <alternatives>
       <math id="IEq11_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mn>
          7.0
         </mn>
         <mi>
          %
         </mi>
        </mrow>
       </math>
       <tex-math id="IEq11_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$7.0 \%$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq11.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     به‌دست آمد (پیش‌بینی میانگین MAE =
     <inline-formula id="IEq12">
      <alternatives>
       <math id="IEq12_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mn>
          9.9
         </mn>
         <mi>
          %
         </mi>
        </mrow>
       </math>
       <tex-math id="IEq12_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$9.9 \%$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq12.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     ). برای هر مدل، روش‌های تفسیر توجه مناسب به بطن چپ به‌عنوان منطقه مورد علاقه برای تولید پیش‌بینی‌ها را نشان می‌دهند. مقایسه عملکرد مدل بر اساس تعداد فریم‌های ویدئویی نمونه‌برداری شده نشان نداد که عملکرد مدل پس از 11 فریم در هر وظیفه پیش‌بینی بهبود یافته است
     <sup>
      <xref ref-type="bibr" rid="CR18">
       18
      </xref>
      ,
      <xref ref-type="bibr" rid="CR19">
       19
      </xref>
     </sup>
     .
     <fig id="Fig3" position="float">
      <label>
       Fig. 3
      </label>
      <caption xml:lang="en">
       <title>
        عملکرد و تفسیر EchoNet برای اندازه و عملکرد بطن.
       </title>
       <p>
        عملکرد EchoNet برای
        <bold>
         a
        </bold>
        حجم انتهای سیستولی بطن چپ پیش‌بینی‌شده،
        <bold>
         b
        </bold>
        حجم انتهای دیاستولی بطن چپ پیش‌بینی‌شده،
        <bold>
         c
        </bold>
        کسر تخلیه محاسبه‌شده از ESV و EDV پیش‌بینی‌شده، و
        <bold>
         d
        </bold>
        کسر تخلیه پیش‌بینی‌شده.
        <bold>
         e
        </bold>
        تصویر ورودی، تفسیر و همپوشانی برای مدل کسر تخلیه.
       </p>
      </caption>
      <graphic href="/ProjectMundo-Anon/MediaObjects/10X1038_s41746-019-0216-8/41746_2019_216_Fig3_HTML.png" mime-subtype="PNG" specific-use="web"/>
     </fig>
    </p>
   </sec>
   <sec id="Sec5">
    <title>
     پیش‌بینی عوامل خطر سیستمیک قلبی عروقی
    </title>
    <p id="Par10">
     با عملکرد خوب در شناسایی ساختارهای محلی و برآورد اندازه‌گیری‌های حجمی قلب، ما به دنبال تعیین این بودیم که آیا EchoNet می‌تواند فنوتیپ‌های سیستمیک که خطر قلبی‌عروقی را تغییر می‌دهند نیز شناسایی کند. کارهای قبلی نشان داده‌اند که شبکه‌های عصبی پیچشی عمیق ظرفیت قدرتمندی برای تجمیع اطلاعات در مورد همبستگی‌های بصری بین داده‌های تصویربرداری پزشکی و فنوتیپ‌های سیستمیک دارند. EchoNet فنوتیپ‌های سیستمیک سن (
     <inline-formula id="IEq13">
      <alternatives>
       <math id="IEq13_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           R
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
       </math>
       <tex-math id="IEq13_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$${R}^{2}$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq13.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     = 0.46, MAE = 9.8 سال، پیش‌بینی میانگین MAE = 13.4 سال)، جنسیت (AUC = 0.88)، وزن (
     <inline-formula id="IEq14">
      <alternatives>
       <math id="IEq14_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           R
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
       </math>
       <tex-math id="IEq14_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$${R}^{2}$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq14.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     = 0.56, MAE = 10.7 کیلوگرم، پیش‌بینی میانگین MAE = 15.4 کیلوگرم)، و قد (
     <inline-formula id="IEq15">
      <alternatives>
       <math id="IEq15_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           R
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
       </math>
       <tex-math id="IEq15_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$${R}^{2}$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq15.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     = 0.33, MAE = 0.07 متر، پیش‌بینی میانگین MAE = 0.09 متر) را با عملکرد مشابه به پیش‌بینی‌های قبلی ویژگی‌های خاص قلبی پیش‌بینی کرد (Fig.
     <xref ref-type="fig" rid="Fig4">
      4
     </xref>
     a). شناخته شده است که ویژگی‌هایی مانند اندازه و هندسه حفره قلب با سن، جنسیت، وزن و قد متفاوت است، اما مفسران انسانی نمی‌توانند این فنوتیپ‌های سیستمیک را تنها از تصاویر اکوکاردیوگرام پیش‌بینی کنند. ما همچنین یادگیری چند وظیفه‌ای را بررسی کردیم - به اشتراک‌گذاری برخی از پارامترهای مدل در حین پیش‌بینی در سراسر فنوتیپ‌های مختلف - و این عملکرد مدل را بهبود نداد. نمودارهای بلاند-آلتمن از دقت مدل در رابطه با پیش‌بینی‌ها در Fig.
     <xref ref-type="fig" rid="Fig5">
      5
     </xref>
     و در مواد تکمیلی نشان داده شده است
     <sup>
      <xref ref-type="bibr" rid="CR12">
       12
      </xref>
      <xref ref-type="bibr" rid="CR27">
       27
      </xref>
      ,
      <xref ref-type="bibr" rid="CR28">
       28
      </xref>
     </sup>
     .
     <fig id="Fig4" position="float">
      <label>
       Fig. 4
      </label>
      <caption xml:lang="en">
       <title>
        عملکرد و تفسیر EchoNet برای فنوتیپ‌های سیستمیک.
       </title>
       <p>
        <bold>
         a
        </bold>
        عملکرد EchoNet برای پیش‌بینی چهار فنوتیپ سیستمیک (جنسیت، وزن، قد و سن) با استفاده از تصاویر نمای چهار حفره‌ای راسی. نواحی سایه‌دار نشان‌دهنده فواصل اطمینان ۹۵٪ هستند.
        <bold>
         b
        </bold>
        تفسیر مدل‌های فنوتیپ سیستمیک با مثال‌های مثبت نماینده که در کنار نواحی مورد علاقه نشان داده شده‌اند.
       </p>
      </caption>
      <graphic href="/ProjectMundo-Anon/MediaObjects/10X1038_s41746-019-0216-8/41746_2019_216_Fig4_HTML.png" mime-subtype="PNG" specific-use="web"/>
     </fig>
     <fig id="Fig5" position="float">
      <label>
       Fig. 5
      </label>
      <caption xml:lang="en">
       <title>
        نمودارهای بلاند-آلتمن از عملکرد EchoNet برای وظایف پیش‌بینی رگرسیون.
       </title>
       <p>
        خط سیاه جامد نشان‌دهنده میانه است. خطوط نقطه‌چین نارنجی، قرمز و آبی محدوده مرکزی ۵۰٪، ۷۵٪ و ۹۵٪ موارد را بر اساس تفاوت‌های بین مقادیر خودکار و اندازه‌گیری‌شده نشان می‌دهند.
       </p>
      </caption>
      <graphic href="/ProjectMundo-Anon/MediaObjects/10X1038_s41746-019-0216-8/41746_2019_216_Fig5_HTML.png" mime-subtype="PNG" specific-use="web"/>
     </fig>
    </p>
    <p id="Par11">
     در نهایت، ما از همان روش‌های نقشه حساسیت مبتنی بر گرادیان برای شناسایی مناطق مورد علاقه برای مدل‌هایی که فنوتیپ‌های سیستمیک را پیش‌بینی می‌کنند که برای کارشناسان انسانی دشوار است، استفاده کردیم. این مناطق مورد علاقه برای این مدل‌ها تمایل به پراکندگی بیشتری دارند، که نشان می‌دهد مدل‌ها برای فنوتیپ‌های سیستمیک به اندازه ویژگی‌های فردی یا مناطق محلی تکیه نمی‌کنند (Fig.
     <xref ref-type="fig" rid="Fig4">
      4
     </xref>
     b). تفسیرها برای مدل‌هایی که وزن و قد را پیش‌بینی می‌کنند توجه خاصی به نوک بخش اسکن داشتند، که نشان می‌دهد اطلاعات مربوط به ضخامت و ویژگی‌های دیواره قفسه سینه و بافت خارج قلبی برای پیش‌بینی وزن و قد بود.
    </p>
   </sec>
  </sec>
  <sec id="Sec6" sec-type="discussion">
   <title>
    بحث
   </title>
   <p id="Par12">
    در این مطالعه، ما نشان می‌دهیم که شبکه‌های عصبی پیچشی عمیق که بر روی تصاویر استاندارد اکوکاردیوگرام آموزش دیده‌اند می‌توانند ویژگی‌های محلی، معیارهای قابل تفسیر انسانی از عملکرد قلب و فنوتیپ‌های سیستمیک مانند سن، جنسیت، وزن و قد بیمار را شناسایی کنند. مدل‌های ما دقت پیش‌بینی بالایی برای وظایفی که به راحتی توسط مفسران انسانی انجام می‌شود، مانند برآورد کسر جهشی و حجم حفره‌ها و شناسایی لیدهای ضربان‌ساز، و همچنین برای وظایفی که برای مفسران انسانی چالش‌برانگیز است، مانند پیش‌بینی فنوتیپ‌های سیستمیک از تصاویر قلب به تنهایی، به‌دست آوردند. برخلاف کارهای قبلی در این زمینه، به جای استفاده از نتایج برچسب‌گذاری دستی، ما رویکردی را توصیف و مثال می‌زنیم که از فنوتیپ‌ها و تفسیرهای قبلاً به‌دست‌آمده از سوابق بالینی برای آموزش مدل استفاده می‌کند، که می‌تواند اعتبار خارجی بیشتری و تعمیم سریع‌تر با مجموعه داده‌های آموزشی بزرگ‌تر را امکان‌پذیر کند.
   </p>
   <p id="Par13">
    یکی از انتقادات رایج به مدل‌های یادگیری عمیق در مجموعه داده‌های تصویربرداری پزشکی، ماهیت "جعبه سیاه" پیش‌بینی‌ها و ناتوانی در درک توانایی مدل‌ها برای شناسایی ویژگی‌های مرتبط است. علاوه بر نشان دادن عملکرد پیش‌بینی روش‌های ما، ما پیش‌بینی‌های مدل را با برجسته کردن مناطق مهم و قابل قبول بیولوژیکی که با هر تفسیر مطابقت دارند، اعتبارسنجی می‌کنیم. این نتایج اولین ارائه تکنیک‌های تفسیر برای مدل‌های یادگیری عمیق بر روی تصاویر اکوکاردیوگرافی را نشان می‌دهند و می‌توانند اعتماد به مدل‌های ساده را ایجاد کنند زیرا پیکسل‌های مرتبط هنگام شناسایی ساختارهای محلی مانند لیدهای ضربان‌ساز برجسته می‌شوند. علاوه بر این، این رویکرد استفاده از چارچوب‌های تفسیر برای شناسایی مناطق مورد علاقه ممکن است زمینه‌های اضافی برای درک فیزیولوژی انسانی هنگام تفسیر خروجی‌های مدل‌های یادگیری عمیق برای فنوتیپ‌های چالش‌برانگیز و غیرقابل توضیح انسانی در تصویربرداری پزشکی فراهم کند. این نتایج گامی به سوی ارزیابی خودکار تصاویر اکوکاردیوگرام از طریق یادگیری عمیق را نشان می‌دهند. ما معتقدیم که این تحقیق می‌تواند رویکردهای آینده برای غربالگری بیماری قلبی‌عروقی تحت بالینی و درک پایه بیولوژیکی پیری قلبی‌عروقی را تکمیل کند.
   </p>
   <p id="Par14">
    در حالی که سن، جنسیت، وزن و قد فنوتیپ‌های بصری نسبتاً واضحی هستند، مقاله ما مدل‌هایی را ارائه می‌دهد که این فنوتیپ‌های سیستمیک را در نقشه راهی از پیش‌بینی‌های مبتنی بر ویژگی‌های محلی ساده، به پیش‌بینی‌های اندازه‌گیری پویا پیچیده‌تر و در نهایت به طبقه‌بندی‌های دشوار انسانی از فنوتیپ‌های سیستمیک بدون ویژگی‌های محلی واضح پیش‌بینی می‌کنند. مطالعات قبلی نشان داده‌اند که تصویربرداری پزشکی از سایر سیستم‌های اندام می‌تواند عوامل خطر قلبی‌عروقی از جمله سن، جنسیت و فشار خون را با شناسایی ویژگی‌های محلی فنوتیپ‌های سیستمیک پیش‌بینی کند. اخیراً، مدل‌های یادگیری عمیق مبتنی بر ECG 12 لید نشان داده‌اند که می‌توانند سن و جنسیت را به‌طور دقیق پیش‌بینی کنند، که یک فنوتیپ قلبی برای پیری و دیسمورفیسم جنسیتی را بیشتر تأیید می‌کند. نتایج ما یک مسیر دیگر برای شناسایی فنوتیپ‌های سیستمیک از طریق تصویربرداری خاص سیستم اندام شناسایی می‌کند. این نتایج توسط مطالعات قبلی که مقادیر نرمال سطح جمعیت برای اندازه‌های حفره‌های ساختارهای قلبی را نشان داده‌اند، پشتیبانی می‌شود، زیرا شرکت‌کنندگان بر اساس سن، جنسیت، قد و وزن متفاوت هستند. تغییرات مرتبط با سن در قلب، به‌ویژه تغییر اندازه‌های حفره و پارامترهای پر شدن دیاستولیک، به‌خوبی مشخص شده‌اند، و مطالعه ما بر این اساس کار می‌کند تا نشان دهد که این سیگنال‌ها وجود دارند تا امکان پیش‌بینی این فنوتیپ‌ها را با دقتی که قبلاً گزارش نشده است، فراهم کنند. از آنجا که فنوتیپ‌های سیستمیک سن، جنسیت و شاخص توده بدنی به شدت با نتایج قلبی‌عروقی و امید به زندگی کلی مرتبط هستند، توانایی مدل‌های یادگیری عمیق برای شناسایی ویژگی‌های نهفته پیش‌بینی‌کننده نشان می‌دهد که کار آینده بر روی مدل‌های یادگیری عمیق مبتنی بر تصویر می‌تواند ویژگی‌هایی را که از دید ناظران انسانی پنهان است شناسایی کند و نتایج و مرگ و میر را پیش‌بینی کند
    <sup>
     <xref ref-type="bibr" rid="CR12">
      12
     </xref>
     <xref ref-type="bibr" rid="CR29">
      29
     </xref>
     <xref ref-type="bibr" rid="CR27">
      27
     </xref>
     ,
     <xref ref-type="bibr" rid="CR28">
      28
     </xref>
     <xref ref-type="bibr" rid="CR30">
      30
     </xref>
     ,
     <xref ref-type="bibr" rid="CR31">
      31
     </xref>
     <xref ref-type="bibr" rid="CR32">
      32
     </xref>
     ,
     <xref ref-type="bibr" rid="CR33">
      33
     </xref>
     ,
     <xref ref-type="bibr" rid="CR34">
      34
     </xref>
    </sup>
    .
   </p>
   <p id="Par15">
    علاوه بر اندازه حفره، ویژگی‌های خارج قلبی و همچنین ویژگی‌های برچسب‌گذاری نشده اضافی در مدل‌های ما برای پیش‌بینی فنوتیپ‌های سیستمیک بیمار گنجانده شده‌اند. ناحیه نزدیک به مبدل، که نمایانگر بافت زیرجلدی، دیواره قفسه سینه، پارانشیم ریه و سایر ساختارهای خارج قلبی است، در مدل‌های پیش‌بینی وزن و قد برجسته شده است. این نقشه‌های تفسیر با دانش قبلی که بیماران چاق اغلب دارای چالش‌هایی در کسب تصویر هستند، سازگار است، اما دقتی که در پیش‌بینی قد و وزن به ارمغان می‌آورد شگفت‌انگیز است. بررسی بازنگری پیش‌بینی‌ها توسط مدل ما نشان می‌دهد که ویژگی‌های قابل تفسیر انسانی که قابلیت زیستی را نشان می‌دهند. در نقشه‌های برجستگی برای مدل پیش‌بینی سن، توجه قابل توجهی به تقاطع قلب، شامل سپتوم داخل دهلیزی، جایی که حلقه آئورت به‌عنوان نمای نزدیک‌تر به نمای پنج حفره‌ای آپیکال می‌شود، درج سپتوم برگچه‌های میترال و تریکوسپید و دستگاه میترال معطوف شد. این ناحیه‌ای است که در آن کلسیفیکاسیون متفاوت می‌تواند دیده شود، به‌ویژه در دریچه آئورت و حلقه میترال، و به‌خوبی با تغییرات مرتبط با سن مرتبط است. تصاویری که پیش‌بینی می‌شود از بیماران جوان‌تر باشند نیز ترجیح برای دهلیزهای کوچک را نشان می‌دهند و با مطالعات قبلی که تغییرات مرتبط با سن در دهلیز چپ را نشان می‌دهند، سازگار است. حلقه بازخورد بین پزشک و مدل‌های یادگیری ماشین با بررسی بالینی تصاویر پیش‌بینی‌شده مناسب و نامناسب می‌تواند به درک بیشتر از تغییرات طبیعی در اکوکاردیوگرام‌های انسانی و همچنین شناسایی ویژگی‌هایی که قبلاً توسط مفسران انسانی نادیده گرفته شده‌اند کمک کند. درک اشتباهات طبقه‌بندی، مانند بیماران با سن بیولوژیکی جوان اما سن پیش‌بینی‌شده بالا، و بررسی بیشتر افراد افراطی می‌تواند به شناسایی بیماری قلبی‌عروقی تحت بالینی و درک بهتر فرآیند پیری کمک کند
    <sup>
     <xref ref-type="bibr" rid="CR35">
      35
     </xref>
     ,
     <xref ref-type="bibr" rid="CR36">
      36
     </xref>
     <xref ref-type="bibr" rid="CR37">
      37
     </xref>
     ,
     <xref ref-type="bibr" rid="CR38">
      38
     </xref>
     <xref ref-type="bibr" rid="CR31">
      31
     </xref>
     ,
     <xref ref-type="bibr" rid="CR39">
      39
     </xref>
    </sup>
    .
   </p>
   <p id="Par16">
    کارهای بنیادی قبلی در زمینه تفسیر یادگیری عمیق تصاویر اکوکاردیوگرام بر روی مکانیک دستیابی به نمای صحیح اکوکاردیوگرافی و سناریوهای دست‌ساز با جمعیت‌های بیمار به دقت انتخاب شده و پردازش چند مرحله‌ای و انتخاب و محاسبه ویژگی‌های پس‌پردازش متمرکز بوده است. کار توصیف شده در اینجا بر استفاده از معماری‌ها و تکنیک‌های یادگیری عمیق مدرن‌تر در چارچوب استفاده از فنوتیپ‌های قبلاً قضاوت شده با پتانسیل مقیاس‌پذیری سریع الگوریتم‌ها به عمل بالینی تمرکز دارد. با گسترش سریع منابع محاسباتی، ما توانستیم تصاویر با وضوح بالاتر (۲۹۹×۲۹۹ به جای ۶۰×۸۰ در مطالعات قبلی) را وارد کنیم و رویکردی "پایان به پایان" برای پیش‌بینی فنوتیپ‌های پیچیده مانند کسر جهشی که واریانس کمتری نسبت به تکنیک‌های چند مرحله‌ای دارد که نیاز به شناسایی انتهای سیستول، انتهای دیاستول و مراحل جداسازی جداگانه دارند، ارائه دهیم
    <sup>
     <xref ref-type="bibr" rid="CR5">
      5
     </xref>
     ,
     <xref ref-type="bibr" rid="CR24">
      24
     </xref>
     <xref ref-type="bibr" rid="CR24">
      24
     </xref>
     <xref ref-type="bibr" rid="CR5">
      5
     </xref>
    </sup>
    .
   </p>
   <p id="Par17">
    در حالی که عملکرد مدل ما نسبت به نتایج کارهای قبلی بهبود یافته است، ارزیابی EchoNet از اندازه‌گیری‌های بالینی ESV، EDV و EF دارای واریانس غیرقابل‌چشم‌پوشی است و از ارزیابی انسانی این معیارها فراتر نمی‌رود. برای این وظایف، زمینه بالینی و درک اطلاعات متنی و اندازه‌گیری‌های دیگر احتمالاً اهمیت زیادی برای وظیفه آموزشی دارد. به عنوان مثال، ارزیابی EF به عنوان نسبت ESV و EDV خطاها را بزرگ می‌کند و عملکرد بدتری نسبت به تخمین ESV یا EDV به صورت جداگانه دارد. کار آینده نیاز به ادغام بیشتر اطلاعات زمانی بین فریم‌ها برای ارزیابی بهتر حرکت قلب و وابستگی‌های متقابل در ساختارهای قلبی دارد. علاوه بر اندازه‌گیری‌های کمی، ارزیابی انسانی ساختارهای قلبی، مانند ردیابی‌های بطن چپ، می‌تواند مجموعه داده‌های آموزشی با ارزش بالایی باشد.
   </p>
   <p id="Par18">
    تکنیک‌های جدید یادگیری ماشین برای تفسیر فعال‌سازی شبکه‌ها نیز برای اولین بار ارائه شده‌اند تا مناطق مورد علاقه در تفسیر تصاویر اکوکاردیوگرام را درک کنند. در حالی که کارهای قبلی از نتایج دست‌نشانده و گروه‌های بیمار برای اکثر برچسب‌های نتایج خود استفاده کرده‌اند، ما رویکردی را توصیف و نمایش می‌دهیم که از فنوتیپ‌ها و تفسیرهای قبلاً به‌دست‌آمده از سوابق بالینی برای آموزش مدل استفاده می‌کند، که می‌تواند اعتبار خارجی بیشتری را فراهم کند و تعمیم سریع‌تری با مجموعه داده‌های آموزشی بزرگ‌تر امکان‌پذیر کند. علاوه بر این، با توجه به تفاوت قابل توجه بین تصاویر در ImageNet و تصاویر اکوکاردیوگرام، پیش‌آموزش با وزن‌های ImageNet به طور قابل توجهی عملکرد مدل را بهبود نداد، اما مدل‌های ما که بر روی فنوتیپ‌های سیستمیک آموزش دیده‌اند می‌توانند وزن‌های شروع خوبی برای کارهای آینده در آموزش بر روی تصاویر اکوکاردیوگرام با فنوتیپ‌های پیچیده‌تر باشند
    <sup>
     <xref ref-type="bibr" rid="CR25">
      25
     </xref>
    </sup>
    .
   </p>
   <p id="Par19">
    مطالعات قبلی یادگیری عمیق بر روی تصویربرداری پزشکی بر روی روش‌های تصویربرداری پرهزینه که در محیط‌های غنی از منابع رایج هستند یا تصویربرداری زیرتخصصی با نشانه‌گذاری متمرکز تمرکز داشته‌اند. این روش‌ها اغلب به نشانه‌گذاری بازنگری شده توسط کارشناسان نیاز دارند زیرا جریان کار بالینی اغلب به اندازه‌گیری‌های دقیق یا مکان‌یابی‌ها نیاز ندارد. در توسعه هر مدل یادگیری ماشین برای سوالات بهداشتی، اعتبار خارجی از اهمیت درجه اول برخوردار است. یک نکته مهم در کار ما این است که تصاویر به‌دست‌آمده از یک نوع دستگاه اولتراسوند بوده و مجموعه داده آزمایشی ما از بیماران مختلف بود اما همچنین با استفاده از همان دستگاه و در همان مؤسسه اسکن شده‌اند. رویکرد ما مدل‌های یادگیری عمیق را بر روی مطالعات قبلی و نشانه‌گذاری‌های مرتبط از EMR آموزش می‌دهد تا از داده‌های گذشته برای استقرار سریع مدل‌های یادگیری ماشین استفاده کند. این رویکرد از دو مزیت اکوکاردیوگرافی بهره می‌برد، اول اینکه اکوکاردیوگرافی یکی از پرکاربردترین مطالعات تصویربرداری در ایالات متحده است و دوم اینکه اکوکاردیوگرافی اغلب از گزارش‌دهی ساختاری استفاده می‌کند، که پیشرفت‌های یادگیری عمیق را به‌ویژه قابل‌اجرا و قابل‌تعمیم می‌کند. با این حال، چنین روشی به استاندارد بالینی بستگی دارد، زیرا تنوع شناخته‌شده‌ای بین روش‌های مشتق‌شده از MRI و اکوکاردیوگرافی وجود دارد و آموزش بر روی گزارش‌های بالینی نیاز به کنترل کیفیت دقیق از آزمایشگاه اکوکاردیوگرافی مؤسسه دارد. کار آینده بر روی یادگیری عمیق اکوکاردیوگرافی نیاز به تأیید عملکرد در جمعیت‌ها و محیط‌های گسترده‌تر دارد. خودکارسازی تفسیر اکوکاردیوگرافی از طریق یادگیری عمیق می‌تواند مراقبت‌های قلبی‌عروقی را بیشتر در دسترس قرار دهد. با استفاده بیشتر از اولتراسوند در نقطه مراقبت توسط تعداد فزاینده‌ای از پزشکان، از پزشکان اورژانس گرفته تا داخلی‌ها و بیهوشی‌ها، و یادگیری عمیق بر روی تصاویر اولتراسوند قلبی می‌تواند پیش‌بینی‌ها و تشخیص‌های دقیقی را به طیف وسیع‌تری از بیماران ارائه دهد
    <sup>
     <xref ref-type="bibr" rid="CR40">
      40
     </xref>
     ,
     <xref ref-type="bibr" rid="CR41">
      41
     </xref>
     <xref ref-type="bibr" rid="CR12">
      12
     </xref>
     ,
     <xref ref-type="bibr" rid="CR13">
      13
     </xref>
     ,
     <xref ref-type="bibr" rid="CR16">
      16
     </xref>
     <xref ref-type="bibr" rid="CR42">
      42
     </xref>
    </sup>
    .
   </p>
   <p id="Par20">
    در خلاصه، ما شواهدی ارائه می‌دهیم که یادگیری عمیق می‌تواند وظایف تفسیر انسانی رایج را بازتولید کند و از اطلاعات اضافی برای پیش‌بینی فنوتیپ‌های سیستمیک که می‌تواند به طبقه‌بندی بهتر خطر قلبی‌عروقی کمک کند، استفاده کند. ما از روش‌های تفسیر استفاده کردیم که می‌تواند بازخورد مناطق مورد علاقه مرتبط را برای بررسی بیشتر توسط متخصصان قلب ارائه دهد تا پیری را بهتر درک کنند و از بیماری قلبی‌عروقی جلوگیری کنند. کار ما می‌تواند ارزیابی فیزیولوژی قلب، آناتومی و طبقه‌بندی خطر را در سطح جمعیت با خودکارسازی جریان‌های کاری رایج در اکوکاردیوگرافی بالینی ممکن سازد و تفسیر تخصصی را به جمعیت عمومی بیماران دموکراتیزه کند.
   </p>
  </sec>
  <sec id="Sec7" sec-type="methods">
   <title>
    روش‌ها
   </title>
   <sec id="Sec8">
    <title>
     مجموعه داده
    </title>
    <p id="Par21">
     پایگاه داده اکوکاردیوگرافی استنفورد شامل تصاویر، گزارش‌های پزشک و داده‌های بالینی از بیمارانی است که در بیمارستان استنفورد تحت اکوکاردیوگرافی در جریان مراقبت‌های معمول قرار گرفته‌اند. آزمایشگاه اکوکاردیوگرافی معتبر، تصویربرداری قلبی را به طیف وسیعی از بیماران با شرایط مختلف قلبی از جمله فیبریلاسیون دهلیزی، بیماری عروق کرونر، کاردیومیوپاتی، تنگی آئورت و آمیلوئیدوز ارائه می‌دهد. برای این مطالعه، ما از ۳۳۱۲ مطالعه اکوکاردیوگرافی جامع غیر استرس متوالی که بین ژوئن ۲۰۱۸ و دسامبر ۲۰۱۸ به‌دست‌آمده بود، استفاده کردیم و بیماران را به‌طور تصادفی به گروه‌های آموزشی، اعتبارسنجی و آزمایشی مستقل تقسیم کردیم. ویدئوهای نمای استاندارد قلبی، ویدئوهای داپلر رنگی و تصاویر ثابت هر مطالعه را تشکیل می‌دهند و در قالب DICOM ذخیره می‌شوند. ویدئوها برای به‌دست‌آوردن ۱,۶۲۴,۷۸۰ تصویر مقیاس‌شده ۲۹۹×۲۹۹ پیکسل نمونه‌برداری شدند. نرخ نمونه‌برداری برای بهینه‌سازی اندازه مدل و زمان آموزش در حالی که عملکرد مدل حفظ می‌شود انتخاب شد و جزئیات پیش‌پردازش اضافی در مواد تکمیلی توصیف شده است. برای هر تصویر، اطلاعات مربوط به به‌دست‌آوردن تصویر، اطلاعات شناسایی و سایر اطلاعات خارج از بخش تصویربرداری از طریق ماسک‌گذاری حذف شد. تفسیرهای انسانی از گزارش تفسیر شده توسط پزشک و ویژگی‌های بالینی از پرونده پزشکی الکترونیکی با هر مطالعه اکوکاردیوگرافی برای آموزش مدل مطابقت داده شد. این مطالعه توسط IRB دانشگاه استنفورد تأیید شد. رضایت‌نامه کتبی برای بازنگری گذشته‌نگر تصاویر به‌دست‌آمده در جریان مراقبت‌های استاندارد معاف شد.
    </p>
   </sec>
   <sec id="Sec9">
    <title>
     مدل
    </title>
    <p id="Par22">
     ما یک معماری CNN را انتخاب کردیم که عرض و عمق شبکه را متعادل کند تا هزینه محاسباتی آموزش را مدیریت کند. ما از معماری مبتنی بر Inception-Resnet-v1 برای پیش‌بینی تمام فنوتیپ‌های خود استفاده کردیم. این معماری عملکرد قوی در مجموعه داده‌های معیار مانند چالش تشخیص تصویر ILSVR2012 (Imagenet) دارد و از نظر محاسباتی نسبت به شبکه‌های دیگر کارآمد است. پیش‌آموزش Inception-ResNet با ImageNet به طور قابل توجهی عملکرد مدل را افزایش نداد و مدل نهایی ما از وزن‌های به‌طور تصادفی آغاز شده استفاده کرد
     <sup>
      <xref ref-type="bibr" rid="CR10">
       10
      </xref>
      <xref ref-type="bibr" rid="CR9">
       9
      </xref>
      <xref ref-type="bibr" rid="CR43">
       43
      </xref>
     </sup>
     .
    </p>
    <p id="Par23">
     برای هر وظیفه پیش‌بینی، یک معماری CNN بر روی فریم‌های جداگانه از هر ویدئوی اکوکاردیوگرام با برچسب‌های خروجی که یا از پرونده الکترونیکی پزشکی یا از گزارش پزشک استخراج شده بودند، آموزش داده شد. از هر ویدئو، ۲۰ فریم (یک فریم در هر ۱۰۰ میلی‌ثانیه) از اولین فریم ویدئو نمونه‌برداری شد. پیش‌بینی نهایی با میانگین‌گیری از تمام پیش‌بینی‌های فریم‌های جداگانه انجام شد. چندین روش جایگزین برای تجمیع پیش‌بینی‌های سطح فریم به یک پیش‌بینی سطح بیمار بررسی شد و نتایج بهتری نسبت به میانگین‌گیری ساده به دست نیامد.
    </p>
    <p id="Par24">
     آموزش مدل با استفاده از کتابخانه TensorFlow انجام شد که قادر به استفاده از قابلیت‌های پردازش موازی واحدهای پردازش گرافیکی (GPUs) برای آموزش سریع مدل‌های یادگیری عمیق است. ما بهینه‌ساز Adam را به عنوان الگوریتم بهینه‌سازی خود انتخاب کردیم که از نظر محاسباتی کارآمد است، حافظه کمی مصرف می‌کند و عملکرد برتری در بسیاری از وظایف یادگیری عمیق نشان داده است. به عنوان زیان پیش‌بینی، از زیان آنتروپی متقاطع برای وظایف طبقه‌بندی و زیان خطای مربعی برای وظایف رگرسیون همراه با استفاده از زیان تنظیم وزن-کاهش برای جلوگیری از بیش‌برازش استفاده کردیم. ما انواع دیگر زیان پیش‌بینی (زیان مطلق، زیان Huber برای رگرسیون و زیان Focal برای طبقه‌بندی) را بررسی کردیم و آن‌ها عملکرد را بهبود ندادند. برای هر وظیفه پیش‌بینی، بهترین ابرپارامترها را با استفاده از جستجوی شبکه‌ای (۲۴ مدل آموزش‌دیده برای هر وظیفه) برای بهینه‌سازی نرخ یادگیری و عامل تنظیم وزن-کاهش انتخاب کردیم. برای انجام انتخاب مدل، برای هر وظیفه، داده‌های آموزشی را به مجموعه‌های آموزشی و اعتبارسنجی تقسیم کردیم با استفاده از
     <inline-formula id="IEq16">
      <alternatives>
       <math id="IEq16_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mn>
          10
         </mn>
         <mi>
          %
         </mi>
        </mrow>
       </math>
       <tex-math id="IEq16_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10 \%$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq16.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     از داده‌های آموزشی به عنوان مجموعه اعتبارسنجی نگه‌داشته‌شده؛ مدلی که بهترین عملکرد را در مجموعه اعتبارسنجی داشت سپس بر روی مجموعه آزمون بررسی شد تا نتایج عملکرد نهایی گزارش شود. پس از آموزش مدل‌ها، آن‌ها بر روی مجموعه جداگانه‌ای از فریم‌های آزمون که از مطالعات اکوکاردیوگرام ۳۳۷ بیمار دیگر با جمعیت‌شناسی مشابه جمع‌آوری شده بودند، ارزیابی شدند (جدول
     <xref ref-type="table" rid="Tab1">
      1
     </xref>
     ). این بیماران به صورت تصادفی برای یک مجموعه آزمون ۱۰% نگه‌داشته‌شده انتخاب شدند و مدل در طول آموزش آن‌ها را ندیده بود
     <sup>
      <xref ref-type="bibr" rid="CR44">
       44
      </xref>
      <xref ref-type="bibr" rid="CR45">
       45
      </xref>
      <xref ref-type="bibr" rid="CR46">
       46
      </xref>
      <xref ref-type="bibr" rid="CR47">
       47
      </xref>
      <xref ref-type="bibr" rid="CR48">
       48
      </xref>
     </sup>
     .
    </p>
   </sec>
   <sec id="Sec10">
    <title>
     افزایش داده
    </title>
    <p id="Par25">
     عملکرد مدل با افزایش اندازه نمونه داده‌های ورودی بهبود یافت. آزمایش‌های ما بهبود نسبی اضافی را با افزایش تعداد بیماران نمایندگی شده در گروه آموزشی در مقایسه با نمونه‌برداری بیش از حد فریم‌ها در هر بیمار نشان داد. افزایش داده با استفاده از روش‌های قبلاً تأیید شده، همچنین به طور قابل توجهی تعمیم پیش‌بینی‌های مدل را با کاهش بیش‌برازش در مجموعه آموزشی بهبود بخشید. در طول فرآیند آموزش، در هر مرحله بهینه‌سازی، هر تصویر آموزشی از طریق تغییرات هندسی (مانند چرخش، انعکاس و ترجمه) و تغییرات در کنتراست و اشباع تغییر می‌کند. در نتیجه، مجموعه داده آموزشی به یک مجموعه داده مؤثر بزرگ‌تر افزایش می‌یابد. در این کار، با تقلید از تغییرات در کسب تصویر اکوکاردیوگرافی، از چرخش تصادفی و افزایش اشباع تصادفی برای افزایش داده استفاده کردیم (Fig.
     <xref ref-type="fig" rid="Fig1">
      1
     </xref>
     c). در هر مرحله از نزول گرادیان تصادفی در فرآیند آموزش، ۲۴ فریم آموزشی به صورت تصادفی نمونه‌برداری می‌کنیم و هر فریم آموزشی را با یک چرخش تصادفی بین -۲۰ تا ۲۰ درجه و با افزودن یک عدد به صورت یکنواخت بین -۰.۱ تا ۰.۱ به پیکسل‌های تصویر (مقادیر پیکسل‌ها نرمال‌سازی شده‌اند) برای افزایش یا کاهش روشنایی تصویر تغییر می‌دهیم. افزایش داده منجر به بهبود برای همه وظایف شد؛ بین ۱-۴% بهبود در معیار AUC برای وظایف طبقه‌بندی و ۲-۱۰% بهبود در
     <inline-formula id="IEq17">
      <alternatives>
       <math id="IEq17_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           R
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
       </math>
       <tex-math id="IEq17_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}^{2}$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq17.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     امتیاز برای وظایف رگرسیون
     <sup>
      <xref ref-type="bibr" rid="CR49">
       49
      </xref>
      ,
      <xref ref-type="bibr" rid="CR50">
       50
      </xref>
     </sup>
     .
    </p>
   </sec>
   <sec id="Sec11">
    <title>
     انتخاب نمای قلبی
    </title>
    <p id="Par26">
     ابتدا تلاش کردیم از تمام تصاویر اکوکاردیوگرام برای وظایف پیش‌بینی استفاده کنیم اما با توجه به اندازه مطالعات اکوکاردیوگرام، تلاش‌های اولیه با زمان‌های طولانی آموزش، همگرایی ضعیف مدل و دشواری در اشباع مدل مواجه شد. با دانستن اینکه در یک مطالعه جامع اکوکاردیوگرافی، همان ساختارهای قلبی اغلب از دیدگاه‌های متعدد برای تأیید و تأیید ارزیابی‌ها از دیدگاه‌های دیگر مشاهده می‌شوند، با استفاده از زیرمجموعه‌هایی از تصاویر بر اساس دیدگاه قلبی، آموزش مدل را آزمایش کردیم. همان‌طور که در Fig.
     <xref ref-type="fig" rid="Fig1">
      1
     </xref>
     b توصیف شده است، انتخابی از رایج‌ترین دیدگاه‌های استاندارد اکوکاردیوگرام برای عملکرد مدل ارزیابی شد. تصاویر از هر مطالعه با استفاده از یک روش آموزشی نظارت‌شده که قبلاً توصیف شده بود، طبقه‌بندی شدند. ما به دنبال شناسایی دیدگاه‌های غنی از اطلاعات با آموزش مدل‌های جداگانه بر روی زیرمجموعه‌های تصاویر مجموعه داده از تنها یک دیدگاه قلبی بودیم. آموزش یک مدل با استفاده از تنها یک دیدگاه قلبی منجر به کاهش یک مرتبه بزرگی در زمان آموزش و هزینه محاسباتی با حفظ عملکرد پیش‌بینی مشابه زمانی که از دیدگاه‌های غنی از اطلاعات استفاده می‌شد، می‌شود. برای هر یک از وظایف پیش‌بینی و انتخاب خاص ابرپارامترها، آموزش یک مدل بر روی مجموعه داده A4C-View در ~۳۰ ساعت با استفاده از یک GPU Titan XP همگرا می‌شود. فرآیند آموزش همان مدل و وظیفه پیش‌بینی در ~۲۴۰ ساعت با استفاده از تمام دیدگاه‌ها در مجموعه داده همگرا می‌شود. با توجه به تعادل مطلوب عملکرد به هزینه محاسباتی و همچنین دانش قبلی در مورد اینکه کدام دیدگاه‌ها بیشتر توسط کاردیولوژیست‌ها اولویت‌بندی می‌شوند، دیدگاه چهار حفره‌ای آپیکال را به عنوان مجموعه آموزشی ورودی برای آزمایش‌های بعدی در آموزش ویژگی‌های محلی، برآوردهای حجمی و فنوتیپ‌های سیستمیک انتخاب کردیم
     <sup>
      <xref ref-type="bibr" rid="CR5">
       5
      </xref>
     </sup>
     .
    </p>
   </sec>
   <sec id="Sec12">
    <title>
     قابلیت تفسیر
    </title>
    <p id="Par27">
     روش‌های تفسیرپذیری برای مدل‌های یادگیری عمیق توسعه یافته‌اند تا پیش‌بینی‌های شبکه عصبی عمیق جعبه‌سیاه را توضیح دهند. یکی از خانواده‌های روش‌های تفسیر، روش‌های نقشه حساسیت هستند که به دنبال توضیح پیش‌بینی مدل آموزش‌دیده بر روی یک ورودی خاص با اختصاص یک امتیاز اهمیت عددی به هر یک از ویژگی‌های ورودی یا پیکسل‌ها هستند. اگر ورودی مدل یک تصویر باشد، نقشه حساسیت حاصل می‌تواند به صورت یک نقشه حرارتی دو بعدی با همان اندازه تصویر نمایش داده شود که در آن پیکسل‌های مهم‌تر تصویر روشن‌تر از سایر پیکسل‌ها هستند. روش‌های نقشه حساسیت اهمیت هر ویژگی ورودی را به عنوان اثر تغییر آن بر پیش‌بینی مدل محاسبه می‌کنند. اگر پیکسل مهم نباشد، تغییر باید کوچک باشد و بالعکس.
    </p>
    <p id="Par28">
     توسط Baehrens و همکاران معرفی شد و توسط Simonyan و همکاران به شبکه‌های عصبی عمیق اعمال شد، ساده‌ترین راه برای محاسبه چنین امتیازی این است که یک تقریب خطی مرتبه اول از مدل با گرفتن گرادیان خروجی نسبت به ورودی داشته باشیم؛ وزن‌های مدل خطی حاصل حساسیت خروجی به تغییر ویژگی‌های مربوطه (پیکسل‌ها) هستند. به طور رسمی‌تر، با توجه به
     <inline-formula id="IEq18">
      <alternatives>
       <math id="IEq18_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         d
        </mi>
       </math>
       <tex-math id="IEq18_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq18.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     -بعدی
     <inline-formula id="IEq19">
      <alternatives>
       <math id="IEq19_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <msub>
          <mrow>
           <mi mathvariant="bold">
            x
           </mi>
          </mrow>
          <mrow>
           <mi>
            t
           </mi>
          </mrow>
         </msub>
         <mo>
          ∈
         </mo>
         <msup>
          <mrow>
           <mi mathvariant="double-struck">
            R
           </mi>
          </mrow>
          <mrow>
           <mi>
            d
           </mi>
          </mrow>
         </msup>
        </mrow>
       </math>
       <tex-math id="IEq19_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\bf{x}}}_{t}\in {{\mathbb{R}}}^{d}$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq19.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     و تابع پیش‌بینی مدل
     <inline-formula id="IEq20">
      <alternatives>
       <math id="IEq20_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mi>
          f
         </mi>
         <mrow>
          <mo>
           (
          </mo>
          <mrow>
           <mo>
            .
           </mo>
          </mrow>
          <mo>
           )
          </mo>
         </mrow>
        </mrow>
       </math>
       <tex-math id="IEq20_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f(.)$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq20.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     ، امتیاز اهمیت ویژگی
     <inline-formula id="IEq21">
      <alternatives>
       <math id="IEq21_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         j
        </mi>
       </math>
       <tex-math id="IEq21_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$j$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq21.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     ام است
     <inline-formula id="IEq22">
      <alternatives>
       <math id="IEq22_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mo>
          ∣
         </mo>
         <msub>
          <mrow>
           <mo>
            ∇
           </mo>
          </mrow>
          <mrow>
           <mi mathvariant="bold">
            x
           </mi>
          </mrow>
         </msub>
         <mi>
          f
         </mi>
         <msub>
          <mrow>
           <mrow>
            <mo>
             (
            </mo>
            <mrow>
             <msub>
              <mrow>
               <mi mathvariant="bold">
                x
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
              </mrow>
             </msub>
            </mrow>
            <mo>
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
           <mi>
            j
           </mi>
          </mrow>
         </msub>
         <mo>
          ∣
         </mo>
        </mrow>
       </math>
       <tex-math id="IEq22_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$| {\nabla }_{{\bf{x}}}f{({{\bf{x}}}_{t})}_{j}|$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq22.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     <sup>
      <xref ref-type="bibr" rid="CR51">
       51
      </xref>
      <xref ref-type="bibr" rid="CR52">
       52
      </xref>
     </sup>
     . گسترش‌های بیشتری به این روش گرادیان معرفی شدند تا تفسیرهای بهتری از مدل به دست آورند و نقشه‌های حساسیتی تولید کنند که از نظر ادراکی برای کاربران انسانی آسان‌تر قابل درک باشند: LRP، DeepLIFT، گرادیان‌های یکپارچه و غیره. این روش‌های نقشه حساسیت، با این حال، از نویز بصری و حساسیت به تغییرات ورودی رنج می‌برند. روش SmoothGrad هر دو مشکل را با افزودن نویز سفید به تصویر و سپس گرفتن میانگین نقشه‌های حساسیت حاصل کاهش می‌دهد. در این کار، ما از SmoothGrad با روش گرادیان ساده به دلیل کارایی محاسباتی آن استفاده می‌کنیم. سایر روش‌های تفسیر از جمله گرادیان‌های یکپارچه آزمایش شدند اما منجر به تجسم‌های بهتری نشدند
     <sup>
      <xref ref-type="bibr" rid="CR53">
       53
      </xref>
      <xref ref-type="bibr" rid="CR54">
       54
      </xref>
      <xref ref-type="bibr" rid="CR55">
       55
      </xref>
      <xref ref-type="bibr" rid="CR25">
       25
      </xref>
      <xref ref-type="bibr" rid="CR56">
       56
      </xref>
      <xref ref-type="bibr" rid="CR25">
       25
      </xref>
      <xref ref-type="bibr" rid="CR57">
       57
      </xref>
     </sup>
     .
    </p>
   </sec>
   <sec id="Sec13">
    <title>
     درس‌هایی از آموزش مدل و آزمایش‌ها
    </title>
    <p id="Par29">
     عملکرد EchoNet با تلاش برای افزایش اندازه داده‌ها، همگن‌سازی داده‌های ورودی و بهینه‌سازی آموزش مدل با جستجوی ابرپارامتر به طور قابل توجهی بهبود یافت. تجربه ما نشان می‌دهد که افزایش تعداد بیماران منحصر به فرد در مجموعه آموزشی می‌تواند مدل را به طور قابل توجهی بهبود بخشد، بیشتر از افزایش نرخ نمونه‌برداری فریم‌ها از همان بیماران. همگن‌سازی تصاویر ورودی با انتخاب دیدگاه قلبی قبل از آموزش مدل به طور قابل توجهی سرعت آموزش را بهبود بخشید و زمان محاسباتی را بدون از دست دادن قابل توجهی در عملکرد مدل کاهش داد. در نهایت، ما دریافتیم که نتایج می‌توانند با انتخاب دقیق ابرپارامترها به طور قابل توجهی بهبود یابند؛ بین ۷-۹% در معیار AUC برای وظایف طبقه‌بندی و ۳-۱۰% در
     <inline-formula id="IEq23">
      <alternatives>
       <math id="IEq23_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           R
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
       </math>
       <tex-math id="IEq23_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}^{2}$$\end{document}
       </tex-math>
       <inline-graphic href="41746_2019_216_Article_IEq23.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     امتیاز برای وظایف رگرسیون.
    </p>
   </sec>
   <sec id="Sec14">
    <title>
     خلاصه گزارش
    </title>
    <p id="Par30">
     اطلاعات بیشتر در مورد طراحی تحقیق در
     <xref ref-type="supplementary-material" rid="MOESM2">
      خلاصه گزارش تحقیق Nature
     </xref>
     مرتبط با این مقاله موجود است.
    </p>
   </sec>
  </sec>
 </body>
 <back>
  <ack>
   <title>
    قدردانی
   </title>
   <p>
    این کار توسط کمک‌هزینه پایلوت تحقیقات ترجمه‌ای و پزشکی کاربردی استنفورد و کمک‌هزینه بذر مرکز هوش مصنوعی در تصویربرداری و پزشکی استنفورد حمایت می‌شود. D.O. توسط بورسیه تحقیقاتی کالج آمریکایی قلب و عروق/مرک حمایت می‌شود. A.G. توسط بورسیه تحصیلات تکمیلی استنفورد-رابرت بوش در علوم و مهندسی حمایت می‌شود. J.Y.Z. توسط NSF CCF 1763191، NIH R21 MD012867-01، NIH P30AG059307، و کمک‌هزینه‌هایی از بنیاد سیلیکون ولی و ابتکار چان-زاکربرگ حمایت می‌شود.
   </p>
  </ack>
  <sec sec-type="author-contribution">
   <title>
    مشارکت نویسندگان
   </title>
   <p>
    مفهوم و طراحی اولیه مطالعه: D.O. کسب داده‌ها: D.O., D.H.L. آموزش مدل: A.G., J.Y.Z. تحلیل و تفسیر داده‌ها: A.G., D.O., E.A.A., و J.Y.Z. نگارش مقاله: A.G., D.O. بازبینی انتقادی مقاله برای محتوای فکری مهم: A.G., D.O., A.A., B.H., J.H.C., R.A.H., D.H.L., E.A.A. و J.Y.Z. تحلیل آماری: A.G., D.O., E.A.A., J.Y.Z.
   </p>
  </sec>
  <sec sec-type="data-availability">
   <title>
    دسترسی به داده‌ها
   </title>
   <p>
    The data comes from medical records and imaging from Stanford Healthcare and is not publicly available. The de-identified data is available from the authors upon reasonable request and with permission of the institutional review board.
   </p>
  </sec>
  <sec sec-type="data-availability">
   <title>
    دسترسی به کد
   </title>
   <p>
    The code is freely available at
    <ext-link ext-link-type="uri" xlink:href="https://github.com/amiratag/EchoNet">
     https://github.com/amiratag/EchoNet
    </ext-link>
    .
   </p>
  </sec>
  <sec sec-type="ethics-statement">
   <sec id="FPar1" sec-type="COI-statement">
    <title>
     منافع متعارض
    </title>
    <p id="Par31">
     The authors declare no competing interests.
    </p>
   </sec>
  </sec>
  <ref-list id="Bib1">
   <title>
    مراجع
   </title>
   <ref-list>
    <ref id="CR1">
     <label>
      1.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Heidenreich
        </surname>
        <given-names>
         P
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Forecasting the future of cardiovascular disease in the united states: a policy statement from the american heart association
      </article-title>
      <source>
       Circulation
      </source>
      <year>
       2011
      </year>
      <volume>
       123
      </volume>
      <fpage>
       933
      </fpage>
      <lpage>
       944
      </lpage>
      <pub-id pub-id-type="doi">
       10.1161/CIR.0b013e31820a55f5
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR2">
     <label>
      2.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Cohen
        </surname>
        <given-names>
         M
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Racial and ethnic differences in the treatment of acute myocardial infarction: findings from the get with the guidelines-coronary artery disease program
      </article-title>
      <source>
       Circulation
      </source>
      <year>
       2010
      </year>
      <volume>
       121
      </volume>
      <fpage>
       2294
      </fpage>
      <lpage>
       2301
      </lpage>
      <pub-id pub-id-type="doi">
       10.1161/CIRCULATIONAHA.109.922286
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR3">
     <label>
      3.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Havranek
        </surname>
        <given-names>
         E
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Social determinants of risk and outcomes of cardiovascular disease a scientific statement from the american heart association
      </article-title>
      <source>
       Circulation
      </source>
      <year>
       2015
      </year>
      <volume>
       132
      </volume>
      <fpage>
       873
      </fpage>
      <lpage>
       898
      </lpage>
      <pub-id pub-id-type="doi">
       10.1161/CIR.0000000000000228
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR4">
     <label>
      4.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Madani
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <name>
        <surname>
         Ong
        </surname>
        <given-names>
         JR
        </given-names>
       </name>
       <name>
        <surname>
         Tiberwal
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <name>
        <surname>
         Mofrad
        </surname>
        <given-names>
         MR
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       US hospital use of echocardiography: Insights from the nationwide inpatient sample
      </article-title>
      <source>
       J. Am. Coll. Cardiol.
      </source>
      <year>
       2016
      </year>
      <volume>
       67
      </volume>
      <fpage>
       502
      </fpage>
      <lpage>
       511
      </lpage>
     </mixed-citation>
    </ref>
    <ref id="CR5">
     <label>
      5.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Zhang
        </surname>
        <given-names>
         J
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Fully automated echocardiogram interpretation in clinical practice: feasibility and diagnostic accuracy
      </article-title>
      <source>
       Circulation
      </source>
      <year>
       2018
      </year>
      <volume>
       138
      </volume>
      <fpage>
       1623
      </fpage>
      <lpage>
       1635
      </lpage>
      <pub-id pub-id-type="doi">
       10.1161/CIRCULATIONAHA.118.034338
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR6">
     <label>
      6.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Madani
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <name>
        <surname>
         Ong
        </surname>
        <given-names>
         JR
        </given-names>
       </name>
       <name>
        <surname>
         Tiberwal
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <name>
        <surname>
         Mofrad
        </surname>
        <given-names>
         MR
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Deep echocardiography: data-efficient supervised and semisupervised deep learning towards automated diagnosis of cardiac disease
      </article-title>
      <source>
       npj Digital Med.
      </source>
      <year>
       2018
      </year>
      <volume>
       1
      </volume>
      <pub-id pub-id-type="doi">
       10.1038/s41746-018-0065-x
      </pub-id>
      <elocation-id>
       59
      </elocation-id>
     </mixed-citation>
    </ref>
    <ref id="CR7">
     <label>
      7.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Chen
        </surname>
        <given-names>
         JH
        </given-names>
       </name>
       <name>
        <surname>
         Asch
        </surname>
        <given-names>
         SM
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Machine learning and prediction in medicine-beyond the peak of inflated expectations
      </article-title>
      <source>
       N. Engl. J. Med.
      </source>
      <year>
       2017
      </year>
      <volume>
       376
      </volume>
      <fpage>
       2507
      </fpage>
      <pub-id pub-id-type="doi">
       10.1056/NEJMp1702071
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR8">
     <label>
      8.
     </label>
     <mixed-citation publication-type="other">
      Dong, C., Loy, C.C., He, K. &amp; Tang, X. Learning a deep convolutional network for image super-resolution. in
      <italic>
       European conference on computer vision
      </italic>
      , 184–199 (Springer, 2014).
     </mixed-citation>
    </ref>
    <ref id="CR9">
     <label>
      9.
     </label>
     <mixed-citation publication-type="other">
      Russakovsky, O. et al. Imagenet large scale visual recognition challenge.
      <italic>
       Int. j. comp. vis
      </italic>
      .
      <bold>
       115
      </bold>
      , 211–252 (2015).
     </mixed-citation>
    </ref>
    <ref id="CR10">
     <label>
      10.
     </label>
     <mixed-citation publication-type="other">
      Szegedy, C., Ioffe, S., Vanhoucke, V. &amp; Alemi, A.A., Inception-v4, inception-resnet and the impact of residual connections on learning. In
      <italic>
       Thirty-First AAAI Conference on Artificial Intelligence
      </italic>
      (AAAI.org, 2017).
     </mixed-citation>
    </ref>
    <ref id="CR11">
     <label>
      11.
     </label>
     <mixed-citation publication-type="other">
      Karpathy, A. et al. Large-scale video classification with convolutional neural networks. In
      <italic>
       Proc. of the IEEE conference on Computer Vision and Pattern Recognition
      </italic>
      , 1725–1732 (IEEE, 2014).
     </mixed-citation>
    </ref>
    <ref id="CR12">
     <label>
      12.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Poplin
        </surname>
        <given-names>
         R
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning
      </article-title>
      <source>
       Nat. Biomed. Eng.
      </source>
      <year>
       2018
      </year>
      <volume>
       2
      </volume>
      <fpage>
       158
      </fpage>
      <pub-id pub-id-type="doi">
       10.1038/s41551-018-0195-0
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR13">
     <label>
      13.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Esteva
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Dermatologist-level classification of skin cancer with deep neural networks
      </article-title>
      <source>
       Nature
      </source>
      <year>
       2017
      </year>
      <volume>
       542
      </volume>
      <fpage>
       115
      </fpage>
      <pub-id assigning-authority="ChemPort ( Chemical Abstract Service )" pub-id-type="other">
       1:CAS:528:DC%2BC2sXhsFGltrY%3D
      </pub-id>
      <pub-id pub-id-type="doi">
       10.1038/nature21056
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR14">
     <label>
      14.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Coudray
        </surname>
        <given-names>
         N
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Classification and mutation prediction from non-small cell lung cancer histopathology images using deep learning
      </article-title>
      <source>
       Nat. Med.
      </source>
      <year>
       2018
      </year>
      <volume>
       24
      </volume>
      <fpage>
       1559
      </fpage>
      <pub-id assigning-authority="ChemPort ( Chemical Abstract Service )" pub-id-type="other">
       1:CAS:528:DC%2BC1cXhslCjtrbJ
      </pub-id>
      <pub-id pub-id-type="doi">
       10.1038/s41591-018-0177-5
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR15">
     <label>
      15.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Ounkomol
        </surname>
        <given-names>
         C
        </given-names>
       </name>
       <name>
        <surname>
         Seshamani
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <name>
        <surname>
         Maleckar
        </surname>
        <given-names>
         MM
        </given-names>
       </name>
       <name>
        <surname>
         Collman
        </surname>
        <given-names>
         F
        </given-names>
       </name>
       <name>
        <surname>
         Johnson
        </surname>
        <given-names>
         GR
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Label-free prediction of three-dimensional fluorescence images from transmitted-light microscopy
      </article-title>
      <source>
       Nat. Methods
      </source>
      <year>
       2018
      </year>
      <volume>
       15
      </volume>
      <fpage>
       917
      </fpage>
      <pub-id assigning-authority="ChemPort ( Chemical Abstract Service )" pub-id-type="other">
       1:CAS:528:DC%2BC1cXhslCjt7%2FN
      </pub-id>
      <pub-id pub-id-type="doi">
       10.1038/s41592-018-0111-2
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR16">
     <label>
      16.
     </label>
     <mixed-citation publication-type="other">
      Nagpal, K. et al. Development and validation of a deep learning algorithm for improving Gleason scoring of prostate cancer.
      <italic>
       npj Digit. Med.
      </italic>
      <bold>
       2
      </bold>
      , 48 (2019).
     </mixed-citation>
    </ref>
    <ref id="CR17">
     <label>
      17.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Douglas
        </surname>
        <given-names>
         P
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Accf/ase/aha/asnc/hfsa/hrs/scai/sccm/scct/scmr 2011 appropriate use criteria for echocardiography
      </article-title>
      <source>
       J. Am. Soc. Echocardiogr.
      </source>
      <year>
       2011
      </year>
      <volume>
       24
      </volume>
      <fpage>
       229
      </fpage>
      <lpage>
       267
      </lpage>
      <pub-id pub-id-type="doi">
       10.1016/j.echo.2010.12.008
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR18">
     <label>
      18.
     </label>
     <mixed-citation publication-type="other">
      Wood, P.W., Choy, J.B., Nanda, N.C. &amp; Becher, H. Left ventricular ejection fraction and volumes: it depends on the imaging method.
      <italic>
       Echocardiography
      </italic>
      <bold>
       31
      </bold>
      , 87–100 (2014).
     </mixed-citation>
    </ref>
    <ref id="CR19">
     <label>
      19.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Geer
        </surname>
        <given-names>
         DD
        </given-names>
       </name>
       <name>
        <surname>
         Oscarsson
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <name>
        <surname>
         Engvall
        </surname>
        <given-names>
         J
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Variability in echocardiographic measurements of left ventricular function in septic shock patients
      </article-title>
      <source>
       J. Cardiovasc Ultrasound.
      </source>
      <year>
       2015
      </year>
      <volume>
       13
      </volume>
      <fpage>
       19
      </fpage>
      <pub-id pub-id-type="doi">
       10.1186/s12947-015-0015-6
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR20">
     <label>
      20.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         JA
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <name>
        <surname>
         JM
        </surname>
        <given-names>
         G-S
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Echocardiographic variables used to estimate pulmonary artery pressure in dogs
      </article-title>
      <source>
       J. Vet. Intern. Med.
      </source>
      <year>
       2017
      </year>
      <volume>
       31
      </volume>
      <fpage>
       1622
      </fpage>
      <lpage>
       1628
      </lpage>
      <pub-id pub-id-type="doi">
       10.1111/jvim.14846
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR21">
     <label>
      21.
     </label>
     <mixed-citation publication-type="other">
      2019 ACC/AHA/ASE advanced training statement on echocardiography (Revision of the 2003 ACC/AHA Clinical Competence Statement on Echocardiography): a report of the ACC competency management committee.
      <italic>
       J. Am. Coll. Cardiol
      </italic>
      .
      <bold>
       19
      </bold>
      , S0735–S1097 (2019)
     </mixed-citation>
    </ref>
    <ref id="CR22">
     <label>
      22.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         MK
        </surname>
        <given-names>
         F
        </given-names>
       </name>
       <name>
        <surname>
         WS
        </surname>
        <given-names>
         B
        </given-names>
       </name>
       <name>
        <surname>
         DN
        </surname>
        <given-names>
         W
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Systematic review: prediction of perioperative cardiac complications and mortality by the revised cardiac risk index
      </article-title>
      <source>
       Ann. Intern. Med.
      </source>
      <year>
       2010
      </year>
      <volume>
       152
      </volume>
      <fpage>
       26
      </fpage>
      <lpage>
       35
      </lpage>
      <pub-id pub-id-type="doi">
       10.7326/0003-4819-152-1-201001050-00007
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR23">
     <label>
      23.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Abdel-Qadir
        </surname>
        <given-names>
         H
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       A population-based study of cardiovascular mortality following early-stage breast cancer
      </article-title>
      <source>
       JAMA Cardiol.
      </source>
      <year>
       2017
      </year>
      <volume>
       2
      </volume>
      <fpage>
       88
      </fpage>
      <lpage>
       93
      </lpage>
      <pub-id pub-id-type="doi">
       10.1001/jamacardio.2016.3841
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR24">
     <label>
      24.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Madani
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <name>
        <surname>
         Arnaout
        </surname>
        <given-names>
         R
        </given-names>
       </name>
       <name>
        <surname>
         Mofrad
        </surname>
        <given-names>
         M
        </given-names>
       </name>
       <name>
        <surname>
         Arnaout
        </surname>
        <given-names>
         R
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Fast and accurate view classification of echocardiograms using deep learning
      </article-title>
      <source>
       npj Digital Med.
      </source>
      <year>
       2018
      </year>
      <volume>
       1
      </volume>
      <pub-id pub-id-type="doi">
       10.1038/s41746-017-0013-1
      </pub-id>
      <elocation-id>
       6
      </elocation-id>
     </mixed-citation>
    </ref>
    <ref id="CR25">
     <label>
      25.
     </label>
     <mixed-citation publication-type="other">
      Smilkov, D., Thorat, N., Kim, B., Viégas, F. &amp; Wattenberg, M. Smoothgrad: removing noise by adding noise.
      <italic>
       arXiv preprint arXiv:1706.03825
      </italic>
      (2017).
     </mixed-citation>
    </ref>
    <ref id="CR26">
     <label>
      26.
     </label>
     <mixed-citation publication-type="other">
      Abid, A. et al. Gradio: Hassle-free sharing and testing of ml models in the wild. in
      <italic>
       Proc. 36th International Conference on Machine Learning,
      </italic>
      Vol. 72 (JMLR.org, 2019).
     </mixed-citation>
    </ref>
    <ref id="CR27">
     <label>
      27.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Kou
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Echocardiographic reference ranges for normal cardiac chamber size: results from the norre study
      </article-title>
      <source>
       Eur. Heart J. Cardiovasc. Imaging
      </source>
      <year>
       2014
      </year>
      <volume>
       15
      </volume>
      <fpage>
       680
      </fpage>
      <lpage>
       690
      </lpage>
      <pub-id pub-id-type="doi">
       10.1093/ehjci/jet284
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR28">
     <label>
      28.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Pfaffenberger
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Size matters! Impact of age, sex, height, and weight on the normal heart size
      </article-title>
      <source>
       Circ. Cardiovasc. Imaging
      </source>
      <year>
       2013
      </year>
      <volume>
       6
      </volume>
      <fpage>
       1073
      </fpage>
      <lpage>
       1079
      </lpage>
      <pub-id pub-id-type="doi">
       10.1161/CIRCIMAGING.113.000690
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR29">
     <label>
      29.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Attia
        </surname>
        <given-names>
         Z
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Age and sex estimation using artificial intelligence from standard 12-lead ecgs
      </article-title>
      <source>
       Circ.: Arrhythm. Electrophysiol.
      </source>
      <year>
       2019
      </year>
      <volume>
       12
      </volume>
      <elocation-id>
       e007284
      </elocation-id>
     </mixed-citation>
    </ref>
    <ref id="CR30">
     <label>
      30.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Munagala
        </surname>
        <given-names>
         V
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Association of newer diastolic function parameters with age in healthy subjects: a population-based study
      </article-title>
      <source>
       J. Am. Soc. Echocardiogr.
      </source>
      <year>
       2003
      </year>
      <volume>
       16
      </volume>
      <fpage>
       1049
      </fpage>
      <lpage>
       1056
      </lpage>
      <pub-id pub-id-type="doi">
       10.1016/S0894-7317(03)00516-9
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR31">
     <label>
      31.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         D’Andrea
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Left atrial volume index in healthy subjects: clinical and echocardiographic correlates
      </article-title>
      <source>
       Echocardiography
      </source>
      <year>
       2013
      </year>
      <volume>
       30
      </volume>
      <fpage>
       1001
      </fpage>
      <lpage>
       1007
      </lpage>
      <pub-id pub-id-type="pmid">
       23594028
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR32">
     <label>
      32.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Bhaskaran
        </surname>
        <given-names>
         K
        </given-names>
       </name>
       <name>
        <surname>
         dos Santos Silva
        </surname>
        <given-names>
         I
        </given-names>
       </name>
       <name>
        <surname>
         Leon
        </surname>
        <given-names>
         DA
        </given-names>
       </name>
       <name>
        <surname>
         Douglas
        </surname>
        <given-names>
         IJ
        </given-names>
       </name>
       <name>
        <surname>
         Smeeth
        </surname>
        <given-names>
         L
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Body-mass index and mortality among 1.46 million white adults
      </article-title>
      <source>
       N. Engl. J. Med.
      </source>
      <year>
       2010
      </year>
      <volume>
       363
      </volume>
      <fpage>
       2211
      </fpage>
      <lpage>
       2219
      </lpage>
      <pub-id pub-id-type="doi">
       10.1056/NEJMoa1000367
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR33">
     <label>
      33.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         de Gonzalez A
        </surname>
        <given-names>
         B
        </given-names>
       </name>
       <name>
        <surname>
         P
        </surname>
        <given-names>
         H
        </given-names>
       </name>
       <name>
        <surname>
         JR
        </surname>
        <given-names>
         C
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Association of BMI with overall and cause-specific mortality: a population-based cohort study of 3.6 million adults in the UK
      </article-title>
      <source>
       Lancet Diabetes Endocrinol.
      </source>
      <year>
       2018
      </year>
      <volume>
       6
      </volume>
      <fpage>
       944
      </fpage>
      <lpage>
       953
      </lpage>
      <pub-id pub-id-type="doi">
       10.1016/S2213-8587(18)30288-2
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR34">
     <label>
      34.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Xu
        </surname>
        <given-names>
         H
        </given-names>
       </name>
       <name>
        <surname>
         Cupples
        </surname>
        <given-names>
         LA
        </given-names>
       </name>
       <name>
        <surname>
         Stokes
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <name>
        <surname>
         Liu
        </surname>
        <given-names>
         CT
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Association of obesity with mortality over 24 years of weight history findings from the framingham heart study
      </article-title>
      <source>
       JAMA Netw. Open
      </source>
      <year>
       2018
      </year>
      <volume>
       1
      </volume>
      <pub-id pub-id-type="doi">
       10.1001/jamanetworkopen.2018.4587
      </pub-id>
      <elocation-id>
       e184587
      </elocation-id>
     </mixed-citation>
    </ref>
    <ref id="CR35">
     <label>
      35.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Madu
        </surname>
        <given-names>
         EC
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Transesophageal dobutamine stress echocardiography in the evaluation of myocardial ischemia in morbidly obese subjects
      </article-title>
      <source>
       Chest.
      </source>
      <year>
       2000
      </year>
      <volume>
       117
      </volume>
      <fpage>
       657
      </fpage>
      <lpage>
       661
      </lpage>
      <pub-id assigning-authority="ChemPort ( Chemical Abstract Service )" pub-id-type="other">
       1:STN:280:DC%2BD3c7nvFGnsQ%3D%3D
      </pub-id>
      <pub-id pub-id-type="doi">
       10.1378/chest.117.3.657
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR36">
     <label>
      36.
     </label>
     <mixed-citation publication-type="other">
      Medical Advisory Secretariat. Use of contrast agents with echocardiography in patients with suboptimal echocardiography.
      <italic>
       Ont. Health Technol. Assess. Ser
      </italic>
      .
      <bold>
       10
      </bold>
      , 1–17 (2010).
     </mixed-citation>
    </ref>
    <ref id="CR37">
     <label>
      37.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Kälsch
        </surname>
        <given-names>
         H
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Aortic calcification onset and progression: Association with the development of coronary atherosclerosis
      </article-title>
      <source>
       J Am Heart Assoc.
      </source>
      <year>
       2017
      </year>
      <volume>
       6
      </volume>
      <pub-id pub-id-type="doi">
       10.1161/JAHA.116.005093
      </pub-id>
      <elocation-id>
       e005093
      </elocation-id>
     </mixed-citation>
    </ref>
    <ref id="CR38">
     <label>
      38.
     </label>
     <mixed-citation publication-type="other">
      Eleid, M.F., Foley, T.A., Said, S.M., Pislaru, S.V. &amp; Rihal, C.S. Severe mitral annular calcification: multimodality imaging for therapeutic strategies and interventions.
      <italic>
       JACC: Cardiovas. Imaging
      </italic>
      <bold>
       9
      </bold>
      , 1318–1337 (2016).
     </mixed-citation>
    </ref>
    <ref id="CR39">
     <label>
      39.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Aurigemma
        </surname>
        <given-names>
         G
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Left atrial volume and geometry in healthy aging: the cardiovascular health study
      </article-title>
      <source>
       Circ. Cardiovasc. Imaging
      </source>
      <year>
       2009
      </year>
      <volume>
       2
      </volume>
      <fpage>
       282
      </fpage>
      <lpage>
       289
      </lpage>
      <pub-id pub-id-type="doi">
       10.1161/CIRCIMAGING.108.826602
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR40">
     <label>
      40.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Bello
        </surname>
        <given-names>
         GA
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Deep-learning cardiac motion analysis for human survival prediction
      </article-title>
      <source>
       Nat. Mach. Intell.
      </source>
      <year>
       2019
      </year>
      <volume>
       1
      </volume>
      <fpage>
       95
      </fpage>
      <pub-id pub-id-type="doi">
       10.1038/s42256-019-0019-2
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR41">
     <label>
      41.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Ardila
        </surname>
        <given-names>
         D
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography
      </article-title>
      <source>
       Nat. Med.
      </source>
      <year>
       2019
      </year>
      <volume>
       25
      </volume>
      <fpage>
       954
      </fpage>
      <lpage>
       961
      </lpage>
      <pub-id assigning-authority="ChemPort ( Chemical Abstract Service )" pub-id-type="other">
       1:CAS:528:DC%2BC1MXhtVWqurfO
      </pub-id>
      <pub-id pub-id-type="doi">
       10.1038/s41591-019-0447-x
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR42">
     <label>
      42.
     </label>
     <mixed-citation publication-type="other">
      Virnig, B.A. et al. Trends in the Use of Echocardiography. Echocardiography Trends. Data Points #20 (prepared by the University of Minnesota DEcIDE Center, under Contract No. HHSA29020100013I). Rockville, MD: Agency for Healthcare Research and Quality; May 2014. AHRQ Publication No. 14-EHC034-EF (2007–2011).
     </mixed-citation>
    </ref>
    <ref id="CR43">
     <label>
      43.
     </label>
     <mixed-citation publication-type="other">
      Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. &amp; Wojna, Z. Rethinking the inception architecture for computer vision. In
      <italic>
       Proc. of the IEEE conference on computer vision and pattern recognition
      </italic>
      , 2818–2826 (IEEE, 2016).
     </mixed-citation>
    </ref>
    <ref id="CR44">
     <label>
      44.
     </label>
     <mixed-citation publication-type="other">
      Abadi, M. et al. Tensorflow: A system for large-scale machine learning. In
      <italic>
       12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)
      </italic>
      265–283 (2016).
     </mixed-citation>
    </ref>
    <ref id="CR45">
     <label>
      45.
     </label>
     <mixed-citation publication-type="other">
      Kingma, D.P. &amp; Ba, J. Adam: a method for stochastic optimization. 3rd International Conference on Learning Representations, {ICLR} 2015, (San Diego, CA, USA, 2015) Conference Track Proceedings.
     </mixed-citation>
    </ref>
    <ref id="CR46">
     <label>
      46.
     </label>
     <mixed-citation publication-type="other">
      Krogh, A. &amp; Hertz, J.A. A simple weight decay can improve generalization. In
      <italic>
       Advances in neural information processing systems
      </italic>
      , 950–957 (1992).
     </mixed-citation>
    </ref>
    <ref id="CR47">
     <label>
      47.
     </label>
     <mixed-citation publication-type="other">
      Huber, P.J. Robust estimation of a location parameter. in
      <italic>
       Breakthroughs in statistics
      </italic>
      , 492–518 (Springer, 1992).
     </mixed-citation>
    </ref>
    <ref id="CR48">
     <label>
      48.
     </label>
     <mixed-citation publication-type="other">
      Lin, T.Y., Goyal, P., Girshick, R., He, K. &amp; Dollár, P. Focal loss for dense object detection. In
      <italic>
       Proc. of the IEEE international conference on computer vision,
      </italic>
      2980–2988 (IEEE, 2017).
     </mixed-citation>
    </ref>
    <ref id="CR49">
     <label>
      49.
     </label>
     <mixed-citation publication-type="other">
      Perez, L. &amp; Wang, J. The effectiveness of data augmentation in image classification using deep learning.
      <italic>
       arXiv preprint arXiv:1712.04621
      </italic>
      (2017).
     </mixed-citation>
    </ref>
    <ref id="CR50">
     <label>
      50.
     </label>
     <mixed-citation publication-type="other">
      Lim, S., Kim, I., Kim, T., Kim, C. &amp; Kim, S. Fast AutoAugment In Advances in Neural Information Processing Systems, 6662–6672 (2019).
     </mixed-citation>
    </ref>
    <ref id="CR51">
     <label>
      51.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Baehrens
        </surname>
        <given-names>
         D
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       How to explain individual classification decisions
      </article-title>
      <source>
       Journal of Machine Learning Research
      </source>
      <year>
       2010
      </year>
      <volume>
       11
      </volume>
      <fpage>
       1803
      </fpage>
      <lpage>
       1831
      </lpage>
     </mixed-citation>
    </ref>
    <ref id="CR52">
     <label>
      52.
     </label>
     <mixed-citation publication-type="other">
      Simonyan, K., Vedaldi, A. &amp; Zisserman, A. Deep inside convolutional networks: Visualising image classification models and saliency maps.
      <italic>
       arXiv preprint arXiv:1312.6034
      </italic>
      (2013).
     </mixed-citation>
    </ref>
    <ref id="CR53">
     <label>
      53.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Bach
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation
      </article-title>
      <source>
       PloS ONE
      </source>
      <year>
       2015
      </year>
      <volume>
       10
      </volume>
      <pub-id pub-id-type="doi">
       10.1371/journal.pone.0130140
      </pub-id>
      <elocation-id>
       e0130140
      </elocation-id>
     </mixed-citation>
    </ref>
    <ref id="CR54">
     <label>
      54.
     </label>
     <mixed-citation publication-type="other">
      Shrikumar, A., Greenside, P. &amp; Kundaje, A. Learning important features through propagating activation differences. In
      <italic>
       Proc. of the 34th International Conference on Machine Learning
      </italic>
      , vol. 70, 3145–3153 (JMLR, 2017)
     </mixed-citation>
    </ref>
    <ref id="CR55">
     <label>
      55.
     </label>
     <mixed-citation publication-type="other">
      Sundararajan, M., Taly, A. &amp; Yan, Q. Axiomatic attribution for deep networks. in
      <italic>
       Proc. 34th International Conference on Machine Learning,
      </italic>
      Vol. 70, 3319–3328 (JMLR. org, 2017).
     </mixed-citation>
    </ref>
    <ref id="CR56">
     <label>
      56.
     </label>
     <mixed-citation publication-type="other">
      Ghorbani, A., Abid, A. &amp; Zou, J. Interpretation of neural networks is fragile. In
      <italic>
       Proc. of the AAAI Conference on Artificial Intelligence
      </italic>
      , Vol. 33, 3681–3688 (AAAI.org, 2019).
     </mixed-citation>
    </ref>
    <ref id="CR57">
     <label>
      57.
     </label>
     <mixed-citation publication-type="other">
      Levine, A., Singla, S. &amp; Feizi, S. Certifiably robust interpretation in deep learning.
      <italic>
       arXiv preprint arXiv:1905.12105
      </italic>
      (2019).
     </mixed-citation>
    </ref>
   </ref-list>
  </ref-list>
  <app-group>
   <app id="App1" specific-use="web-only">
    <sec id="Sec15">
     <title>
      اطلاعات تکمیلی
     </title>
     <p id="Par32">
      <supplementary-material content-type="local-data" id="MOESM1" xlink:title="Supplementary information">
       <media mime-subtype="pdf" mimetype="application" xlink:href="MediaObjects/41746_2019_216_MOESM1_ESM.pdf">
        <caption xml:lang="en">
         <p>
          Supplementary Information
         </p>
        </caption>
       </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM2" xlink:title="Supplementary information">
       <media mime-subtype="pdf" mimetype="application" xlink:href="MediaObjects/41746_2019_216_MOESM2_ESM.pdf">
        <caption xml:lang="en">
         <p>
          Reporting Summary
         </p>
        </caption>
       </media>
      </supplementary-material>
     </p>
    </sec>
   </app>
  </app-group>
  <notes notes-type="ESMHint">
   <title>
    اطلاعات تکمیلی
   </title>
   <p>
    <bold>
     Supplementary information
    </bold>
    is available for this paper at
    <ext-link ext-link-type="doi" xlink:href="10.1038/s41746-019-0216-8">
     https://doi.org/10.1038/s41746-019-0216-8
    </ext-link>
    .
   </p>
  </notes>
  <notes notes-type="Misc">
   <p>
    <bold>
     Publisher’s note
    </bold>
    Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
   </p>
  </notes>
 </back>
</article>
