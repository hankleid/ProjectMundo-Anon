<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type='text/xsl' href='/w/ProjectMundo-Anon-106A/style/jats-html.xsl'?>
<!DOCTYPE response>
<article article-type="research-article" dtd-version="1.2" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
 <front>
  <journal-meta>
   <journal-id journal-id-type="publisher-id">
    41598
   </journal-id>
   <journal-id journal-id-type="doi">
    10.1038/41598.2045-2322
   </journal-id>
   <journal-title-group>
    <journal-title>
     Scientific Reports
    </journal-title>
    <abbrev-journal-title abbrev-type="publisher">
     Sci Rep
    </abbrev-journal-title>
   </journal-title-group>
   <issn pub-type="epub">
    2045-2322
   </issn>
   <publisher>
    <publisher-name>
     Nature Publishing Group UK
    </publisher-name>
    <publisher-loc>
     London
    </publisher-loc>
   </publisher>
  </journal-meta>
  <article-meta>
   <article-id pub-id-type="publisher-id">
    s41598-022-23052-9
   </article-id>
   <article-id pub-id-type="manuscript">
    23052
   </article-id>
   <article-id pub-id-type="doi">
    10.1038/s41598-022-23052-9
   </article-id>
   <article-categories>
    <subj-group subj-group-type="heading">
     <subject>
      Article
     </subject>
    </subj-group>
    <subj-group subj-group-type="SubjectPath">
     <subject>
      /639/166
     </subject>
    </subj-group>
    <subj-group subj-group-type="SubjectPath">
     <subject>
      /639/166/987
     </subject>
    </subj-group>
    <subj-group subj-group-type="NatureArticleTypeID">
     <subject>
      article
     </subject>
    </subj-group>
   </article-categories>
   <title-group>
    <article-title xml:lang="en">
     Apprendimento contrastivo di linguaggio e visione di concetti generali di moda
    </article-title>
   </title-group>
   <contrib-group>
    <contrib contrib-type="author" corresp="yes" id="Au1">
     <name name-style="western">
      <surname>
       Chia
      </surname>
      <given-names>
       Patrick John
      </given-names>
     </name>
     <address>
      <email>
       pchia@coveo.com
      </email>
     </address>
     <xref ref-type="aff" rid="Aff1">
      1
     </xref>
     <xref ref-type="corresp" rid="IDs41598022230529_cor1">
      a
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au2">
     <name name-style="western">
      <surname>
       Attanasio
      </surname>
      <given-names>
       Giuseppe
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff2">
      2
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au3">
     <name name-style="western">
      <surname>
       Bianchi
      </surname>
      <given-names>
       Federico
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff3">
      3
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au4">
     <name name-style="western">
      <surname>
       Terragni
      </surname>
      <given-names>
       Silvia
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff4">
      4
     </xref>
     <xref ref-type="aff" rid="Aff7">
      7
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au5">
     <name name-style="western">
      <surname>
       Magalhães
      </surname>
      <given-names>
       Ana Rita
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff5">
      5
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au6">
     <name name-style="western">
      <surname>
       Goncalves
      </surname>
      <given-names>
       Diogo
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff5">
      5
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au7">
     <name name-style="western">
      <surname>
       Greco
      </surname>
      <given-names>
       Ciro
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff6">
      6
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au8">
     <name name-style="western">
      <surname>
       Tagliabue
      </surname>
      <given-names>
       Jacopo
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff6">
      6
     </xref>
    </contrib>
    <aff id="Aff1">
     <label>
      1
     </label>
     <institution-wrap>
      <institution content-type="org-name">
       Coveo
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      Montreal
     </addr-line>
     <country country="CA">
      Canada
     </country>
    </aff>
    <aff id="Aff2">
     <label>
      2
     </label>
     <institution-wrap>
      <institution-id institution-id-type="GRID">
       grid.7945.f
      </institution-id>
      <institution-id institution-id-type="ISNI">
       0000 0001 2165 6939
      </institution-id>
      <institution content-type="org-name">
       Bocconi University
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      Milan
     </addr-line>
     <country country="IT">
      Italy
     </country>
    </aff>
    <aff id="Aff3">
     <label>
      3
     </label>
     <institution-wrap>
      <institution-id institution-id-type="GRID">
       grid.168010.e
      </institution-id>
      <institution-id institution-id-type="ISNI">
       0000000419368956
      </institution-id>
      <institution content-type="org-name">
       Stanford University
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      Stanford
     </addr-line>
     <addr-line content-type="state">
      CA
     </addr-line>
     <country country="US">
      USA
     </country>
    </aff>
    <aff id="Aff4">
     <label>
      4
     </label>
     <institution-wrap>
      <institution content-type="org-name">
       Telepathy Labs
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      Zurich
     </addr-line>
     <country country="CH">
      Switzerland
     </country>
    </aff>
    <aff id="Aff5">
     <label>
      5
     </label>
     <institution-wrap>
      <institution content-type="org-name">
       Farfetch
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      Porto
     </addr-line>
     <country country="PT">
      Portugal
     </country>
    </aff>
    <aff id="Aff6">
     <label>
      6
     </label>
     <institution-wrap>
      <institution content-type="org-name">
       South Park Commons
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      New York
     </addr-line>
     <country country="US">
      USA
     </country>
    </aff>
    <aff id="Aff7">
     <label>
      7
     </label>
     <institution-wrap>
      <institution-id institution-id-type="GRID">
       grid.7563.7
      </institution-id>
      <institution-id institution-id-type="ISNI">
       0000 0001 2174 1754
      </institution-id>
      <institution content-type="org-name">
       University of Milano-Bicocca
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      Milan
     </addr-line>
     <country country="IT">
      Italy
     </country>
    </aff>
   </contrib-group>
   <author-notes>
    <corresp id="IDs41598022230529_cor1">
     <label>
      a
     </label>
     <email>
      pchia@coveo.com
     </email>
    </corresp>
   </author-notes>
   <pub-date date-type="pub" publication-format="electronic">
    <day>
     8
    </day>
    <month>
     11
    </month>
    <year>
     2022
    </year>
   </pub-date>
   <pub-date date-type="collection" publication-format="electronic">
    <month>
     12
    </month>
    <year>
     2022
    </year>
   </pub-date>
   <volume>
    12
   </volume>
   <issue seq="18958">
    1
   </issue>
   <elocation-id>
    18958
   </elocation-id>
   <history>
    <date date-type="registration">
     <day>
      25
     </day>
     <month>
      10
     </month>
     <year>
      2022
     </year>
    </date>
    <date date-type="received">
     <day>
      26
     </day>
     <month>
      5
     </month>
     <year>
      2022
     </year>
    </date>
    <date date-type="accepted">
     <day>
      25
     </day>
     <month>
      10
     </month>
     <year>
      2022
     </year>
    </date>
    <date date-type="online">
     <day>
      8
     </day>
     <month>
      11
     </month>
     <year>
      2022
     </year>
    </date>
   </history>
   <pub-history>
    <event event-type="Correction">
     <event-desc>
      A Correction to this paper has been published: https://doi.org/10.1038/s41598-022-26364-y
     </event-desc>
     <date>
      <day>
       23
      </day>
      <month>
       1
      </month>
      <year>
       2023
      </year>
     </date>
    </event>
   </pub-history>
   <permissions>
    <copyright-statement content-type="compact">
     © The Author(s) 2022
    </copyright-statement>
    <copyright-year>
     2022
    </copyright-year>
    <copyright-holder>
     The Author(s)
    </copyright-holder>
    <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/">
     <license-p>
      <bold>
       Open Access
      </bold>
      This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit
      <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">
       http://creativecommons.org/licenses/by/4.0/
      </ext-link>
      .
     </license-p>
    </license>
   </permissions>
   <related-article ext-link-type="doi" related-article-type="correction-forward" xlink:href="10.1038/s41598-022-26364-y"/>
   <abstract id="Abs1" xml:lang="en">
    <title>
     Abstract
    </title>
    <p id="Par1">
     La crescita costante dello shopping online va di pari passo con lo sviluppo di modelli di ML e NLP sempre più complessi. Mentre la maggior parte dei casi d'uso sono considerati come problemi di apprendimento supervisionato specializzato, sosteniamo che i professionisti trarrebbero grande beneficio da rappresentazioni generali e trasferibili dei prodotti. In
     <italic>
      questo
     </italic>
     lavoro, ci basiamo sugli sviluppi recenti nell'apprendimento contrastivo per addestrare
     <italic>
      FashionCLIP
     </italic>
     , un modello simile a
     <italic>
      CLIP
     </italic>
     adattato per l'industria della moda. Dimostriamo l'efficacia delle rappresentazioni apprese da
     <italic>
      FashionCLIP
     </italic>
     con test estensivi su una varietà di compiti, dataset e prove di generalizzazione. Sosteniamo che le adattazioni di grandi modelli pre-addestrati come CLIP offrono nuove prospettive in termini di scalabilità e sostenibilità per certi tipi di attori nell'industria. Infine, dettagliamo i costi e l'impatto ambientale dell'addestramento e rilasciamo i pesi del modello e il codice come contributo open source alla comunità.
    </p>
   </abstract>
   <custom-meta-group>
    <custom-meta>
     <meta-name>
      publisher-imprint-name
     </meta-name>
     <meta-value>
      Nature Portfolio
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      volume-issue-count
     </meta-name>
     <meta-value>
      1
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-article-count
     </meta-name>
     <meta-value>
      22654
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-toc-levels
     </meta-name>
     <meta-value>
      0
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-pricelist-year
     </meta-name>
     <meta-value>
      2022
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-copyright-holder
     </meta-name>
     <meta-value>
      The Author(s)
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-copyright-year
     </meta-name>
     <meta-value>
      2022
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-contains-esm
     </meta-name>
     <meta-value>
      No
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-numbering-style
     </meta-name>
     <meta-value>
      Unnumbered
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-registration-date-year
     </meta-name>
     <meta-value>
      2022
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-registration-date-month
     </meta-name>
     <meta-value>
      10
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-registration-date-day
     </meta-name>
     <meta-value>
      25
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-toc-levels
     </meta-name>
     <meta-value>
      0
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      toc-levels
     </meta-name>
     <meta-value>
      0
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      volume-type
     </meta-name>
     <meta-value>
      Regular
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-product
     </meta-name>
     <meta-value>
      NonStandardArchiveJournal
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      numbering-style
     </meta-name>
     <meta-value>
      Unnumbered
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-grants-type
     </meta-name>
     <meta-value>
      OpenChoice
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      metadata-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      abstract-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      bodypdf-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      bodyhtml-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      bibliography-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      esm-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      online-first
     </meta-name>
     <meta-value>
      false
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      pdf-file-reference
     </meta-name>
     <meta-value>
      BodyRef/PDF/41598_2022_Article_23052.pdf
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      pdf-type
     </meta-name>
     <meta-value>
      Typeset
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      target-type
     </meta-name>
     <meta-value>
      OnlinePDF
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-online-date-year
     </meta-name>
     <meta-value>
      2023
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-online-date-month
     </meta-name>
     <meta-value>
      1
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-online-date-day
     </meta-name>
     <meta-value>
      1
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-type
     </meta-name>
     <meta-value>
      Regular
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-type
     </meta-name>
     <meta-value>
      OriginalPaper
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-primary
     </meta-name>
     <meta-value>
      Science, Humanities and Social Sciences, multidisciplinary
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-secondary
     </meta-name>
     <meta-value>
      Science, Humanities and Social Sciences, multidisciplinary
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-secondary
     </meta-name>
     <meta-value>
      Science, multidisciplinary
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-collection
     </meta-name>
     <meta-value>
      Science (multidisciplinary)
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      open-access
     </meta-name>
     <meta-value>
      true
     </meta-value>
    </custom-meta>
   </custom-meta-group>
  </article-meta>
  <notes notes-type="CrossLinking">
   <sec>
    <p>
     A correction to this article is available online at
     <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-022-26364-y">
      https://doi.org/10.1038/s41598-022-26364-y
     </ext-link>
     .
    </p>
   </sec>
  </notes>
 </front>
 <body>
  <sec id="Sec1">
   <title>
    Introduzione
   </title>
   <sec id="Sec2">
    <title>
     Generalizzazione e scalabilità nell'apprendimento automatico
    </title>
    <p id="Par2">
     La straordinaria crescita del commercio al dettaglio online—nel 2020, 4 trilioni di dollari all'anno—ha profondamente impattato l'industria della moda, con 1 transazione su 4 che ora avviene online
     <sup>
      <xref ref-type="bibr" rid="CR1">
       1
      </xref>
      <xref ref-type="bibr" rid="CR2">
       2
      </xref>
     </sup>
     . La combinazione di grandi quantità di dati e una varietà di casi d'uso ha reso l'e-commerce fertile per modelli di apprendimento automatico (ML) all'avanguardia, con il Natural Language Processing (NLP) coinvolto in raccomandazioni, recupero di informazioni (IR), classificazione dei prodotti e molti altri casi d'uso
     <sup>
      <xref ref-type="bibr" rid="CR3">
       3
      </xref>
      ,
      <xref ref-type="bibr" rid="CR4">
       4
      </xref>
      ,
      <xref ref-type="bibr" rid="CR5">
       5
      </xref>
      <xref ref-type="bibr" rid="CR6">
       6
      </xref>
      <xref ref-type="bibr" rid="CR7">
       7
      </xref>
      <xref ref-type="bibr" rid="CR8">
       8
      </xref>
      ,
      <xref ref-type="bibr" rid="CR9">
       9
      </xref>
      ,
      <xref ref-type="bibr" rid="CR10">
       10
      </xref>
     </sup>
     .
    </p>
    <p id="Par3">
     Tuttavia, mentre la comunità inizia ad affrontare i grandi costi operativi di addestramento e sviluppo dei modelli, sta diventando chiaro che il valore delle innovazioni ML è stato catturato principalmente da pochi attori
     <sup>
      <xref ref-type="bibr" rid="CR11">
       11
      </xref>
      <xref ref-type="bibr" rid="CR12">
       12
      </xref>
     </sup>
     . Mentre il resto dell'industria del retail sta facendo sforzi concreti per adattarsi prontamente, le aziende che offrono prodotti ML come servizio hanno recentemente guadagnato terreno, creando un nuovo mercato da miliardi di dollari
     <sup>
      <xref ref-type="bibr" rid="CR13">
       13
      </xref>
      ,
      <xref ref-type="bibr" rid="CR14">
       14
      </xref>
      ,
      <xref ref-type="bibr" rid="CR15">
       15
      </xref>
      ,
      <xref ref-type="bibr" rid="CR16">
       16
      </xref>
     </sup>
     . La necessità di capacità ML che possano essere applicate a intere industrie e verticali aumenta la posta in gioco per una domanda antica nell'ML:
     <italic>
      possiamo costruire modelli che possano essere riutilizzati su diversi compiti e dataset
     </italic>
     ?
    </p>
    <p id="Par4">
     Mentre la generalizzazione è una virtù teorica, i modelli nel mondo reale spesso hanno successo adattandosi (eccessivamente) a un dataset e un compito specifico
     <sup>
      <xref ref-type="bibr" rid="CR17">
       17
      </xref>
      ,
      <xref ref-type="bibr" rid="CR18">
       18
      </xref>
     </sup>
     . In pratica, la generalizzazione è stata considerata sia difficile da raggiungere
     <italic>
      che
     </italic>
     economicamente indesiderabile per casi d'uso su larga scala. In questo contesto, l'avvento di modelli auto-supervisionati su larga scala come il
     <italic>
      Contrastive Language-Image Pre-training
     </italic>
     (CLIP) è particolarmente interessante sia da un punto di vista teorico che pratico. Costruire su modelli pre-addestrati di grandi dimensioni per apprendere concetti
     <italic>
      generali
     </italic>
     in verticali/industrie specifiche (ad esempio, Moda, Elettronica, Fai-da-te, ecc.) può fornire un modo nuovo e sostenibile per portare i benefici delle capacità ML a un insieme più ampio di professionisti, specialmente al di fuori delle grandi aziende tecnologiche. L'idea sarebbe di perfezionare modelli fondamentali generali per apprendere concetti specifici di un dominio (ad esempio, la moda), ma abbastanza generali da essere applicabili a tutti i casi d'uso all'interno di quel dominio
     <sup>
      <xref ref-type="bibr" rid="CR17">
       17
      </xref>
      <xref ref-type="bibr" rid="CR19">
       19
      </xref>
     </sup>
     .
    </p>
    <p id="Par5">
     In
     <italic>
      questo
     </italic>
     lavoro, mostriamo attraverso test estensivi e codice open-source che l'addestramento multi-modale può essere utilizzato con successo per apprendere concetti generali in un dominio specifico, ovvero la moda. Infatti, sosteniamo che non solo è tecnicamente possibile, ma anche economicamente vantaggioso e praticamente utile, poiché allontanarsi dall'impostazione tradizionale in cui i singoli modelli supervisionati sono addestrati specificamente per caso d'uso riduce i costi di annotazione e manutenzione fornendo soluzioni trasferibili tra compiti.
    </p>
   </sec>
   <sec id="Sec3">
    <title>
     Apprendimento contrastivo auto-supervisionato di concetti di moda
    </title>
    <p id="Par6">
     L'apprendimento contrastivo è recentemente diventato un approccio predominante per apprendere rappresentazioni significative di concetti in ML. Il framework di apprendimento si basa sull'idea che concetti semanticamente correlati (ad esempio, due immagini dello stesso oggetto da punti di vista diversi) dovrebbero avere rappresentazioni
     <italic>
      simili
     </italic>
     , mentre quelli non correlati dovrebbero essere
     <italic>
      dissimili
     </italic>
     <sup/>
     . Inizialmente concepito per l'apprendimento della rappresentazione delle immagini auto-supervisionato, l'apprendimento contrastivo è stato recentemente applicato anche al linguaggio
     <sup>
      <xref ref-type="bibr" rid="CR20">
       20
      </xref>
      ,
      <xref ref-type="bibr" rid="CR21">
       21
      </xref>
      <xref ref-type="bibr" rid="CR22">
       22
      </xref>
      ,
      <xref ref-type="bibr" rid="CR23">
       23
      </xref>
     </sup>
     . Lavori recenti hanno utilizzato l'addestramento contrastivo per collegare diverse modalità, ad esempio, visione e linguaggio, audio e linguaggio, o una combinazione delle tre
     <sup>
      <xref ref-type="bibr" rid="CR24">
       24
      </xref>
      ,
      <xref ref-type="bibr" rid="CR25">
       25
      </xref>
      <xref ref-type="bibr" rid="CR26">
       26
      </xref>
      ,
      <xref ref-type="bibr" rid="CR27">
       27
      </xref>
      <xref ref-type="bibr" rid="CR28">
       28
      </xref>
      ,
      <xref ref-type="bibr" rid="CR29">
       29
      </xref>
     </sup>
     . Questi modelli apprendono rappresentazioni di concetti da diverse modalità (ad esempio, un estratto testuale come "un cane che corre su un campo" e un'immagine che rappresenta la scena) e le ottimizzano per essere vicine in uno spazio latente condiviso. Fondamentalmente, la pipeline tipica è auto-supervisionata: poiché non è coinvolta alcuna annotazione manuale (ad esempio, nell'esempio precedente, si possono raccogliere coppie immagine-testo dal web), l'intervento umano è limitato a decidere quale compito di pre-addestramento debba essere utilizzato.
    </p>
    <p id="Par7">
     <monospace>
      CLIP
     </monospace>
     è una rete neurale multi-modale visione-linguaggio addestrata tramite CL per associare concetti visivi con il testo. Il modello comprende un codificatore di visione e uno di testo, ciascuno seguito da uno strato lineare per proiettare le rappresentazioni di immagini e testo nello stesso spazio latente.
     <monospace>
      CLIP
     </monospace>
     è addestrato a
     <italic>
      posizionare
     </italic>
     immagini e descrizioni corrispondenti (ad esempio, un'immagine di una maglietta rossa e la sua descrizione "una maglietta rossa") vicine nello spazio vettoriale (vedi Fig.
     <xref ref-type="fig" rid="Fig1">
      1
     </xref>
     per un esempio). Quando addestrato su 400 milioni di coppie
     <immagine>
      , testo&gt; raccolte da internet,
      <monospace>
       CLIP
      </monospace>
      ha dimostrato un trasferimento competitivo zero-shot o few-shot a compiti a valle come OCR e classificazione fine-grained degli oggetti
      <sup>
       <xref ref-type="bibr" rid="CR17">
        17
       </xref>
       <xref ref-type="bibr" rid="CR17">
        17
       </xref>
      </sup>
      .
     </immagine>
    </p>
    <p id="Par8">
     Più formalmente,
     <monospace>
      CLIP
     </monospace>
     è un modello multi-modale che utilizza un codificatore di immagini (
     <inline-formula id="IEq1">
      <alternatives>
       <math id="IEq1_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi>
          I
         </mi>
         <msup>
          <mi>
           θ
          </mi>
          <mi>
           I
          </mi>
         </msup>
        </msub>
       </math>
       <tex-math id="IEq1_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$I_{\theta ^I}$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq1.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     ) e un codificatore di testo (
     <inline-formula id="IEq2">
      <alternatives>
       <math id="IEq2_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi>
          T
         </mi>
         <msup>
          <mi>
           θ
          </mi>
          <mi>
           T
          </mi>
         </msup>
        </msub>
       </math>
       <tex-math id="IEq2_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$T_{\theta ^T}$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq2.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     ). Entrambi i codificatori sono reti neurali profonde che mappano rappresentazioni grezze (cioè, un'immagine e un testo) a un vettore denso di 512 dimensioni (ad esempio, data un'immagine
     <italic>
      i
     </italic>
     ,
     <inline-formula id="IEq3">
      <alternatives>
       <math id="IEq3_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <msub>
          <mi>
           I
          </mi>
          <msup>
           <mi>
            θ
           </mi>
           <mi>
            I
           </mi>
          </msup>
         </msub>
         <mrow>
          <mo stretchy="false">
           (
          </mo>
          <mi>
           i
          </mi>
          <mo stretchy="false">
           )
          </mo>
         </mrow>
         <mo>
          ∈
         </mo>
         <msup>
          <mrow>
           <mi mathvariant="double-struck">
            R
           </mi>
          </mrow>
          <mn>
           512
          </mn>
         </msup>
        </mrow>
       </math>
       <tex-math id="IEq3_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$I_{\theta ^I}(i) \in \mathbb {R}^{512}$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq3.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     ). Durante l'addestramento,
     <italic>
      N
     </italic>
     coppie di immagini e testi corrispondenti
     <inline-formula id="IEq4">
      <alternatives>
       <math id="IEq4_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mo>
          &lt;
         </mo>
         <mi>
          i
         </mi>
         <mo>
          ,
         </mo>
         <mi>
          t
         </mi>
         <mo>
          &gt;
         </mo>
        </mrow>
       </math>
       <tex-math id="IEq4_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$&lt;i, t&gt;$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq4.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     sono selezionate (ad esempio, come in Fig.
     <xref ref-type="fig" rid="Fig1">
      1
     </xref>
     , l'immagine di una maglietta rossa e la descrizione "una maglietta rossa"), codificate usando
     <inline-formula id="IEq5">
      <alternatives>
       <math id="IEq5_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi>
          I
         </mi>
         <msup>
          <mi>
           θ
          </mi>
          <mi>
           I
          </mi>
         </msup>
        </msub>
       </math>
       <tex-math id="IEq5_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$I_{\theta ^I}$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq5.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     e
     <inline-formula id="IEq6">
      <alternatives>
       <math id="IEq6_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi>
          T
         </mi>
         <msup>
          <mi>
           θ
          </mi>
          <mi>
           T
          </mi>
         </msup>
        </msub>
       </math>
       <tex-math id="IEq6_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$T_{\theta ^T}$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq6.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     ,
     <inline-formula id="IEq7">
      <alternatives>
       <math id="IEq7_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi>
          L
         </mi>
         <mn>
          2
         </mn>
        </msub>
       </math>
       <tex-math id="IEq7_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$L_2$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq7.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     -normalizzate, e confrontate a coppie.
     <monospace>
      CLIP
     </monospace>
     minimizza la perdita di entropia incrociata in modo che
     <inline-formula id="IEq8">
      <alternatives>
       <math id="IEq8_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <msub>
          <mover accent="true">
           <mrow>
            <mi>
             I
            </mi>
           </mrow>
           <mrow>
            <mo stretchy="false">
             ¯
            </mo>
           </mrow>
          </mover>
          <msup>
           <mi>
            θ
           </mi>
           <mi>
            I
           </mi>
          </msup>
         </msub>
         <mrow>
          <mo stretchy="false">
           (
          </mo>
          <msub>
           <mi>
            i
           </mi>
           <mi>
            j
           </mi>
          </msub>
          <mo stretchy="false">
           )
          </mo>
         </mrow>
         <mo>
          ·
         </mo>
         <msub>
          <mover accent="true">
           <mrow>
            <mi>
             T
            </mi>
           </mrow>
           <mrow>
            <mo stretchy="false">
             ¯
            </mo>
           </mrow>
          </mover>
          <msup>
           <mi>
            θ
           </mi>
           <mi>
            T
           </mi>
          </msup>
         </msub>
         <mrow>
          <mo stretchy="false">
           (
          </mo>
          <msub>
           <mi>
            t
           </mi>
           <mi>
            k
           </mi>
          </msub>
          <mo stretchy="false">
           )
          </mo>
         </mrow>
        </mrow>
       </math>
       <tex-math id="IEq8_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$\bar{I}_{\theta ^I}(i_j) \cdot \bar{T}_{\theta ^T}(t_k)$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq8.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     per
     <inline-formula id="IEq9">
      <alternatives>
       <math id="IEq9_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mi>
          j
         </mi>
         <mo>
          ,
         </mo>
         <mi>
          k
         </mi>
         <mo>
          =
         </mo>
         <mn>
          1
         </mn>
         <mo>
          ,
         </mo>
         <mo>
          .
         </mo>
         <mo>
          .
         </mo>
         <mo>
          ,
         </mo>
         <mi>
          N
         </mi>
        </mrow>
       </math>
       <tex-math id="IEq9_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$j,k = 1,..,N$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq9.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     è massimo quando la didascalia è abbinata all'immagine corretta (
     <inline-formula id="IEq10">
      <alternatives>
       <math id="IEq10_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mi>
          j
         </mi>
         <mo>
          =
         </mo>
         <mi>
          k
         </mi>
        </mrow>
       </math>
       <tex-math id="IEq10_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$j=k$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq10.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     ), e basso altrimenti (
     <inline-formula id="IEq11">
      <alternatives>
       <math id="IEq11_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mi>
          j
         </mi>
         <mo>
          ≠
         </mo>
         <mi>
          k
         </mi>
        </mrow>
       </math>
       <tex-math id="IEq11_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$j \ne k$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq11.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     ), dove
     <inline-formula id="IEq12">
      <alternatives>
       <math id="IEq12_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <msub>
          <mover accent="true">
           <mrow>
            <mi>
             I
            </mi>
           </mrow>
           <mrow>
            <mo stretchy="false">
             ¯
            </mo>
           </mrow>
          </mover>
          <msup>
           <mi>
            θ
           </mi>
           <mi>
            I
           </mi>
          </msup>
         </msub>
         <mrow>
          <mo stretchy="false">
           (
          </mo>
          <mo>
           ·
          </mo>
          <mo stretchy="false">
           )
          </mo>
         </mrow>
        </mrow>
       </math>
       <tex-math id="IEq12_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$\bar{I}_{\theta ^I}(\cdot )$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq12.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     /
     <inline-formula id="IEq13">
      <alternatives>
       <math id="IEq13_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <msub>
          <mover accent="true">
           <mrow>
            <mi>
             T
            </mi>
           </mrow>
           <mrow>
            <mo stretchy="false">
             ¯
            </mo>
           </mrow>
          </mover>
          <msup>
           <mi>
            θ
           </mi>
           <mi>
            I
           </mi>
          </msup>
         </msub>
         <mrow>
          <mo stretchy="false">
           (
          </mo>
          <mo>
           ·
          </mo>
          <mo stretchy="false">
           )
          </mo>
         </mrow>
        </mrow>
       </math>
       <tex-math id="IEq13_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$\bar{T}_{\theta ^I}(\cdot )$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq13.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     sono gli output
     <inline-formula id="IEq14">
      <alternatives>
       <math id="IEq14_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi>
          L
         </mi>
         <mn>
          2
         </mn>
        </msub>
       </math>
       <tex-math id="IEq14_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$L_2$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq14.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     -normalizzati dei codificatori di immagini e testo. Riassumiamo l'obiettivo di ottimizzazione per
     <monospace>
      CLIP
     </monospace>
     nelle Eq. (
     <xref ref-type="disp-formula" rid="Equ1">
      1
     </xref>
     ) e (
     <xref ref-type="disp-formula" rid="Equ2">
      2
     </xref>
     ).
     <disp-formula id="Equ1">
      <label>
       1
      </label>
      <alternatives>
       <math display="block" id="Equ1_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mtable>
          <mtr>
           <mtd/>
           <mtd columnalign="left">
            <mrow>
             <mi mathvariant="script">
              L
             </mi>
             <mrow>
              <mo stretchy="false">
               (
              </mo>
              <msup>
               <mi>
                θ
               </mi>
               <mi>
                I
               </mi>
              </msup>
              <mo>
               ,
              </mo>
              <msup>
               <mi>
                θ
               </mi>
               <mi>
                T
               </mi>
              </msup>
              <mo stretchy="false">
               )
              </mo>
             </mrow>
             <mo>
              =
             </mo>
             <mo>
              -
             </mo>
             <mfrac>
              <mn>
               1
              </mn>
              <mrow>
               <mn>
                2
               </mn>
               <mi>
                N
               </mi>
              </mrow>
             </mfrac>
             <mfenced close=")" open="(">
              <munder>
               <mo>
                ∑
               </mo>
               <mi>
                j
               </mi>
              </munder>
              <mo>
               log
              </mo>
              <mfrac>
               <msup>
                <mi>
                 e
                </mi>
                <mrow>
                 <msub>
                  <mover accent="true">
                   <mrow>
                    <mi>
                     I
                    </mi>
                   </mrow>
                   <mrow>
                    <mo stretchy="false">
                     ¯
                    </mo>
                   </mrow>
                  </mover>
                  <msup>
                   <mi>
                    θ
                   </mi>
                   <mi>
                    I
                   </mi>
                  </msup>
                 </msub>
                 <mrow>
                  <mo stretchy="false">
                   (
                  </mo>
                  <msub>
                   <mi>
                    i
                   </mi>
                   <mi>
                    j
                   </mi>
                  </msub>
                  <mo stretchy="false">
                   )
                  </mo>
                 </mrow>
                 <mo>
                  ·
                 </mo>
                 <msub>
                  <mover accent="true">
                   <mrow>
                    <mi>
                     T
                    </mi>
                   </mrow>
                   <mrow>
                    <mo stretchy="false">
                     ¯
                    </mo>
                   </mrow>
                  </mover>
                  <msup>
                   <mi>
                    θ
                   </mi>
                   <mi>
                    T
                   </mi>
                  </msup>
                 </msub>
                 <mrow>
                  <mo stretchy="false">
                   (
                  </mo>
                  <msub>
                   <mi>
                    t
                   </mi>
                   <mi>
                    j
                   </mi>
                  </msub>
                  <mo stretchy="false">
                   )
                  </mo>
                 </mrow>
                </mrow>
               </msup>
               <mrow>
                <msub>
                 <mo>
                  ∑
                 </mo>
                 <msup>
                  <mi>
                   k
                  </mi>
                  <msup>
                   <mrow/>
                   <mo>
                    ′
                   </mo>
                  </msup>
                 </msup>
                </msub>
                <msup>
                 <mi>
                  e
                 </mi>
                 <mrow>
                  <msub>
                   <mover accent="true">
                    <mrow>
                     <mi>
                      I
                     </mi>
                    </mrow>
                    <mrow>
                     <mo stretchy="false">
                      ¯
                     </mo>
                    </mrow>
                   </mover>
                   <msup>
                    <mi>
                     θ
                    </mi>
                    <mi>
                     I
                    </mi>
                   </msup>
                  </msub>
                  <mrow>
                   <mo stretchy="false">
                    (
                   </mo>
                   <msub>
                    <mi>
                     i
                    </mi>
                    <mi>
                     j
                    </mi>
                   </msub>
                   <mo stretchy="false">
                    )
                   </mo>
                  </mrow>
                  <mo>
                   ·
                  </mo>
                  <msub>
                   <mover accent="true">
                    <mrow>
                     <mi>
                      T
                     </mi>
                    </mrow>
                    <mrow>
                     <mo stretchy="false">
                      ¯
                     </mo>
                    </mrow>
                   </mover>
                   <msup>
                    <mi>
                     θ
                    </mi>
                    <mi>
                     T
                    </mi>
                   </msup>
                  </msub>
                  <mrow>
                   <mo stretchy="false">
                    (
                   </mo>
                   <msub>
                    <mi>
                     t
                    </mi>
                    <msup>
                     <mi>
                      k
                     </mi>
                     <msup>
                      <mrow/>
                      <mo>
                       ′
                      </mo>
                     </msup>
                    </msup>
                   </msub>
                   <mo stretchy="false">
                    )
                   </mo>
                  </mrow>
                 </mrow>
                </msup>
               </mrow>
              </mfrac>
              <mo>
               +
              </mo>
              <munder>
               <mo>
                ∑
               </mo>
               <mi>
                k
               </mi>
              </munder>
              <mo>
               log
              </mo>
              <mfrac>
               <msup>
                <mi>
                 e
                </mi>
                <mrow>
                 <msub>
                  <mover accent="true">
                   <mrow>
                    <mi>
                     I
                    </mi>
                   </mrow>
                   <mrow>
                    <mo stretchy="false">
                     ¯
                    </mo>
                   </mrow>
                  </mover>
                  <msup>
                   <mi>
                    θ
                   </mi>
                   <mi>
                    I
                   </mi>
                  </msup>
                 </msub>
                 <mrow>
                  <mo stretchy="false">
                   (
                  </mo>
                  <msub>
                   <mi>
                    i
                   </mi>
                   <mi>
                    k
                   </mi>
                  </msub>
                  <mo stretchy="false">
                   )
                  </mo>
                 </mrow>
                 <mo>
                  ·
                 </mo>
                 <msub>
                  <mover accent="true">
                   <mrow>
                    <mi>
                     T
                    </mi>
                   </mrow>
                   <mrow>
                    <mo stretchy="false">
                     ¯
                    </mo>
                   </mrow>
                  </mover>
                  <msup>
                   <mi>
                    θ
                   </mi>
                   <mi>
                    T
                   </mi>
                  </msup>
                 </msub>
                 <mrow>
                  <mo stretchy="false">
                   (
                  </mo>
                  <msub>
                   <mi>
                    t
                   </mi>
                   <mi>
                    k
                   </mi>
                  </msub>
                  <mo stretchy="false">
                   )
                  </mo>
                 </mrow>
                </mrow>
               </msup>
               <mrow>
                <msub>
                 <mo>
                  ∑
                 </mo>
                 <msup>
                  <mi>
                   j
                  </mi>
                  <msup>
                   <mrow/>
                   <mo>
                    ′
                   </mo>
                  </msup>
                 </msup>
                </msub>
                <msup>
                 <mi>
                  e
                 </mi>
                 <mrow>
                  <msub>
                   <mover accent="true">
                    <mrow>
                     <mi>
                      I
                     </mi>
                    </mrow>
                    <mrow>
                     <mo stretchy="false">
                      ¯
                     </mo>
                    </mrow>
                   </mover>
                   <msup>
                    <mi>
                     θ
                    </mi>
                    <mi>
                     I
                    </mi>
                   </msup>
                  </msub>
                  <mrow>
                   <mo stretchy="false">
                    (
                   </mo>
                   <msub>
                    <mi>
                     i
                    </mi>
                    <msup>
                     <mi>
                      j
                     </mi>
                     <msup>
                      <mrow/>
                      <mo>
                       ′
                      </mo>
                     </msup>
                    </msup>
                   </msub>
                   <mo stretchy="false">
                    )
                   </mo>
                  </mrow>
                  <mo>
                   ·
                  </mo>
                  <msub>
                   <mover accent="true">
                    <mrow>
                     <mi>
                      T
                     </mi>
                    </mrow>
                    <mrow>
                     <mo stretchy="false">
                      ¯
                     </mo>
                    </mrow>
                   </mover>
                   <msup>
                    <mi>
                     θ
                    </mi>
                    <mi>
                     T
                    </mi>
                   </msup>
                  </msub>
                  <mrow>
                   <mo stretchy="false">
                    (
                   </mo>
                   <msub>
                    <mi>
                     t
                    </mi>
                    <mi>
                     k
                    </mi>
                   </msub>
                   <mo stretchy="false">
                    )
                   </mo>
                  </mrow>
                 </mrow>
                </msup>
               </mrow>
              </mfrac>
             </mfenced>
            </mrow>
           </mtd>
          </mtr>
         </mtable>
        </mrow>
       </math>
       <tex-math id="Equ1_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$\begin{aligned}{\mathscr {L}}(\theta ^{I}, \theta ^{T}) = - \frac{1}{2N} \left( \sum _{j} \log \frac{e^{\bar{I}_{\theta ^I}(i_{j}) \cdot \bar{T}_{\theta ^T}(t_{j})}}{\sum _{k^{'}} e^{\bar{I}_{\theta ^{I}}(i_{j}) \cdot \bar{T}_{\theta ^{T}}(t_{k^{'}})}} + \sum _{k} \log \frac{e^{\bar{I}_{\theta ^I}(i_k) \cdot \bar{T}_{\theta ^{T}}(t_{k})}}{\sum _{j^{'}} e^{\bar{I}_{\theta ^{I}}(i_{j^{'}}) \cdot \bar{T}_{\theta ^{T}}(t_{k})}} \right) \end{aligned}$$\end{document}
       </tex-math>
       <graphic href="41598_2022_23052_Article_Equ1.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </disp-formula>
     <disp-formula id="Equ2">
      <label>
       2
      </label>
      <alternatives>
       <math display="block" id="Equ2_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mtable>
          <mtr>
           <mtd/>
           <mtd columnalign="left">
            <mrow>
             <msup>
              <mi>
               θ
              </mi>
              <mrow>
               <mi>
                I
               </mi>
               <mrow/>
               <mo>
                ∗
               </mo>
              </mrow>
             </msup>
             <mo>
              ,
             </mo>
             <msup>
              <mi>
               θ
              </mi>
              <mrow>
               <mi>
                T
               </mi>
               <mrow/>
               <mo>
                ∗
               </mo>
              </mrow>
             </msup>
             <mo>
              =
             </mo>
             <munder>
              <mrow>
               <mi mathvariant="normal">
                arg
               </mi>
               <mspace width="0.166667em"/>
               <mi mathvariant="normal">
                min
               </mi>
              </mrow>
              <mrow>
               <msup>
                <mi>
                 θ
                </mi>
                <mi>
                 I
                </mi>
               </msup>
               <mo>
                ,
               </mo>
               <msup>
                <mi>
                 θ
                </mi>
                <mi>
                 T
                </mi>
               </msup>
              </mrow>
             </munder>
             <mspace width="0.166667em"/>
             <mi mathvariant="script">
              L
             </mi>
             <mrow>
              <mo stretchy="false">
               (
              </mo>
              <msup>
               <mi>
                θ
               </mi>
               <mi>
                I
               </mi>
              </msup>
              <mo>
               ,
              </mo>
              <msup>
               <mi>
                θ
               </mi>
               <mi>
                T
               </mi>
              </msup>
              <mo stretchy="false">
               )
              </mo>
             </mrow>
            </mrow>
           </mtd>
          </mtr>
         </mtable>
        </mrow>
       </math>
       <tex-math id="Equ2_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$\begin{aligned}\theta ^{I*}, \theta ^{T*} = {\mathop {\mathrm{arg\,min}}\limits _{\theta ^{I}, \theta ^{T}}}\, {\mathscr {L}}(\theta ^{I},\theta ^{T}) \end{aligned}$$\end{document}
       </tex-math>
       <graphic href="41598_2022_23052_Article_Equ2.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </disp-formula>
     Qui,
     <inline-formula id="IEq15">
      <alternatives>
       <math id="IEq15_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mi>
          θ
         </mi>
         <mi>
          I
         </mi>
        </msup>
       </math>
       <tex-math id="IEq15_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$\theta ^I$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq15.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     e
     <inline-formula id="IEq16">
      <alternatives>
       <math id="IEq16_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mi>
          θ
         </mi>
         <mi>
          T
         </mi>
        </msup>
       </math>
       <tex-math id="IEq16_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$\theta ^T$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq16.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     sono i parametri apprendibili delle reti neurali codificatrici di immagini e testo, e l'operatore
     <inline-formula id="IEq17">
      <alternatives>
       <math id="IEq17_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mo stretchy="false">
          (
         </mo>
         <mo>
          ·
         </mo>
         <mo stretchy="false">
          )
         </mo>
        </mrow>
       </math>
       <tex-math id="IEq17_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$(\cdot )$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq17.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     rappresenta il prodotto scalare. Il primo operando di addizione dell'Equazione
     <xref ref-type="disp-formula" rid="Equ1">
      1
     </xref>
     è l'entropia incrociata sull'asse delle immagini, mentre il secondo operando di addizione è sull'asse del testo.
     <fig id="Fig1" position="float">
      <label>
       Figure 1
      </label>
      <caption xml:lang="en">
       <p>
        Rappresentazione bidimensionale delle immagini e del testo nello spazio vettoriale di FashionCLIP
        <bold>
         prima
        </bold>
        e
        <bold>
         dopo
        </bold>
        l'addestramento. Le immagini e le loro corrispondenti descrizioni testuali sono incorporate più vicine tra loro nello spazio vettoriale latente dopo l'addestramento.
       </p>
      </caption>
      <graphic href="/ProjectMundo/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig1_HTML.png" id="MO3" mime-subtype="PNG" specific-use="web"/>
     </fig>
    </p>
    <p id="Par9">
     Recentemente, i professionisti del settore hanno iniziato a riconoscere l'importanza e l'utilità del pre-addestramento contrastivo per il loro dominio di riferimento, con diversi lavori che presentano applicazioni a valle di successo a partire dal modello
     <monospace>
      CLIP
     </monospace>
     <sup>
      <xref ref-type="bibr" rid="CR30">
       30
      </xref>
     </sup>
     . Nella moda, la natura multi-modale di CLIP è stata trovata utile nei modelli discriminativi recenti, che sono stati sviluppati sotto il paradigma standard di modelli supervisionati specifici per compito. Nel contesto generativo, CLIP spesso completa un quadro più ampio: ad esempio, CLIP è utilizzato per apprendere codebook linguisticamente fondati in Variational Auto Encoders o per guidare la sintesi e la manipolazione delle immagini nei modelli generativi di diffusione. Sebbene interessante per il grounding (vedi sotto Fig.
     <xref ref-type="fig" rid="Fig8">
      8
     </xref>
     ), il caso d'uso target (generazione di immagini) e il focus più ristretto (singolo compito, singolo dataset) non sono facilmente comparabili a
     <monospace>
      FashionCLIP
     </monospace>
     , ma suggeriscono invece una possibile applicazione complementare nei casi d'uso generativi. Tuttavia, nessuna applicazione recente di CLIP è stata sviluppata per produrre rappresentazioni a livello industriale attraverso molteplici casi d'uso e dataset. In altre parole, CLIP è stato utilizzato solo come modello pre-addestrato, senza alcun tentativo di superare i problemi operativi e concettuali dei modelli supervisionati a singolo compito
     <sup>
      <xref ref-type="bibr" rid="CR31">
       31
      </xref>
      ,
      <xref ref-type="bibr" rid="CR32">
       32
      </xref>
      <xref ref-type="bibr" rid="CR33">
       33
      </xref>
      <xref ref-type="bibr" rid="CR34">
       34
      </xref>
      ,
      <xref ref-type="bibr" rid="CR35">
       35
      </xref>
     </sup>
     .
    </p>
    <p id="Par10">
     In
     <italic>
      questo
     </italic>
     lavoro, introduciamo
     <monospace>
      FashionCLIP
     </monospace>
     , un modello basato su CLIP esplicitamente addestrato e testato per produrre rappresentazioni generali di prodotti per concetti di moda. Addestriamo
     <monospace>
      FashionCLIP
     </monospace>
     su un ampio dataset di moda nuovo e di alta qualità: come discusso nella sezione successiva, il nostro obiettivo è stabilire se tale fine-tuning sia sufficiente a produrre rappresentazioni di prodotti trasferibili in modalità zero-shot a dataset completamente nuovi.
    </p>
   </sec>
   <sec id="Sec4">
    <title>
     Domanda di ricerca e metodologia
    </title>
    <p id="Par11">
     I modelli supervisionati standard per applicazioni specifiche verticali come la moda sono costosi da addestrare e operare, fornendo una grande barriera all'ingresso per i fornitori SaaS e i giocatori più piccoli
     <sup>
      <xref ref-type="bibr" rid="CR12">
       12
      </xref>
     </sup>
     . Ad esempio, un modello di classificazione dei prodotti potrebbe essere addestrato su coppie
     <inline-formula id="IEq18">
      <alternatives>
       <math id="IEq18_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mo>
          &lt;
         </mo>
         <mi>
          p
         </mi>
         <mi>
          r
         </mi>
         <mi>
          o
         </mi>
         <mi>
          d
         </mi>
         <mi>
          u
         </mi>
         <mi>
          c
         </mi>
         <mi>
          t
         </mi>
         <mspace width="0.166667em"/>
         <mi>
          d
         </mi>
         <mi>
          e
         </mi>
         <mi>
          s
         </mi>
         <mi>
          c
         </mi>
         <mi>
          r
         </mi>
         <mi>
          i
         </mi>
         <mi>
          p
         </mi>
         <mi>
          t
         </mi>
         <mi>
          i
         </mi>
         <mi>
          o
         </mi>
         <mi>
          n
         </mi>
         <mo>
          ,
         </mo>
         <mspace width="0.166667em"/>
         <mi>
          c
         </mi>
         <mi>
          a
         </mi>
         <mi>
          t
         </mi>
         <mi>
          e
         </mi>
         <mi>
          g
         </mi>
         <mi>
          o
         </mi>
         <mi>
          r
         </mi>
         <mi>
          y
         </mi>
         <mo>
          &gt;
         </mo>
        </mrow>
       </math>
       <tex-math id="IEq18_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$&lt;product\,description,\,category&gt;$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq18.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     derivate dai dati del catalogo mentre si ottimizza per l'accuratezza della classificazione: se le etichette cambiano, o il modello viene distribuito su un catalogo diverso, l'accuratezza diminuirebbe. È importante notare che passare a architetture basate su CLIP, come
     <italic>
      CMA-CLIP
     </italic>
     , non risolve
     <italic>
      ipso facto
     </italic>
     il problema: se CLIP è utilizzato come modello per compito, solleverà gli stessi problemi di scalabilità dei metodi supervisionati tradizionali
     <sup>
      <xref ref-type="bibr" rid="CR36">
       36
      </xref>
      <xref ref-type="bibr" rid="CR37">
       37
      </xref>
     </sup>
     .
    </p>
    <p id="Par12">
     Dopo l'addestramento di
     <monospace>
      FashionCLIP
     </monospace>
     , ci siamo posti una domanda più ampia e potenzialmente più impattante: dato il dataset giusto e la procedura di fine-tuning, possiamo apprendere concetti multi-modali che siano abbastanza generali per l'intero dominio della moda? Procediamo con una combinazione di benchmark quantitativi – ispirati sia dalla letteratura esistente che da problemi noti per essere importanti nell'industria – e sonde qualitative per rispondere: poiché ottenere concetti generali è il nostro obiettivo, è importante verificare che
     <monospace>
      FashionCLIP
     </monospace>
     non apprenda solo un dataset (ad esempio, una "collezione Armani"), ma concetti genuinamente trasferibili, come "gonna", "maniche", ecc. Ispirandoci a CLIP, i nostri due benchmark iniziali testeranno come
     <monospace>
      FashionCLIP
     </monospace>
     passa dal testo all'immagine e viceversa (vedi Fig.
     <xref ref-type="fig" rid="Fig2">
      2
     </xref>
     ):
     <list list-type="order">
      <list-item>
       <p id="Par13">
        <italic>
         Dal testo all'immagine
        </italic>
        La ricerca di prodotti è uno dei principali canali di interazione e ricavi tra un negozio e i suoi utenti, rappresentando in media dal 30% al 60% dei ricavi totali online
        <sup>
         <xref ref-type="bibr" rid="CR38">
          38
         </xref>
         ,
         <xref ref-type="bibr" rid="CR39">
          39
         </xref>
        </sup>
        . Storicamente, la ricerca di prodotti è stata eseguita principalmente con caratteristiche testuali, abbinando prima le query e le descrizioni dei prodotti in un indice e poi riordinando i risultati candidati
        <sup>
         <xref ref-type="bibr" rid="CR40">
          40
         </xref>
         ,
         <xref ref-type="bibr" rid="CR41">
          41
         </xref>
         ,
         <xref ref-type="bibr" rid="CR42">
          42
         </xref>
         <xref ref-type="bibr" rid="CR43">
          43
         </xref>
        </sup>
        . Tuttavia, ci sono buone ragioni per credere che includere caratteristiche visive possa portare miglioramenti significativi poiché le immagini sono spesso l'aspetto più curato del catalogo. Al contrario, la qualità del testo varia tra verticali, lingue e feed di prodotti specifici. I nostri test estensivi mostrano che
        <monospace>
         FashionCLIP
        </monospace>
        apprende concetti di moda e li applica con successo a prodotti non visti e descrizioni incomplete o ambigue.
       </p>
      </list-item>
     </list>
     <fig id="Fig2" position="float">
      <label>
       Figure 2
      </label>
      <caption xml:lang="en">
       <p>
        Panoramica schematica del recupero multi-modale (sinistra) e dei compiti di classificazione zero-shot (destra).
       </p>
      </caption>
      <graphic href="/ProjectMundo/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig2_HTML.png" id="MO4" mime-subtype="PNG" specific-use="web"/>
     </fig>
    </p>
   </sec>
  </sec>
  <sec id="Sec5" sec-type="results">
   <title>
    Risultati
   </title>
   <p id="Par20">
    In questa sezione, dettagliamo le prestazioni di
    <monospace>
     FashionCLIP
    </monospace>
    su una gamma di compiti, dimostrando l'efficacia dell'adattamento al dominio e l'applicabilità dei modelli simili a
    <monospace>
     CLIP
    </monospace>
    alla moda. I dettagli sull'addestramento e sulla valutazione sono disponibili nella Sezione “
    <xref ref-type="sec" rid="Sec12">
     Methods
    </xref>
    ”. Utilizziamo una varietà di dataset in-domain e out-of-domain, con vari gradi di somiglianza:
    <bold>
     TEST
    </bold>
    è il set di test di
    <italic>
     Farfetch
    </italic>
    contenente 20k prodotti;
    <bold>
     HOUT-C
    </bold>
    è il dataset contenente una categoria che abbiamo escluso dall'addestramento;
    <bold>
     HOUT-B
    </bold>
    è il dataset contenente due marchi che sono stati esclusi dall'addestramento;
    <bold>
     STLE
    </bold>
    è un dataset di merchandising di
    <italic>
     Farfetch
    </italic>
    ;
    <bold>
     KAGL
    </bold>
    è un sottoinsieme in cui ogni prodotto ha un'immagine su sfondo bianco, una didascalia e una categoria;
    <bold>
     F-MNIST
    </bold>
    contiene 10.000 immagini in scala di grigi di 10 classi di prodotti;
    <bold>
     DEEP
    </bold>
    contiene 4000 immagini di prodotti non standardizzate (cioè, contengono umani) da 50 categorie. Una panoramica dei dati di immagine e testuali offerti da Farfetch (
    <bold>
     TEST, HOUT-C, HOUT-B, STLE
    </bold>
    ),
    <bold>
     KAGL
    </bold>
    ,
    <bold>
     F-MNIST
    </bold>
    e
    <bold>
     DEEP
    </bold>
    può essere trovata in Fig.
    <xref ref-type="fig" rid="Fig3">
     3
    </xref>
    <sup>
     <xref ref-type="bibr" rid="CR50">
      50
     </xref>
     <xref ref-type="bibr" rid="CR51">
      51
     </xref>
     <xref ref-type="bibr" rid="CR52">
      52
     </xref>
    </sup>
    . I nostri ampi benchmark e valutazioni rispondono quantitativamente a due domande di ricerca: la conoscenza specifica del dominio può migliorare la comprensione di CLIP di un'industria (Fig.
    <xref ref-type="fig" rid="Fig5">
     5
    </xref>
    ) e, se sì, tale conoscenza si traduce in diversi casi d'uso e dataset?
    <fig id="Fig3" position="float">
     <label>
      Figure 3
     </label>
     <caption xml:lang="en">
      <p>
       Campione di dati da vari dataset utilizzati. Osserviamo una gamma di distribuzioni sia nelle modalità immagine che testuale. Per la modalità immagine, vediamo una gamma da "Bassa risoluzione, B &amp;N" a "Alta risoluzione, In-the-Wild". Per la modalità testuale, Farfetch offre la migliore "risoluzione testuale", mentre DEEP presenta anche una terminologia molto specifica per la moda. I dataset
       <bold>
        KAGL
       </bold>
       ,
       <bold>
        F-MNIST
       </bold>
       e
       <bold>
        DEEP
       </bold>
       sono pubblicamente disponibili. Per maggiori dettagli riguardanti i dati, vedere la Sezione Disponibilità dei Dati.
      </p>
     </caption>
     <graphic href="/ProjectMundo/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig3_HTML.png" id="MO5" mime-subtype="PNG" specific-use="web"/>
    </fig>
   </p>
   <sec id="Sec6">
    <title>
     Recupero multi-modale
    </title>
    <p id="Par21">
     Il compito di Recupero Multi-modale è descritto come segue: data una descrizione testuale e un insieme di immagini, chiediamo al modello di trovare l'immagine correlata a quella descrizione. Ad esempio, un compito di recupero prodotto implica l'abbinamento di una descrizione del prodotto (ad es., “un polo rosso per uomo”) e una sua foto in un catalogo.
    </p>
    <p id="Par22">
     Il recupero multi-modale è possibile grazie all'obiettivo di ottimizzazione di
     <monospace>
      FashionCLIP
     </monospace>
     che allinea gli spazi latenti di linguaggio e immagine (vedi Fig.
     <xref ref-type="fig" rid="Fig1">
      1
     </xref>
     ). Testiamo
     <monospace>
      FashionCLIP
     </monospace>
     sul recupero multi-modale per valutare i benefici del fine-tuning specifico del dominio nella ricerca di prodotti nel mondo reale.
    </p>
    <p id="Par23">
     Il nostro benchmark prende come input una descrizione del prodotto dal
     <italic>
      set di test
     </italic>
     del catalogo e chiede ai modelli di classificare le immagini dei prodotti corrispondenti alla didascalia—lo standard di riferimento è l'immagine associata al prodotto. Estraggiamo la classifica utilizzando le somiglianze degli embedding:
     <monospace>
      FashionCLIP
     </monospace>
     esegue il prodotto scalare tra l'embedding della didascalia di input e ciascun embedding vettoriale dell'immagine ottenuto tramite
     <inline-formula id="IEq20">
      <alternatives>
       <math id="IEq20_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <msub>
          <mover accent="true">
           <mrow>
            <mi>
             T
            </mi>
           </mrow>
           <mrow>
            <mo stretchy="false">
             ¯
            </mo>
           </mrow>
          </mover>
          <msup>
           <mi>
            θ
           </mi>
           <mi>
            T
           </mi>
          </msup>
         </msub>
         <mrow>
          <mo stretchy="false">
           (
          </mo>
          <mo>
           ·
          </mo>
          <mo stretchy="false">
           )
          </mo>
         </mrow>
        </mrow>
       </math>
       <tex-math id="IEq20_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$\bar{T}_{\theta ^T}(\cdot )$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq20.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     e
     <inline-formula id="IEq21">
      <alternatives>
       <math id="IEq21_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <msub>
          <mover accent="true">
           <mrow>
            <mi>
             I
            </mi>
           </mrow>
           <mrow>
            <mo stretchy="false">
             ¯
            </mo>
           </mrow>
          </mover>
          <msup>
           <mi>
            θ
           </mi>
           <mi>
            I
           </mi>
          </msup>
         </msub>
         <mrow>
          <mo stretchy="false">
           (
          </mo>
          <mo>
           ·
          </mo>
          <mo stretchy="false">
           )
          </mo>
         </mrow>
        </mrow>
       </math>
       <tex-math id="IEq21_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$\bar{I}_{\theta ^I}(\cdot )$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq21.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     rispettivamente e restituisce una classifica in ordine decrescente. Utilizziamo
     <italic>
      HITS@5
     </italic>
     (Hit Rate @
     <inline-formula id="IEq22">
      <alternatives>
       <math id="IEq22_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mi>
          k
         </mi>
         <mo>
          =
         </mo>
         <mn>
          5
         </mn>
        </mrow>
       </math>
       <tex-math id="IEq22_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$k=5$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq22.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     ) e
     <italic>
      MRR
     </italic>
     (Mean Reciprocal Rank) come metriche. La Tabella
     <xref ref-type="table" rid="Tab1">
      1
     </xref>
     confronta
     <monospace>
      FashionCLIP
     </monospace>
     con
     <monospace>
      CLIP
     </monospace>
     non specifico per il dominio su diversi set di test esclusi e mostra come il fine-tuning migliori significativamente la comprensione del nostro dominio target
     <sup>
      <xref ref-type="bibr" rid="CR53">
       53
      </xref>
      <xref ref-type="bibr" rid="CR54">
       54
      </xref>
     </sup>
     .
    </p>
    <p id="Par24">
     Eseguiamo anche test qualitativi estensivi confrontando
     <monospace>
      FashionCLIP
     </monospace>
     con il motore di ricerca di produzione attualmente impiegato nel catalogo. La Fig.
     <xref ref-type="fig" rid="Fig4">
      4
     </xref>
     mostra un caso di particolare interesse per la ricerca di prodotti: in questo esempio, i concetti visivi non appartengono al dominio della moda e non sono disponibili nella didascalia. Il primo confronto (
     <italic>
      sinistra
     </italic>
     ) mostra che
     <monospace>
      FashionCLIP
     </monospace>
     può recuperare il concetto di
     <italic>
      tigre
     </italic>
     quando richiesto con "t-shirt con tigre"; per la stessa query, il motore di ricerca recupera articoli che corrispondono alla categoria, incapace di interpretare
     <italic>
      tigre
     </italic>
     basandosi solo sul testo. Il secondo confronto (
     <italic>
      destra
     </italic>
     ) mostra che
     <monospace>
      FashionCLIP
     </monospace>
     può interpretare
     <italic>
      un gatto
     </italic>
     da un disegno stilizzato, parzialmente occultato. Al contrario, il motore di ricerca non riesce a generalizzare oltre le didascalie che contengono esplicitamente la stringa "gatto". Infine, visualizzare gli embedding appresi (Fig.
     <xref ref-type="fig" rid="Fig5">
      5
     </xref>
     ) aiuta anche a costruire un'intuizione della migliore risoluzione concettuale di
     <monospace>
      FashionCLIP
     </monospace>
     quando si tratta del dominio target.
     <fig id="Fig4" position="float">
      <label>
       Figure 4
      </label>
      <caption xml:lang="en">
       <p>
        Recupero con concetti non legati alla moda. Risultati di esempio per "t-shirt con tigre" e "t-shirt con gatto" da
        <monospace>
         FashionCLIP
        </monospace>
        (
        <italic>
         verde
        </italic>
        ) vs motore di ricerca di produzione
        <italic>
         Farfetch
        </italic>
        (
        <italic>
         rosso
        </italic>
        ).
       </p>
      </caption>
      <graphic href="/ProjectMundo/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig4_HTML.png" id="MO6" mime-subtype="PNG" specific-use="web"/>
     </fig>
     <table-wrap id="Tab1">
      <label>
       Table 1
      </label>
      <caption xml:lang="en">
       <p>
        Confronto tra
        <monospace>
         FashionCLIP
        </monospace>
        (
        <monospace>
         F-CLIP
        </monospace>
        ) e
        <monospace>
         CLIP
        </monospace>
        nel compito di recupero multi-modale.
       </p>
      </caption>
      <table frame="hsides" rules="groups">
       <thead>
        <tr>
         <th align="left">
          <p>
           Modello
          </p>
         </th>
         <th align="left">
          <p>
           Dataset
          </p>
         </th>
         <th align="left">
          <p>
           HITS@5
          </p>
         </th>
         <th align="left">
          <p>
           MRR
          </p>
         </th>
        </tr>
       </thead>
       <tbody>
        <tr>
         <td align="left">
          <p>
           <monospace>
            F-CLIP
           </monospace>
          </p>
         </td>
         <td align="left" rowspan="2">
          <p>
           TEST
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.66
           </bold>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.50
           </bold>
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.28
          </p>
         </td>
         <td align="left">
          <p>
           0.21
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            F-CLIP
           </monospace>
          </p>
         </td>
         <td align="left" rowspan="2">
          <p>
           HOUT-C
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.62
           </bold>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.47
           </bold>
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.33
          </p>
         </td>
         <td align="left">
          <p>
           0.23
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            F-CLIP
           </monospace>
          </p>
         </td>
         <td align="left" rowspan="2">
          <p>
           HOUT-B
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.58
           </bold>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.41
           </bold>
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.31
          </p>
         </td>
         <td align="left">
          <p>
           0.22
          </p>
         </td>
        </tr>
       </tbody>
      </table>
      <table-wrap-foot>
       <p>
        I modelli con le migliori prestazioni sono in grassetto.
       </p>
      </table-wrap-foot>
     </table-wrap>
     <table-wrap id="Tab2">
      <label>
       Table 2
      </label>
      <caption xml:lang="en">
       <p>
        Confronto delle prestazioni di
        <monospace>
         FashionCLIP
        </monospace>
        (
        <monospace>
         F-CLIP
        </monospace>
        ) nel compito di classificazione dei prodotti su diversi dataset (
        <bold>
         F1
        </bold>
        è
        <italic>
         weighted macro F1
        </italic>
        ).
       </p>
      </caption>
      <table frame="hsides" rules="groups">
       <thead>
        <tr>
         <th align="left">
          <p>
           Modello
          </p>
         </th>
         <th align="left">
          <p>
           Dataset
          </p>
         </th>
         <th align="left">
          <p>
           F1
          </p>
         </th>
        </tr>
       </thead>
       <tbody>
        <tr>
         <td align="left">
          <p>
           <monospace>
            F-CLIP
           </monospace>
          </p>
         </td>
         <td align="left" rowspan="2">
          <p>
           TEST
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.39
           </bold>
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.31
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            F-CLIP
           </monospace>
          </p>
         </td>
         <td align="left" rowspan="2">
          <p>
           KAGL
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.67
           </bold>
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.63
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            F-CLIP
           </monospace>
          </p>
         </td>
         <td align="left" rowspan="2">
          <p>
           F-MNIST
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.71
           </bold>
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.66
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            F-CLIP
           </monospace>
          </p>
         </td>
         <td align="left" rowspan="2">
          <p>
           DEEP
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.47
           </bold>
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.45
          </p>
         </td>
        </tr>
       </tbody>
      </table>
      <table-wrap-foot>
       <p>
        I modelli con le migliori prestazioni sono in grassetto.
       </p>
      </table-wrap-foot>
     </table-wrap>
    </p>
   </sec>
   <sec id="Sec7">
    <title>
     Classificazione zero-shot
    </title>
    <p id="Par25">
     Replichiamo l'impostazione originale di classificazione zero-shot di
     <monospace>
      CLIP
     </monospace>
     , che ci consente di valutare quantitativamente la trasferibilità delle rappresentazioni finemente sintonizzate di
     <monospace>
      FashionCLIP
     </monospace>
     a diverse distribuzioni di dati dallo stesso verticale (cioè Moda). Il modello genera
     <italic>
      un
     </italic>
     embedding dell'immagine per l'immagine del prodotto e
     <italic>
      k
     </italic>
     embedding di testo, uno per ciascuna delle etichette nello schema di classificazione (ad esempio, "scarpe", "camicia"). L'etichetta prevista è quella più vicina (misurata tramite prodotto scalare) all'immagine nello spazio vettoriale del modello. Utilizziamo
     <italic>
      weighted macro F1
     </italic>
     come metrica di performance. La Tabella
     <xref ref-type="table" rid="Tab2">
      2
     </xref>
     riassume i risultati di diversi benchmark SOTA. Su tutti i benchmark testati,
     <monospace>
      FashionCLIP
     </monospace>
     è superiore a
     <monospace>
      CLIP
     </monospace>
     , un risultato che suggerisce che il fine-tuning specifico per il dominio è effettivamente utile nel dominio e che si generalizza ad altri dataset completamente non visti
     <sup>
      <xref ref-type="bibr" rid="CR17">
       17
      </xref>
      <xref ref-type="bibr" rid="CR55">
       55
      </xref>
     </sup>
     .
    </p>
    <p id="Par26">
     Inoltre, ci proponiamo di investigare l'ipotesi del "cheating" sul nostro modello specifico per il dominio, cioè l'ipotesi che i modelli supervisionati non si generalizzino così bene come
     <monospace>
      CLIP
     </monospace>
     perché si adattano a caratteristiche spurie uniche per ciascun dataset. Congeliamo l'encoder delle immagini di
     <monospace>
      FashionCLIP
     </monospace>
     e affiniamo un classificatore lineare,
     <monospace>
      LINEAR
     </monospace>
     , sugli embedding generati su un sottoinsieme di categorie (47) dal set di validazione di
     <italic>
      Farfetch
     </italic>
     <sup/>
     . Eseguiamo benchmark su
     <inline-formula id="IEq23">
      <alternatives>
       <math id="IEq23_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi mathvariant="bold">
          TEST
         </mi>
         <mi mathvariant="bold">
          S
         </mi>
        </msub>
       </math>
       <tex-math id="IEq23_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$\mathbf {TEST_S}$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq23.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     ,
     <inline-formula id="IEq24">
      <alternatives>
       <math id="IEq24_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi mathvariant="bold">
          KAGL
         </mi>
         <mi mathvariant="bold">
          S
         </mi>
        </msub>
       </math>
       <tex-math id="IEq24_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$\mathbf {KAGL_S}$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq24.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     ,
     <inline-formula id="IEq25">
      <alternatives>
       <math id="IEq25_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi mathvariant="bold">
         F
        </mi>
       </math>
       <tex-math id="IEq25_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$\mathbf {F}$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq25.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     <bold>
      -
     </bold>
     <inline-formula id="IEq26">
      <alternatives>
       <math id="IEq26_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi mathvariant="bold">
          MNIST
         </mi>
         <mi mathvariant="bold">
          S
         </mi>
        </msub>
       </math>
       <tex-math id="IEq26_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$\mathbf {MNIST_S}$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq26.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     e
     <inline-formula id="IEq27">
      <alternatives>
       <math id="IEq27_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi mathvariant="bold">
          DEEP
         </mi>
         <mi mathvariant="bold">
          S
         </mi>
        </msub>
       </math>
       <tex-math id="IEq27_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$\mathbf {DEEP_S}$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq27.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     , versioni campionate dei rispettivi dataset. Dove le etichette sono diverse, adattiamo
     <monospace>
      LINEAR
     </monospace>
     alle etichette raggruppando i punteggi delle classi rilevanti. Confrontiamo questo con la performance zero-shot, utilizzando le etichette originali per generare gli embedding di testo.
    </p>
    <p id="Par27">
     La Tabella
     <xref ref-type="table" rid="Tab3">
      3
     </xref>
     riporta i nostri risultati, che sono parzialmente simili a quelli di
     <monospace>
      CLIP
     </monospace>
     <sup>
      <xref ref-type="bibr" rid="CR17">
       17
      </xref>
     </sup>
     . Dato che
     <bold>
      F-MNIST
     </bold>
     è molto diverso da
     <bold>
      TEST
     </bold>
     —comparabile, ad esempio, a CIFAR-100 vs. ImageNet—la diminuzione delle prestazioni può essere un'indicazione di cheating. Tuttavia,
     <monospace>
      LINEAR
     </monospace>
     si comporta bene sugli altri dataset, con il maggior guadagno per
     <bold>
      KAGL
     </bold>
     , la cui immagine del prodotto somiglia di più a quelle in
     <bold>
      TEST
     </bold>
     (cioè, articoli ad alta risoluzione su sfondo bianco). Rispetto all'impostazione originale, si potrebbe sostenere che il modello supervisionato ha un compito più facile nel nostro caso: molte meno categorie (
     <inline-formula id="IEq28">
      <alternatives>
       <math id="IEq28_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mn>
          10
         </mn>
         <mn>
          1
         </mn>
        </msup>
       </math>
       <tex-math id="IEq28_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$10^1$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq28.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     vs.
     <inline-formula id="IEq29">
      <alternatives>
       <math id="IEq29_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <msup>
          <mn>
           10
          </mn>
          <mn>
           3
          </mn>
         </msup>
         <mrow>
          <mo stretchy="false">
           )
          </mo>
         </mrow>
        </mrow>
       </math>
       <tex-math id="IEq29_TeX">
        \documentclass[12pt]{minimal}
                \usepackage{amsmath}
                \usepackage{wasysym}
                \usepackage{amsfonts}
                \usepackage{amssymb}
                \usepackage{amsbsy}
                \usepackage{mathrsfs}
                \usepackage{upgreek}
                \setlength{\oddsidemargin}{-69pt}
                \begin{document}$$10^3)$$\end{document}
       </tex-math>
       <inline-graphic href="41598_2022_23052_Article_IEq29.gif" mime-subtype="GIF" specific-use="web"/>
      </alternatives>
     </inline-formula>
     e articoli relativamente omogenei,
     <bold>
      F-MNIST
     </bold>
     a parte
     <sup>
      <xref ref-type="bibr" rid="CR56">
       56
      </xref>
      <xref ref-type="bibr" rid="CR57">
       57
      </xref>
      <xref ref-type="bibr" rid="CR17">
       17
      </xref>
     </sup>
     .
     <table-wrap id="Tab3">
      <label>
       Table 3
      </label>
      <caption xml:lang="en">
       <p>
        Prestazioni di classificazione
        <monospace>
         LINEAR
        </monospace>
        rispetto al zero-shot su
        <monospace>
         F-CLIP
        </monospace>
        (
        <bold>
         F1
        </bold>
        è
        <italic>
         weighted macro F1
        </italic>
        ).
       </p>
      </caption>
      <table frame="hsides" rules="groups">
       <thead>
        <tr>
         <th align="left">
          <p>
           Dataset
          </p>
         </th>
         <th align="left">
          <p>
           F-CLIP
          </p>
         </th>
         <th align="left">
          <p>
           LINEAR
          </p>
         </th>
         <th align="left">
          <p>
           <inline-formula id="IEq30">
            <alternatives>
             <math id="IEq30_Math" xmlns="http://www.w3.org/1998/Math/MathML">
              <mi mathvariant="normal">
               Δ
              </mi>
             </math>
             <tex-math id="IEq30_TeX">
              \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}
             </tex-math>
             <inline-graphic href="41598_2022_23052_Article_IEq30.gif" mime-subtype="GIF" specific-use="web"/>
            </alternatives>
           </inline-formula>
           F1
          </p>
         </th>
        </tr>
       </thead>
       <tbody>
        <tr>
         <td align="left">
          <p>
           <inline-formula id="IEq31">
            <alternatives>
             <math id="IEq31_Math" xmlns="http://www.w3.org/1998/Math/MathML">
              <msub>
               <mi mathvariant="monospace">
                TEST
               </mi>
               <mi mathvariant="monospace">
                S
               </mi>
              </msub>
             </math>
             <tex-math id="IEq31_TeX">
              \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\texttt {TEST}_\texttt {S}$$\end{document}
             </tex-math>
             <inline-graphic href="41598_2022_23052_Article_IEq31.gif" mime-subtype="GIF" specific-use="web"/>
            </alternatives>
           </inline-formula>
          </p>
         </td>
         <td align="left">
          <p>
           0.746
          </p>
         </td>
         <td align="left">
          <p>
           0.900
          </p>
         </td>
         <td align="left">
          <p>
           + 0.154
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <inline-formula id="IEq32">
            <alternatives>
             <math id="IEq32_Math" xmlns="http://www.w3.org/1998/Math/MathML">
              <msub>
               <mi mathvariant="monospace">
                KAGL
               </mi>
               <mi mathvariant="monospace">
                S
               </mi>
              </msub>
             </math>
             <tex-math id="IEq32_TeX">
              \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\texttt {KAGL}_\texttt {S}$$\end{document}
             </tex-math>
             <inline-graphic href="41598_2022_23052_Article_IEq32.gif" mime-subtype="GIF" specific-use="web"/>
            </alternatives>
           </inline-formula>
          </p>
         </td>
         <td align="left">
          <p>
           0.764
          </p>
         </td>
         <td align="left">
          <p>
           0.881
          </p>
         </td>
         <td align="left">
          <p>
           + 0.117
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <inline-formula id="IEq33">
            <alternatives>
             <math id="IEq33_Math" xmlns="http://www.w3.org/1998/Math/MathML">
              <msub>
               <mi mathvariant="monospace">
                DEEP
               </mi>
               <mi mathvariant="monospace">
                S
               </mi>
              </msub>
             </math>
             <tex-math id="IEq33_TeX">
              \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\texttt {DEEP}_\texttt {S}$$\end{document}
             </tex-math>
             <inline-graphic href="41598_2022_23052_Article_IEq33.gif" mime-subtype="GIF" specific-use="web"/>
            </alternatives>
           </inline-formula>
          </p>
         </td>
         <td align="left">
          <p>
           0.411
          </p>
         </td>
         <td align="left">
          <p>
           0.444
          </p>
         </td>
         <td align="left">
          <p>
           + 0.033
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <inline-formula id="IEq34">
            <alternatives>
             <math id="IEq34_Math" xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
               <mi mathvariant="monospace">
                F
               </mi>
               <mo>
                -
               </mo>
               <msub>
                <mi mathvariant="monospace">
                 MNIST
                </mi>
                <mi mathvariant="monospace">
                 S
                </mi>
               </msub>
              </mrow>
             </math>
             <tex-math id="IEq34_TeX">
              \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\texttt {F}-\texttt {MNIST}_\texttt {S}$$\end{document}
             </tex-math>
             <inline-graphic href="41598_2022_23052_Article_IEq34.gif" mime-subtype="GIF" specific-use="web"/>
            </alternatives>
           </inline-formula>
          </p>
         </td>
         <td align="left">
          <p>
           0.781
          </p>
         </td>
         <td align="left">
          <p>
           0.602
          </p>
         </td>
         <td align="left">
          <p>
           − 0.179
          </p>
         </td>
        </tr>
       </tbody>
      </table>
     </table-wrap>
     <table-wrap id="Tab4">
      <label>
       Table 4
      </label>
      <caption xml:lang="en">
       <p>
        F1 macro su
        <bold>
         STLE
        </bold>
        ;
        <monospace>
         Prior
        </monospace>
        classifica utilizzando le probabilità empiriche delle classi.
       </p>
      </caption>
      <table frame="hsides" rules="groups">
       <thead>
        <tr>
         <th align="left">
          <p>
           Modello
          </p>
         </th>
         <th align="left">
          <p>
           Uomo
          </p>
         </th>
         <th align="left">
          <p>
           Donna
          </p>
         </th>
        </tr>
       </thead>
       <tbody>
        <tr>
         <td align="left">
          <p>
           <monospace>
            Prior
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.24
          </p>
         </td>
         <td align="left">
          <p>
           0.20
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            F-CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.36
           </bold>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.27
           </bold>
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.33
          </p>
         </td>
         <td align="left">
          <p>
           0.17
          </p>
         </td>
        </tr>
       </tbody>
      </table>
      <table-wrap-foot>
       <p>
        I modelli con le migliori prestazioni sono in grassetto.
       </p>
      </table-wrap-foot>
     </table-wrap>
    </p>
    <p id="Par28">
     Mentre lasciamo l'indagine sulla classificazione della moda in contesti più ecologici come lavoro futuro, i nostri risultati contengono intuizioni praticabili per implementazioni nel mondo reale. In particolare, i classificatori supervisionati richiedono ancora un buon grado di intervento manuale anche per dataset simili, e sono completamente inutilizzabili su problemi vicini ma diversi. La Tabella
     <xref ref-type="table" rid="Tab4">
      4
     </xref>
     riporta le prestazioni su
     <bold>
      STLE
     </bold>
     divise per articoli relativi a Uomo e Donna. I prodotti nel dataset provengono ancora da
     <italic>
      Farfetch
     </italic>
     , ma le etichette sono assegnate manualmente dai merchandiser e sono ortogonali alla tassonomia (
     <italic>
      classico, streetwear, edgy
     </italic>
     vs.
     <italic>
      scarpe, cappelli, borse
     </italic>
     ). La versatilità offerta dalla supervisione linguistica consente ai modelli zero-shot di affrontare la sfida con una semplice ingegneria dei prompt ("un articolo in stile
     <italic>
      classico
     </italic>
     "); al contrario, i modelli supervisionati richiederebbero un nuovo pipeline di addestramento e valutazione. Come sottolineato sopra, l'apprendimento di concetti generali di moda è la motivazione principale dietro
     <italic>
      questo
     </italic>
     lavoro: mentre pipeline specifiche e supervisionate possono ancora essere la scelta migliore per problemi specifici, non sono più l'unica opzione praticabile in scenari multi-task grazie all'avvento di modelli su larga scala come
     <monospace>
      FashionCLIP
     </monospace>
     <sup/>
     . Sebbene nessuna risposta singola possa adattarsi a tutti i casi d'uso, desideriamo incoraggiare il processo decisionale basato sui dati tracciando tutte le opzioni e fornendo valutazioni di costi e prestazioni.
     <fig id="Fig5" position="float">
      <label>
       Figure 5
      </label>
      <caption xml:lang="en">
       <p>
        Confronto della proiezione T-SNE dello Spazio Immagini di
        <monospace>
         F-CLIP
        </monospace>
        e
        <monospace>
         CLIP
        </monospace>
        . Osserviamo un miglior raggruppamento (punteggio silhouette 0.115 vs 0.0745
        <sup>
         <xref ref-type="bibr" rid="CR58">
          58
         </xref>
        </sup>
        ) in
        <monospace>
         F-CLIP
        </monospace>
        per categorie come Camicie, Gonne e Vestiti, dove i prodotti formano un cluster più denso con meno sovrapposizione tra le categorie, suggerendo che lo spazio latente di
        <monospace>
         F-CLIP
        </monospace>
        è meglio adattato ai concetti di moda.
       </p>
      </caption>
      <graphic href="/ProjectMundo/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig5_HTML.png" id="MO7" mime-subtype="PNG" specific-use="web"/>
     </fig>
    </p>
   </sec>
   <sec id="Sec8">
    <title>
     Fondamento e composizionalità
    </title>
    <p id="Par29">
     Come argomentato nell'
     <italic>
      Introduzione
     </italic>
     , dato che siamo interessati a stabilire una connessione tra generalità e scalabilità attraverso modelli multi-modali di grandi dimensioni, è importante valutare ulteriormente la qualità delle rappresentazioni apprese. Mentre la questione se
     <monospace>
      FashionCLIP
     </monospace>
     <italic>
      apprenda
     </italic>
     la moda è stata affrontata quantitativamente sopra, siamo anche interessati a valutare il modello da una prospettiva teorica più ampia di comprensione del linguaggio, offrendo uno sguardo sull'estensione delle capacità di generalizzazione “vere” di
     <monospace>
      FashionCLIP
     </monospace>
     ,
     <italic>
      ala
     </italic>
     “uso infinito di mezzi finiti”
     <sup>
      <xref ref-type="bibr" rid="CR59">
       59
      </xref>
     </sup>
     .
    </p>
    <p id="Par30">
     La letteratura sulla composizionalità del linguaggio si estende per secoli: limitandoci solo a lavori recenti, il
     <italic>
      grounding
     </italic>
     è stato esplorato in connessione con l'apprendimento efficiente e la “vera comprensione”
     <sup>
      <xref ref-type="bibr" rid="CR60">
       60
      </xref>
      ,
      <xref ref-type="bibr" rid="CR61">
       61
      </xref>
      <xref ref-type="bibr" rid="CR62">
       62
      </xref>
      ,
      <xref ref-type="bibr" rid="CR63">
       63
      </xref>
     </sup>
     . L'uso di principi combinatori per testare le capacità di generalizzazione è una strategia nota nel mondo dei giocattoli: sfruttiamo intuizioni dal nostro dominio di riferimento per operazionalizzare principi simili su oggetti del
     <italic>
      mondo reale
     </italic>
     <sup>
      <xref ref-type="bibr" rid="CR64">
       64
      </xref>
      ,
      <xref ref-type="bibr" rid="CR65">
       65
      </xref>
     </sup>
     .
    </p>
    <p id="Par31">
     In questa sezione, forniamo prove di grounding semantico in
     <monospace>
      FashionCLIP
     </monospace>
     e su questa base offriamo un'indagine preliminare delle sue capacità composizionali. La nostra analisi parte da due lezioni della ricerca precedente. In primo luogo, le
     <italic>
      mappe di localizzazione
     </italic>
     sono un modo efficace per sondare il modello per la conoscenza
     <italic>
      referenziale
     </italic>
     (qui prendiamo in prestito la distinzione referenziale/inferenziale dal lavoro classico di Marconi) e la conoscenza lessicale visivamente radicata. In secondo luogo, dal punto di vista linguistico, la maggior parte delle query di ricerca nella moda ha la forma di Frasi Nominali (NPs)—ad esempio, “vestito armani”. Pertanto, la semantica delle NP può essere considerata una buona generalizzazione del mondo reale per studiare le capacità
     <monospace>
      composizionali
     </monospace>
     e
     <italic>
      inferenziali
     </italic>
     di
     <monospace>
      FashionCLIP
     </monospace>
     <sup>
      <xref ref-type="bibr" rid="CR66">
       66
      </xref>
      ,
      <xref ref-type="bibr" rid="CR67">
       67
      </xref>
      <xref ref-type="bibr" rid="CR68">
       68
      </xref>
      <xref ref-type="bibr" rid="CR69">
       69
      </xref>
      <xref ref-type="bibr" rid="CR9">
       9
      </xref>
      ,
      <xref ref-type="bibr" rid="CR70">
       70
      </xref>
     </sup>
     .
     <fig id="Fig6" position="float">
      <label>
       Figure 6
      </label>
      <caption xml:lang="en">
       <p>
        Conoscenza lessicale radicata. Le mappe sono sonde facili da usare nella conoscenza della moda del modello.
        <italic>
         Da sinistra a destra
        </italic>
        : mappa di localizzazione per "maniche lunghe" su un polo rosso; sneakers e la mappa per "Nike", una cover per telefono e la mappa per "Palm Angels"; la stessa cover per telefono e mappa, quando il logo è scritto con un font fuori distribuzione in un nuovo punto.
       </p>
      </caption>
      <graphic href="/ProjectMundo/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig6_HTML.png" id="MO8" mime-subtype="PNG" specific-use="web"/>
     </fig>
    </p>
    <sec id="Sec9">
     <title>
      Fondamento
     </title>
     <p id="Par32">
      Sondiamo
      <monospace>
       FashionCLIP
      </monospace>
      per prove di conoscenza referenziale e indaghiamo le sue capacità di grounding utilizzando mappe di localizzazione. Applichiamo ulteriormente le mappe di localizzazione al compito di parsing della moda zero-shot—un problema aperto cruciale nell'industria
      <sup>
       <xref ref-type="bibr" rid="CR71">
        71
       </xref>
      </sup>
      .
     </p>
     <p id="Par33">
      Le mappe di localizzazione si ottengono oscurando ripetutamente diverse parti dell'immagine. Codifichiamo quindi ciascuna versione oscurata e misuriamo la sua distanza dal testo target nello spazio contrastivo. Intuitivamente, più l'immagine viene allontanata dall'occultamento, più forte era il legame tra il concetto visivo rimosso e il testo e, a sua volta, più alto è il suo punteggio sulla mappa. Il parsing della moda è un caso specifico di segmentazione semantica in cui le annotazioni delle bounding box contengono articoli di abbigliamento. Estraggiamo le annotazioni delle bounding box (come un'approssimazione della segmentazione fine) dalle mappe di localizzazione trovando il rettangolo di delimitazione minimo delle aree altamente attivate.
     </p>
     <p id="Par34">
      Come mostrato nelle Figg.
      <xref ref-type="fig" rid="Fig6">
       6
      </xref>
      e
      <xref ref-type="fig" rid="Fig8">
       8
      </xref>
      , caratteristiche come "tacchi alti", "cinturino alla caviglia", "maniche lunghe" sono ben rappresentate in
      <monospace>
       FashionCLIP
      </monospace>
      ; il modello sembra anche essere molto consapevole dei marchi, in forma più o meno esplicita.
      <monospace>
       FashionCLIP
      </monospace>
      rileva il logo astratto sulle
      <italic>
       sneakers
      </italic>
      (Fig.
      <xref ref-type="fig" rid="Fig6">
       6
      </xref>
      ), oltre a mostrare (simile a
      <monospace>
       CLIP
      </monospace>
      ) buone capacità di OCR, quando riconosce un logo come stringa di testo esplicita. La Fig.
      <xref ref-type="fig" rid="Fig7">
       7
      </xref>
      mostra annotazioni di bounding box zero-shot di alcuni campioni nel dataset ModaNet precedentemente non visto. Sebbene sia improbabile che i modelli zero-shot possano sostituire l'addestramento specializzato per la segmentazione, crediamo che modelli come
      <monospace>
       FashionCLIP
      </monospace>
      potrebbero fornire un modo economico per generare etichette probabilistiche per pipeline di supervisione debole
      <sup>
       <xref ref-type="bibr" rid="CR72">
        72
       </xref>
      </sup>
      .
      <fig id="Fig7" position="float">
       <label>
        Figure 7
       </label>
       <caption xml:lang="en">
        <p>
         Rilevamento delle bounding-box degli oggetti. Le mappe di localizzazione possono essere facilmente estese per fornire bounding-box zero-shot per gli oggetti di interesse. Le bounding-box verdi mostrano le posizioni previste per i concetti di moda "Zaino" (sinistra) e "Cappello di paglia" (destra). Le immagini sopra sono tratte dal dataset pubblico Unsplash Lite Dataset 1.2.0: FashionCLIP è stato testato ampiamente su ModaNet - si prega di contattare gli autori per i link a queste immagini.
        </p>
       </caption>
       <graphic href="/ProjectMundo/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig7_HTML.png" id="MO9" mime-subtype="PNG" specific-use="web"/>
      </fig>
     </p>
    </sec>
    <sec id="Sec10">
     <title>
      Composizionalità
     </title>
     <p id="Par35">
      Data l'evidenza preliminare che i concetti isolati si mappano in modo affidabile su regioni visive, la nostra ipotesi di lavoro è che
      <monospace>
       FashionCLIP
      </monospace>
      dovrebbe esibire vere abilità
      <italic>
       inferenziali
      </italic>
      <italic>
       componendo
      </italic>
      tali concetti per generare nuovi NPs.
     </p>
     <p id="Par36">
      Ci basiamo sulla conoscenza del dominio, sulla letteratura precedente e sull'inventario di
      <italic>
       Farfetch
      </italic>
      per sondare il modello per la conoscenza di
      <italic>
       marchi
      </italic>
      (ad es. "nike"),
      <italic>
       caratteristiche
      </italic>
      ("tacchi alti") e
      <italic>
       disegni
      </italic>
      ("tastiera"), verificando manualmente la mappatura testo-regione per ciascuno di questi concetti tramite mappe di localizzazione. Dato che questi singoli concetti sono radicati in regioni (Fig.
      <xref ref-type="fig" rid="Fig8">
       8
      </xref>
      ), potremmo sfruttare questa conoscenza per generare nuove immagini e NPs
      <italic>
       sistematicamente
      </italic>
      <sup>
       <xref ref-type="bibr" rid="CR52">
        52
       </xref>
      </sup>
      . Fondamentale, possiamo assegnare una semantica definita a un nuovo NP
      <italic>
       marchio + oggetto
      </italic>
      che descrive un "oggetto improbabile" che non è mai stato visto prima (Fig.
      <xref ref-type="fig" rid="Fig9">
       9
      </xref>
      ). Gli oggetti improbabili variano: possono rappresentare combinazioni strane di concetti, come un
      <italic>
       abito lungo Nike
      </italic>
      , un oggetto surreale,
      <italic>
       sneakers con maniglie
      </italic>
      , o un'estensione improbabile di articoli di moda esistenti, come la
      <italic>
       pochette tastiera
      </italic>
      (che generalizza il tema trovato per la prima volta nella
      <italic>
       cravatta tastiera
      </italic>
      di J. Mugatu). Un nuovo NP come "abito nike" richiederebbe che la regione visiva corrispondente alla parola
      <italic>
       abito
      </italic>
      contenga la regione visiva del logo corrispondente alla parola
      <italic>
       nike
      </italic>
      <sup/>
      .
      <fig id="Fig8" position="float">
       <label>
        Figure 8
       </label>
       <caption xml:lang="en">
        <p>
         Grounding e composizionalità. Mappe di localizzazione per un prodotto recuperato con la query "sandali con cinturino alla caviglia e tacchi alti": da sinistra a destra, il prodotto, "cinturino alla caviglia", "sandali", "tacchi alti").
        </p>
       </caption>
       <graphic href="/ProjectMundo/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig8_HTML.jpg" id="MO10" mime-subtype="JPEG" specific-use="web"/>
      </fig>
     </p>
     <p id="Par37">
      Integriamo la nostra analisi riutilizzando la nostra pipeline di classificazione e recupero: nel compito di classificazione,
      <monospace>
       FashionCLIP
      </monospace>
      raggiunge un'accuratezza di 0.74 quando viene chiesto di scegliere l'etichetta improbabile da un insieme di distrattori credibili. I seguenti sono esempi di casi di test:
      <list list-type="bullet">
       <list-item>
        <p id="Par38">
         <italic>
          obiettivo
         </italic>
         :
         <italic>
          ABITO NIKE
         </italic>
         (come visto nella Fig.
         <xref ref-type="fig" rid="Fig9">
          9
         </xref>
         ),
         <italic>
          etichette
         </italic>
         : Abito Nike, un abito Armani, una camicia, la bandiera dell'Italia, un abito Gucci, una t-shirt Nike;
        </p>
       </list-item>
      </list>
      <fig id="Fig9" position="float">
       <label>
        Figure 9
       </label>
       <caption xml:lang="en">
        <p>
         Prodotti improbabili. Combinando caratteristiche di moda, marchi e articoli in modi nuovi, otteniamo prodotti visivamente realistici con chiari significati composizionali zero-shot. Da sinistra a destra: "abito lungo Nike", "converse con maniglie", "scarpe rosse con tacco alto nero", "pochette tastiera".
        </p>
       </caption>
       <graphic href="/ProjectMundo/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig9_HTML.png" id="MO11" mime-subtype="PNG" specific-use="web"/>
      </fig>
     </p>
     <p id="Par41">
      Sebbene un'indagine completa delle capacità composizionali sia al di là dello scopo di
      <italic>
       questo
      </italic>
      contributo, le inferenze di
      <monospace>
       FashionCLIP
      </monospace>
      su prodotti improbabili suggeriscono la presenza di
      <italic>
       un certo
      </italic>
      grado di composizionalità: concetti di moda importanti sono "identificabili" nello spazio latente e possono essere isolati e ricombinati in concetti non visti, esibendo su piccola scala la generalizzazione creativa che solitamente associamo ai sistemi simbolici
      <sup>
       <xref ref-type="bibr" rid="CR73">
        73
       </xref>
      </sup>
      . Inoltre, la capacità di distinguere "scarpe rosse con tacco nero" da "scarpe nere con tacco rosso" implica una conoscenza che va oltre una semantica di bag-of-words
      <sup>
       <xref ref-type="bibr" rid="CR74">
        74
       </xref>
      </sup>
      .
     </p>
     <p id="Par42">
      Ricerche recenti suggeriscono che le capacità composizionali di CLIP siano limitate.
      <sup>
       <xref ref-type="bibr" rid="CR75">
        75
       </xref>
      </sup>
      . Come mostrato dai nostri risultati, domini ristretti consentono una manipolazione diretta, senza il rischio di confusione; infatti, i domini ristretti possono essere più facili da esplorare ma è necessaria un'ulteriore indagine per confermare le capacità composizionali. Inoltre, come suggerito dall'uso dell'obiettivo MASKClip introdotto nel modello
      <italic>
       ARMANI
      </italic>
      , l'aggiunta di una segmentazione visiva esplicita può indurre una migliore discriminazione per certi concetti di moda. Sebbene perdite più costose siano un'area interessante all'intersezione tra grounding e composizionalità, dato sia il focus generativo ristretto che l'entità dei miglioramenti nel documento originale, le loro conclusioni non possono essere applicate direttamente a
      <monospace>
       FashionCLIP
      </monospace>
      <sup>
       <xref ref-type="bibr" rid="CR33">
        33
       </xref>
       <xref ref-type="bibr" rid="CR33">
        33
       </xref>
      </sup>
      . Attendiamo con interesse di condurre ricerche future combinando intuizioni da casi d'uso generativi e discriminativi.
     </p>
    </sec>
   </sec>
  </sec>
  <sec id="Sec11" sec-type="discussion">
   <title>
    Discussione
   </title>
   <p id="Par43">
    <monospace>
     FashionCLIP
    </monospace>
    è un adattamento di dominio di
    <monospace>
     CLIP
    </monospace>
    , motivato da casi d'uso centrali nella moda: diversamente dai metodi
    <italic>
     supervisionati specifici per compito
    </italic>
    ,
    <monospace>
     FashionCLIP
    </monospace>
    non necessita di un'architettura specializzata, etichettatura e tuning. Abbiamo verificato ampiamente la flessibilità offerta dalla supervisione linguistica e indagato le capacità semantiche di
    <monospace>
     FashionCLIP
    </monospace>
    su nuovi compiti. Il nostro focus su un'industria specifica consente non solo guadagni pratici ma apre anche possibilità teoriche restringendo il dominio, che è ancora ampio, ma anche facile da manipolare. Fornendo prove quantitative e qualitative che l'apprendimento contrastivo, unito a un dataset ampio e diversificato, può effettivamente produrre concetti industriali multi-modello generali, colleghiamo virtù teoriche con significativi guadagni pratici e apriamo nuove possibilità per scalare il dispiegamento orizzontale dei sistemi di apprendimento automatico in modo efficace
    <sup>
     <xref ref-type="bibr" rid="CR71">
      71
     </xref>
    </sup>
    .
   </p>
   <p id="Par44">
    Come sistema veramente generale, i concetti di
    <monospace>
     FashionCLIP
    </monospace>
    potrebbero essere utilizzati per molti più compiti: ad esempio, le rappresentazioni multi-modali possono essere caratteristiche in sistemi a valle, o utilizzate direttamente per raccomandazioni zero-shot in scenari item-to-item; la classificazione su etichette arbitrarie potrebbe essere utilizzata come un meccanismo di etichettatura veloce e scalabile, supportando l'etichettatura probabilistica o la generazione di dati per modelli IR multi-modali
    <sup>
     <xref ref-type="bibr" rid="CR76">
      76
     </xref>
     <xref ref-type="bibr" rid="CR77">
      77
     </xref>
     <xref ref-type="bibr" rid="CR78">
      78
     </xref>
    </sup>
    . Pur lasciando questo (e molti altri temi) a future iterazioni, crediamo che
    <italic>
     questo
    </italic>
    lavoro—con i suoi artefatti e metodologia—sia una prima valutazione completa del grande potenziale dei concetti multi-modali generali e trasferibili per il commercio digitale.
    <fig id="Fig10" position="float">
     <label>
      Figure 10
     </label>
     <caption xml:lang="en">
      <p>
       Attacco tipografico.
       <monospace>
        FashionCLIP
       </monospace>
       identifica correttamente l'oggetto a sinistra come una "mela", ma classifica erroneamente quello a destra come "nike air", poiché il testo agisce come un elemento di confusione
       <sup>
        <xref ref-type="bibr" rid="CR79">
         79
        </xref>
       </sup>
       .
      </p>
     </caption>
     <graphic href="/ProjectMundo/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig10_HTML.jpg" id="MO12" mime-subtype="JPEG" specific-use="web"/>
    </fig>
   </p>
   <p id="Par45">
    Gli autori sono consapevoli dei rischi dei modelli multi-modali simili a
    <italic>
     CLIP
    </italic>
    in produzione associati alla loro limitata robustezza, così come delle problematiche generali legate ai bias nei modelli di linguaggio di grandi dimensioni pre-addestrati su larga scala. In particolare, riconosciamo che il rischio di attacchi avversari sui modelli multi-modali è un'area di ricerca attiva
    <sup>
     <xref ref-type="bibr" rid="CR80">
      80
     </xref>
     ,
     <xref ref-type="bibr" rid="CR81">
      81
     </xref>
    </sup>
    . Nei limiti delle nostre conoscenze, non abbiamo motivo di credere che
    <monospace>
     FashionCLIP
    </monospace>
    introduca alcun rischio
    <italic>
     aggiuntivo
    </italic>
    rispetto al CLIP originale. Come con il modello originale, va notato che
    <monospace>
     FashionCLIP
    </monospace>
    sembra essere suscettibile agli "attacchi tipografici" (Fig.
    <xref ref-type="fig" rid="Fig10">
     10
    </xref>
    ). Nessun dataset utilizzato per l'addestramento o il test contiene PII e/o altri dati sensibili degli utenti.
   </p>
  </sec>
  <sec id="Sec12" sec-type="methods">
   <title>
    Metodi
   </title>
   <sec id="Sec13">
    <title>
     Dataset di addestramento
    </title>
    <p id="Par46">
     <italic>
      Farfetch
     </italic>
     ha reso disponibile per la prima volta un dataset in inglese comprendente oltre 800k prodotti di moda, con più di 3k marchi attraverso dozzine di tipi di oggetti. Rispetto ad altri grandi dataset di moda, il nostro dataset è significativamente più completo di DeepFashion, che manca di descrizioni testuali dettagliate, e persino più grande di CM-Fashion, che è stato raccolto senza alcun coinvolgimento diretto da parte di
     <italic>
      Farfetch
     </italic>
     <sup>
      <xref ref-type="bibr" rid="CR52">
       52
      </xref>
      <xref ref-type="bibr" rid="CR33">
       33
      </xref>
     </sup>
     . Gli articoli sono organizzati in alberi gerarchici, producendo una tassonomia a tre livelli: per esempio, gli
     <italic>
      alberi
     </italic>
     potrebbero essere qualcosa come
     <italic>
      Abbigliamento&gt; Vestiti&gt; Vestiti da giorno
     </italic>
     o
     <italic>
      Abbigliamento&gt; Cappotti&gt; Parka
     </italic>
     , per un totale di oltre 800 alberi. Come input per l'encoder di immagini, utilizziamo l'immagine standard del prodotto, che è una foto dell'articolo su uno sfondo bianco, senza persone (le immagini seguono un insieme specifico di regole riguardanti il posizionamento dell'articolo, le luci della foto, ecc., progettate per evidenziare le caratteristiche dell'articolo); per quanto riguarda il testo,
     <italic>
      Farfetch
     </italic>
     ha due tipi di testo,
     <italic>
      highlight
     </italic>
     (ad es., “strisce”, “maniche lunghe”, “Armani”) e una
     <italic>
      breve descrizione
     </italic>
     (“t-shirt in stile anni '80”). Vedi Fig.
     <xref ref-type="fig" rid="Fig3">
      3
     </xref>
     per un esempio.
    </p>
    <p id="Par47">
     Creiamo un set di addestramento, validazione e test dal catalogo campionando casualmente i prodotti. I nostri set finali di addestramento e validazione comprendono rispettivamente 700k e 50k prodotti da 188 categorie.
    </p>
   </sec>
   <sec id="Sec14">
    <title>
     Pipeline di addestramento
    </title>
    <p id="Par48">
     Applichiamo il fine-tuning partendo dal
     <monospace>
      CLIP
     </monospace>
     pre-addestrato con i seguenti parametri: utilizziamo l'Adam Optimizer con betas in (0.9, 0.98), epsilon di 1e−6 e weight decay pari a 0.2 e tre diversi tassi di apprendimento [1e−4, 1e−5, 1e−6]. Addestriamo i modelli per 4 epoche, valutiamo ogni 500 passi e selezioniamo il modello con la perdita di validazione più bassa per ogni configurazione (Tabella
     <xref ref-type="table" rid="Tab5">
      5
     </xref>
     , modello selezionato in grassetto). Nei nostri test preliminari, il modello con la perdita di validazione più bassa in generale non ha generalizzato al meglio nell'impostazione zero-shot. Questo pone una domanda interessante, lasciata per lavori futuri, su come effettuare il fine-tuning di questi grandi modelli pre-addestrati senza perdere in generalizzazione. La pipeline è stata implementata con Metaflow, con l'addestramento eseguito da remoto su GPU cloud; il tracciamento degli esperimenti è stato fornito da Comet
     <sup>
      <xref ref-type="bibr" rid="CR82">
       82
      </xref>
      <xref ref-type="bibr" rid="CR83">
       83
      </xref>
     </sup>
     .
    </p>
   </sec>
   <sec id="Sec15">
    <title>
     Dataset di test
    </title>
    <p id="Par49">
     Prepariamo i seguenti dataset per scopi di test e per valutare ulteriormente l'impatto potenziale del modello in produzione su larga scala.
     <bold>
      TEST
     </bold>
     è il set di test di
     <italic>
      Farfetch
     </italic>
     contenente 20k prodotti;
     <bold>
      HOUT-C
     </bold>
     è il dataset contenente una categoria che abbiamo escluso dall'addestramento (
     <italic>
      Performance Tops
     </italic>
     ), per un totale di 1.5k articoli;
     <bold>
      HOUT-B
     </bold>
     è il dataset contenente due marchi che sono stati esclusi dall'addestramento, per un totale di 1.7k articoli;
     <bold>
      STLE
     </bold>
     è un dataset di merchandising di
     <italic>
      Farfetch
     </italic>
     , completamente indipendente dal catalogo, che classifica 7749 articoli in 6 stili per il genere femminile e 4 stili per il genere maschile; esempi di stili sono
     <italic>
      Classico
     </italic>
     e
     <italic>
      Streetwear
     </italic>
     e ogni articolo può appartenere a più di uno stile;
     <bold>
      KAGL
     </bold>
     è un sottoinsieme di, dove ogni prodotto ha un'immagine su sfondo bianco, una didascalia e una categoria, per un totale di 9990 articoli su 62 categorie;
     <bold>
      F-MNIST
     </bold>
     contiene 10,000 immagini in scala di grigi da 10 classi di prodotti, con intensità dei pixel invertita per ottenere immagini con sfondo bianco (nota che queste immagini hanno una dimensione di 24 × 24 mostrando quindi molti meno dettagli rispetto alle immagini su cui i modelli sono stati addestrati).
     <bold>
      DEEP
     </bold>
     contiene 4000 immagini di prodotti che non sono standardizzate (cioè contengono persone) da 50 categorie
     <sup>
      <xref ref-type="bibr" rid="CR50">
       50
      </xref>
      <xref ref-type="bibr" rid="CR51">
       51
      </xref>
      <xref ref-type="bibr" rid="CR52">
       52
      </xref>
     </sup>
     .
    </p>
   </sec>
   <sec id="Sec16">
    <title>
     Addestramento
     <monospace>
      FashionCLIP
     </monospace>
    </title>
    <p id="Par50">
     Riutilizziamo l'architettura principale di
     <monospace>
      CLIP
     </monospace>
     , che descriviamo brevemente nell'
     <italic>
      Introduzione
     </italic>
     per completezza. Alla fine, otteniamo uno spazio multi-modale dove immagini e testi sono proiettati e appresi congiuntamente: se l'addestramento è stato efficace, ci aspettiamo che, per esempio, l'embedding testuale per la stringa “abito lungo rosso” sia effettivamente simile (misurato tramite il prodotto scalare) agli embedding delle immagini di abiti rossi. La Tabella
     <xref ref-type="table" rid="Tab5">
      5
     </xref>
     mostra il tempo di addestramento, le prestazioni e i costi
     <sup>
      <xref ref-type="bibr" rid="CR17">
       17
      </xref>
     </sup>
     .
     <table-wrap id="Tab5">
      <label>
       Table 5
      </label>
      <caption xml:lang="en">
       <p>
        Confronto tra tempo di addestramento, prestazioni, costi ed emissioni di carbonio su varianti dell'architettura
        <monospace>
         FashionCLIP
        </monospace>
        sul catalogo
        <italic>
         Farfetch
        </italic>
        .
       </p>
      </caption>
      <table frame="hsides" rules="groups">
       <thead>
        <tr>
         <th align="left">
          <p>
           LR
          </p>
         </th>
         <th align="left">
          <p>
           Loss
          </p>
         </th>
         <th align="left">
          <p>
           Tempo(m)
          </p>
         </th>
         <th align="left">
          <p>
           USD
          </p>
         </th>
         <th align="left">
          <p>
           kgCO
           <sub>
            2
           </sub>
           eq
          </p>
         </th>
        </tr>
       </thead>
       <tbody>
        <tr>
         <td align="left">
          <p>
           1e−4
          </p>
         </td>
         <td align="left">
          <p>
           16.0
          </p>
         </td>
         <td align="left">
          <p>
           618
          </p>
         </td>
         <td align="left">
          <p>
           31$
          </p>
         </td>
         <td align="left">
          <p>
           0.77
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           1e−5
          </p>
         </td>
         <td align="left">
          <p>
           1.73
          </p>
         </td>
         <td align="left">
          <p>
           617
          </p>
         </td>
         <td align="left">
          <p>
           31$
          </p>
         </td>
         <td align="left">
          <p>
           0.77
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <bold>
            1e−6
           </bold>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            2.83
           </bold>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            621
           </bold>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            31$
           </bold>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.78
           </bold>
          </p>
         </td>
        </tr>
       </tbody>
      </table>
      <table-wrap-foot>
       <p>
        Il costo è calcolato con il prezzo AWS per un
        <italic>
         p3.2xlarge
        </italic>
        ; le stime sono state condotte utilizzando il calcolatore di impatto del Machine Learning
        <sup>
         <xref ref-type="bibr" rid="CR84">
          84
         </xref>
        </sup>
        . Modello utilizzato per i test in grassetto.
       </p>
      </table-wrap-foot>
     </table-wrap>
    </p>
   </sec>
  </sec>
 </body>
 <back>
  <sec sec-type="author-contribution">
   <title>
    Contributi degli autori
   </title>
   <p>
    FashionCLIP è stato avviato da JT e FB, PC e GA hanno guidato l'implementazione e gli esperimenti; DG e ARM hanno preparato il dataset, eseguito l'EDA e fornito conoscenze di dominio; ST e CG hanno aiutato con il fine-tuning, la valutazione del modello e il background di ricerca. Tutti hanno contribuito alla bozza finale. JT e FB hanno agito come PI senior per il progetto.
   </p>
  </sec>
  <sec sec-type="data-availability">
   <title>
    Disponibilità dei dati
   </title>
   <p>
    The
    <bold>
     KAGL
    </bold>
    ,
    <bold>
     F-MNIST
    </bold>
    , and
    <bold>
     DEEP
    </bold>
    datasets are publicly available. The Farfetch dataset is scheduled to be released in the near future. As part of the ongoing mission to help the retail space leverage the latest A.I. techniques and to promote multidisciplinary research in data science across industries, Farfetch is working to finalize the release of the dataset used in this study under a research-friendly license. Please check
    <ext-link ext-link-type="uri" xlink:href="https://github.com/Farfetch">
     https://github.com/Farfetch
    </ext-link>
    for updates on the data release, and reach out to the authors for preliminary inquiries.
   </p>
  </sec>
  <sec sec-type="ethics-statement">
   <sec id="FPar1" sec-type="COI-statement">
    <title>
     Interessi in competizione
    </title>
    <p id="Par54">
     GA is a member of the Bocconi Institute of Data Science and Analytics (BIDSA) and the Data and Marketing Insights (DMI) unit. FB was a BIDSA and DMI member during the project. JT and CG were at Coveo Labs for the initial phase of the project. The authors declare no competing interests.
    </p>
   </sec>
  </sec>
  <ref-list id="Bib1">
   <title>
    Riferimenti
   </title>
   <ref-list>
    <ref id="CR1">
     <label>
      1.
     </label>
     <mixed-citation publication-type="other">
      Cramer-Flood, E. Global Ecommerce 2020. Ecommerce Decelerates amid Global Retail Contraction but Remains a Bright Spot. (2020).
     </mixed-citation>
    </ref>
    <ref id="CR2">
     <label>
      2.
     </label>
     <mixed-citation publication-type="other">
      McKinsey. The state of Fashion 2019. (2019).
     </mixed-citation>
    </ref>
    <ref id="CR3">
     <label>
      3.
     </label>
     <mixed-citation publication-type="other">
      de Souza Pereira Moreira, G., Jannach, D. &amp; da Cunha, A. M. On the importance of news content representation in hybrid neural session-based recommender systems. In
      <italic>
       INRA@RecSys
      </italic>
      (2019).
     </mixed-citation>
    </ref>
    <ref id="CR4">
     <label>
      4.
     </label>
     <mixed-citation publication-type="other">
      Guo, M.
      <italic>
       et al
      </italic>
      . Deep learning-based online alternative product recommendations at scale. In
      <italic>
       Proceedings of The 3rd Workshop on e-Commerce and NLP
      </italic>
      19–23 (Association for Computational Linguistics, Seattle, WA, USA, 2020).
      <ext-link ext-link-type="doi" xlink:href="10.18653/v1/2020.ecnlp-1.3">
       https://doi.org/10.18653/v1/2020.ecnlp-1.3
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR5">
     <label>
      5.
     </label>
     <mixed-citation publication-type="book">
      <person-group person-group-type="author">
       <name>
        <surname>
         Goncalves
        </surname>
        <given-names>
         D
        </given-names>
       </name>
       <etal/>
      </person-group>
      <person-group person-group-type="editor">
       <name>
        <surname>
         Dokoohaki
        </surname>
        <given-names>
         N
        </given-names>
       </name>
       <name>
        <surname>
         Jaradat
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <name>
        <surname>
         Corona Pampín
        </surname>
        <given-names>
         HJ
        </given-names>
       </name>
       <name>
        <surname>
         Shirvany
        </surname>
        <given-names>
         R
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       The importance of brand affinity in luxury fashion recommendations
      </article-title>
      <source>
       Recommender Systems in Fashion and Retail
      </source>
      <year>
       2021
      </year>
      <publisher-loc>
       Cham
      </publisher-loc>
      <publisher-name>
       Springer International Publishing
      </publisher-name>
      <fpage>
       3
      </fpage>
      <lpage>
       19
      </lpage>
      <pub-id pub-id-type="doi">
       10.1007/978-3-030-66103-8_1
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR6">
     <label>
      6.
     </label>
     <mixed-citation publication-type="other">
      Ai, Q. &amp; Narayanan, R. L. Model-agnostic vs. model-intrinsic interpretability for explainable product search. In
      <italic>
       Proceedings of the 30th ACM International Conference on Information and Knowledge Management, CIKM ’21
      </italic>
      5–15,
      <ext-link ext-link-type="doi" xlink:href="10.1145/3459637.3482276">
       https://doi.org/10.1145/3459637.3482276
      </ext-link>
      (Association for Computing Machinery, New York, NY, USA, 2021).
     </mixed-citation>
    </ref>
    <ref id="CR7">
     <label>
      7.
     </label>
     <mixed-citation publication-type="other">
      Chen, L., Chou, H., Xia, Y. &amp; Miyake, H. Multimodal item categorization fully based on transformer. In
      <italic>
       Proceedings of The 4th Workshop on e-Commerce and NLP
      </italic>
      111–115 (Association for Computational Linguistics, Online, 2021).
      <ext-link ext-link-type="doi" xlink:href="10.18653/v1/2021.ecnlp-1.13">
       https://doi.org/10.18653/v1/2021.ecnlp-1.13
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR8">
     <label>
      8.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Tsagkias
        </surname>
        <given-names>
         M
        </given-names>
       </name>
       <name>
        <surname>
         King
        </surname>
        <given-names>
         TH
        </given-names>
       </name>
       <name>
        <surname>
         Kallumadi
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <name>
        <surname>
         Murdock
        </surname>
        <given-names>
         V
        </given-names>
       </name>
       <name>
        <surname>
         de Rijke
        </surname>
        <given-names>
         M
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Challenges and research opportunities in ecommerce search and recommendations
      </article-title>
      <source>
       SIGIR Forum
      </source>
      <year>
       2021
      </year>
      <volume>
       54
      </volume>
      <fpage>
       1
      </fpage>
      <lpage>
       23
      </lpage>
      <pub-id pub-id-type="doi">
       10.1145/3451964.3451966
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR9">
     <label>
      9.
     </label>
     <mixed-citation publication-type="other">
      Bianchi, F., Tagliabue, J. &amp; Yu, B. Query2Prod2Vec: Grounded word embeddings for eCommerce. In
      <italic>
       Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers
      </italic>
      154–162 (Association for Computational Linguistics, Online, 2021).
      <ext-link ext-link-type="doi" xlink:href="10.18653/v1/2021.naacl-industry.20">
       https://doi.org/10.18653/v1/2021.naacl-industry.20
      </ext-link>
     </mixed-citation>
    </ref>
    <ref id="CR10">
     <label>
      10.
     </label>
     <mixed-citation publication-type="other">
      Tagliabue, J. &amp; Yu, B. Shopping in the multiverse: A counterfactual approach to in-session attribution. In
      <italic>
       Proceedings of the SIGIR 2020 Workshop on eCommerce
      </italic>
      (ECOM 20) (2020).
     </mixed-citation>
    </ref>
    <ref id="CR11">
     <label>
      11.
     </label>
     <mixed-citation publication-type="other">
      Sculley, D.
      <italic>
       et al
      </italic>
      . Hidden technical debt in machine learning systems. In
      <italic>
       NIPS
      </italic>
      (2015).
     </mixed-citation>
    </ref>
    <ref id="CR12">
     <label>
      12.
     </label>
     <mixed-citation publication-type="other">
      Tagliabue, J. You do not need a bigger boat: Recommendations at reasonable scale in a (mostly) serverless and open stack. In
      <italic>
       Fifteenth ACM Conference on Recommender Systems,
      </italic>
      RecSys ’21  598–600  (Association for Computing Machinery, New York, NY, USA, 2021).
      <ext-link ext-link-type="doi" xlink:href="10.1145/3460231.3474604">
       https://doi.org/10.1145/3460231.3474604
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR13">
     <label>
      13.
     </label>
     <mixed-citation publication-type="other">
      Techcrunch. Algolia finds $110M from Accel and Salesforce (2019).
     </mixed-citation>
    </ref>
    <ref id="CR14">
     <label>
      14.
     </label>
     <mixed-citation publication-type="other">
      Techcrunch. Bloomreach raises $150M on $900M valuation and acquires Exponea (2021).
     </mixed-citation>
    </ref>
    <ref id="CR15">
     <label>
      15.
     </label>
     <mixed-citation publication-type="other">
      Techcrunch. Lucidworks raises $100M to expand in AI finds (2019).
     </mixed-citation>
    </ref>
    <ref id="CR16">
     <label>
      16.
     </label>
     <mixed-citation publication-type="other">
      Marotta, S. Canada’s Latest Tech Public Debut Swings Amid Soft IPOs (2021).
      <ext-link ext-link-type="uri" xlink:href="https://www.bloomberg.com/news/articles/2021-11-25/canada-slatest-tech-public-debut-swings-amid-slew-of-soft-ipos">
       https://www.bloomberg.com/news/articles/2021-11-25/canada-slatest-tech-public-debut-swings-amid-slew-of-soft-ipos
      </ext-link>
      . Accessed 04-Nov-2022.
     </mixed-citation>
    </ref>
    <ref id="CR17">
     <label>
      17.
     </label>
     <mixed-citation publication-type="other">
      Radford, A.
      <italic>
       et al
      </italic>
      . Learning transferable visual models from natural language supervision. In
      <italic>
       ICML
      </italic>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR18">
     <label>
      18.
     </label>
     <mixed-citation publication-type="other">
      Shankar, S. Thoughts on ML
Engineering After a Year of my PhD—shreya-shankar.com. (2022).
      <ext-link ext-link-type="uri" xlink:href="https://www.shreyashankar.com/phd-year-one/">
       https://www.shreyashankar.com/phd-year-one/
      </ext-link>
      . Accessed 04-Nov-2022.
     </mixed-citation>
    </ref>
    <ref id="CR19">
     <label>
      19.
     </label>
     <mixed-citation publication-type="other">
      Bommasani, R. et al. On the opportunities and risks of foundation models. CoRR
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2108.07258">
       arXiv:2108.07258
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR20">
     <label>
      20.
     </label>
     <mixed-citation publication-type="other">
      Chen, T., Kornblith, S., Norouzi, M. &amp; Hinton, G. A simple framework for contrastive learning of visual representations. In
      <italic>
       International Conference on Machine Learning
      </italic>
      1597–1607 (PMLR, 2020).
     </mixed-citation>
    </ref>
    <ref id="CR21">
     <label>
      21.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Grill
        </surname>
        <given-names>
         J-B
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Bootstrap your own latent-a new approach to self-supervised learning
      </article-title>
      <source>
       Adv. Neural Inf. Process. Syst.
      </source>
      <year>
       2020
      </year>
      <volume>
       33
      </volume>
      <fpage>
       21271
      </fpage>
      <lpage>
       21284
      </lpage>
     </mixed-citation>
    </ref>
    <ref id="CR22">
     <label>
      22.
     </label>
     <mixed-citation publication-type="other">
      Iter, D., Guu, K., Lansing, L. &amp; Jurafsky, D. Pretraining with contrastive sentence objectives improves discourse performance of language models. In
      <italic>
       Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
      </italic>
      4859–4870 (Association for Computational Linguistics, Online, 2020).
      <ext-link ext-link-type="doi" xlink:href="10.18653/v1/2020.acl-main.439">
       https://doi.org/10.18653/v1/2020.acl-main.439
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR23">
     <label>
      23.
     </label>
     <mixed-citation publication-type="other">
      Su, Y.
      <italic>
       et al
      </italic>
      . A contrastive framework for neural text generation. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2202.06417">
       arXiv:2202.06417
      </ext-link>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR24">
     <label>
      24.
     </label>
     <mixed-citation publication-type="other">
      Jia, C.
      <italic>
       et al
      </italic>
      . Scaling up visual and vision-language representation learning with noisy text supervision. In
      <italic>
       International Conference on Machine Learning
      </italic>
      4904–4916 (PMLR, 2021).
     </mixed-citation>
    </ref>
    <ref id="CR25">
     <label>
      25.
     </label>
     <mixed-citation publication-type="other">
      Mu, N., Kirillov, A., Wagner, D. &amp; Xie, S. Slip: Self-supervision meets language-image pre-training. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2112.12750">
       arXiv:2112.12750
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR26">
     <label>
      26.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Schneider
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <name>
        <surname>
         Baevski
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <name>
        <surname>
         Collobert
        </surname>
        <given-names>
         R
        </given-names>
       </name>
       <name>
        <surname>
         Auli
        </surname>
        <given-names>
         M
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       wav2vec: Unsupervised pre-training for speech recognition
      </article-title>
      <source>
       Proc. Interspeech
      </source>
      <year>
       2019
      </year>
      <volume>
       2019
      </volume>
      <fpage>
       3465
      </fpage>
      <lpage>
       3469
      </lpage>
     </mixed-citation>
    </ref>
    <ref id="CR27">
     <label>
      27.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Baevski
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <name>
        <surname>
         Zhou
        </surname>
        <given-names>
         Y
        </given-names>
       </name>
       <name>
        <surname>
         Mohamed
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <name>
        <surname>
         Auli
        </surname>
        <given-names>
         M
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       wav2vec 2.0: A framework for self-supervised learning of speech representations
      </article-title>
      <source>
       Adv. Neural Inf. Process. Syst.
      </source>
      <year>
       2020
      </year>
      <volume>
       33
      </volume>
      <fpage>
       12449
      </fpage>
      <lpage>
       12460
      </lpage>
     </mixed-citation>
    </ref>
    <ref id="CR28">
     <label>
      28.
     </label>
     <mixed-citation publication-type="other">
      Nagrani, A.
      <italic>
       et al
      </italic>
      . Learning audio-video modalities from image captions. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2204.00679">
       arXiv:2204.00679
      </ext-link>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR29">
     <label>
      29.
     </label>
     <mixed-citation publication-type="other">
      Shvetsova, N.
      <italic>
       et al
      </italic>
      . Everything at once–multi-modal fusion transformer for video retrieval. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2112.04446">
       arXiv:2112.04446
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR30">
     <label>
      30.
     </label>
     <mixed-citation publication-type="other">
      Minderer, M.
      <italic>
       et al
      </italic>
      . Simple open-vocabulary object detection with vision transformers. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2205.06230">
       arXiv:2205.06230
      </ext-link>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR31">
     <label>
      31.
     </label>
     <mixed-citation publication-type="other">
      Liu, H.
      <italic>
       et al
      </italic>
      . Cma-clip: Cross-modality attention clip for image-text classification. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2112.03562">
       arXiv:2112.03562
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR32">
     <label>
      32.
     </label>
     <mixed-citation publication-type="other">
      Sevegnani, K.
      <italic>
       et al
      </italic>
      . Contrastive learning for interactive recommendation in fashion. CoRR
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2207.12033">
       arXiv:2207.12033
      </ext-link>
      ,
      <ext-link ext-link-type="doi" xlink:href="10.48550/arXiv.2207.12033">
       https://doi.org/10.48550/arXiv.2207.12033
      </ext-link>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR33">
     <label>
      33.
     </label>
     <mixed-citation publication-type="other">
      Zhang, X.
      <italic>
       et al
      </italic>
      . Armani: Part-level garment-text alignment for unified cross-modal fashion design. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2208.05621">
       arXiv:2208.05621
      </ext-link>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR34">
     <label>
      34.
     </label>
     <mixed-citation publication-type="other">
      Ramesh, A., Dhariwal, P., Nichol, A., Chu, C. &amp; Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2204.06125">
       arXiv:2204.06125
      </ext-link>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR35">
     <label>
      35.
     </label>
     <mixed-citation publication-type="other">
      Kong, C., Jeon, D., Kwon, O. &amp; Kwak, N. Leveraging off-the-shelf diffusion model for multi-attribute fashion image manipulation. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2210.05872">
       arXiv:2210.05872
      </ext-link>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR36">
     <label>
      36.
     </label>
     <mixed-citation publication-type="other">
      Gupta, V., Karnick, H., Bansal, A. &amp; Jhala, P. Product classification in e-commerce using distributional semantics. In
      <italic>
       COLING
      </italic>
      (2016).
     </mixed-citation>
    </ref>
    <ref id="CR37">
     <label>
      37.
     </label>
     <mixed-citation publication-type="other">
      Fu, J.
      <italic>
       et al
      </italic>
      . Cma-clip: Cross-modality attention clip for text-image classification. In
      <italic>
       IEEE ICIP 2022
      </italic>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR38">
     <label>
      38.
     </label>
     <mixed-citation publication-type="other">
      Commerce, B. How Ecommerce Site Search Can Create a Competitive Advantage. (2021).
      <ext-link ext-link-type="uri" xlink:href="https://www.bigcommerce.com/articles/ecommerce/site-search/#the-effectiveness-of-ecommerce-site-search-">
       https://www.bigcommerce.com/articles/ecommerce/site-search/#the-effectiveness-of-ecommerce-site-search-
      </ext-link>
      . Accessed 04-Nov-2022.
     </mixed-citation>
    </ref>
    <ref id="CR39">
     <label>
      39.
     </label>
     <mixed-citation publication-type="other">
      Alaimo, D. 87% of shoppers now begin product searches online. (2018).
     </mixed-citation>
    </ref>
    <ref id="CR40">
     <label>
      40.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Robertson
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <name>
        <surname>
         Zaragoza
        </surname>
        <given-names>
         H
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       The probabilistic relevance framework: Bm25 and beyond
      </article-title>
      <source>
       Found. Trends Inf. Retr.
      </source>
      <year>
       2009
      </year>
      <volume>
       3
      </volume>
      <fpage>
       333
      </fpage>
      <lpage>
       389
      </lpage>
      <pub-id pub-id-type="doi">
       10.1561/1500000019
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR41">
     <label>
      41.
     </label>
     <mixed-citation publication-type="other">
      Gillick, D., Presta, A. &amp; Tomar, G. S. End-to-end retrieval in continuous space. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1811.08008">
       arXiv:1811.08008
      </ext-link>
      (2018).
     </mixed-citation>
    </ref>
    <ref id="CR42">
     <label>
      42.
     </label>
     <mixed-citation publication-type="other">
      Izacard, G.
      <italic>
       et al.
      </italic>
      Towards unsupervised dense information retrieval with contrastive learning. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2112.09118">
       arXiv:2112.09118
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR43">
     <label>
      43.
     </label>
     <mixed-citation publication-type="other">
      Hu, Y., Da, Q., Zeng, A., Yu, Y. &amp; Xu, Y. Reinforcement learning to rank in e-commerce search engine: Formalization, analysis, and application. In
      <italic>
       Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’18
      </italic>
      , 368–377  (Association for Computing Machinery, New York, NY, USA, 2018).
      <ext-link ext-link-type="doi" xlink:href="10.1145/3219819.3219846">
       https://doi.org/10.1145/3219819.3219846
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR44">
     <label>
      44.
     </label>
     <mixed-citation publication-type="other">
      Chen, L. &amp; Miyake, H. Label-guided learning for item categorization in e-commerce. In
      <italic>
       Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers
      </italic>
      296–303 (Association for Computational Linguistics, Online, 2021).
      <ext-link ext-link-type="doi" xlink:href="10.18653/v1/2021.naacl-industry.37">
       https://doi.org/10.18653/v1/2021.naacl-industry.37
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR45">
     <label>
      45.
     </label>
     <mixed-citation publication-type="other">
      Costin, A. M., Eastman, C. &amp; Issa, R. R. A. The Need for Taxonomies in the Ontological Approach for Interoperability of Heterogeneous Information Models 9–17 (2017).
      <ext-link ext-link-type="uri" xlink:href="https://ascelibrary.org/doi/pdf/10.1061/9780784480830.002">
       https://ascelibrary.org/doi/pdf/10.1061/9780784480830.002
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR46">
     <label>
      46.
     </label>
     <mixed-citation publication-type="other">
      McDowell, M. Taxonomy is the new fashion-tech essential. (2020).
      <ext-link ext-link-type="uri" xlink:href="https://www.voguebusiness.com/technology/taxonomy-is-the-new-fashion-tech-essential-theyes">
       https://www.voguebusiness.com/technology/taxonomy-is-the-new-fashion-tech-essential-theyes
      </ext-link>
      .
Accessed 04-Nov-2022.
     </mixed-citation>
    </ref>
    <ref id="CR47">
     <label>
      47.
     </label>
     <mixed-citation publication-type="other">
      Feizi, S., Singla, S. Salient imagenet: how to discover spurious features in deep learning? In
      <italic>
       International Conference on Learning Representations
      </italic>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR48">
     <label>
      48.
     </label>
     <mixed-citation publication-type="other">
      Chia, P. J. Fashion CLIP.
      <ext-link ext-link-type="uri" xlink:href="https://github.com/patrickjohncyh/fashion-clip/">
       https://github.com/patrickjohncyh/fashion-clip/
      </ext-link>
      (2022). [Online; accessed 15-September-2022].
     </mixed-citation>
    </ref>
    <ref id="CR49">
     <label>
      49.
     </label>
     <mixed-citation publication-type="other">
      Chia, P. J. GradREC.
      <ext-link ext-link-type="uri" xlink:href="https://github.com/patrickjohncyh/gradient-recs/">
       https://github.com/patrickjohncyh/gradient-recs/
      </ext-link>
      (2022). [Online; accessed 15-September-2022].
     </mixed-citation>
    </ref>
    <ref id="CR50">
     <label>
      50.
     </label>
     <mixed-citation publication-type="other">
      Aggarwal, P. Fashion Product Images Dataset. (2020).
     </mixed-citation>
    </ref>
    <ref id="CR51">
     <label>
      51.
     </label>
     <mixed-citation publication-type="other">
      Xiao, H., Rasul, K. &amp; Vollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/cs.LG1708.07747">
       arXiv:cs.LG/1708.07747
      </ext-link>
      (2017).
     </mixed-citation>
    </ref>
    <ref id="CR52">
     <label>
      52.
     </label>
     <mixed-citation publication-type="other">
      Liu, Z., Luo, P., Qiu, S., Wang, X. &amp; Tang, X. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In
      <italic>
       Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
      </italic>
      (2016).
     </mixed-citation>
    </ref>
    <ref id="CR53">
     <label>
      53.
     </label>
     <mixed-citation publication-type="other">
      Cremonesi, P., Koren, Y. &amp; Turrin, R. Performance of recommender algorithms on top-n recommendation tasks. In
      <italic>
       Proceedings of the Fourth ACM Conference on Recommender Systems, RecSys ’10
      </italic>
      , 39–46  (Association for Computing Machinery, New York, NY, USA, 2010).
      <ext-link ext-link-type="doi" xlink:href="10.1145/1864708.1864721">
       https://doi.org/10.1145/1864708.1864721
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR54">
     <label>
      54.
     </label>
     <mixed-citation publication-type="book">
      <person-group person-group-type="author">
       <name>
        <surname>
         Craswell
        </surname>
        <given-names>
         N
        </given-names>
       </name>
      </person-group>
      <source>
       Mean Reciprocal Rank
      </source>
      <year>
       2009
      </year>
      <publisher-loc>
       US, Boston, MA
      </publisher-loc>
      <publisher-name>
       Springer
      </publisher-name>
      <fpage>
       1703
      </fpage>
      <lpage>
       1703
      </lpage>
     </mixed-citation>
    </ref>
    <ref id="CR55">
     <label>
      55.
     </label>
     <mixed-citation publication-type="other">
      Manning, C. D., Raghavan, P. &amp; Schütze, H.
      <italic>
       An Introduction to Information Retrieval
      </italic>
      Online. (Cambridge University Press, Cambridge, UK, 2009).
     </mixed-citation>
    </ref>
    <ref id="CR56">
     <label>
      56.
     </label>
     <mixed-citation publication-type="other">
      Krizhevsky, A.
      <italic>
       Learning Multiple Layers of Features from Tiny Images
      </italic>
      Tech Rep, (2009).
     </mixed-citation>
    </ref>
    <ref id="CR57">
     <label>
      57.
     </label>
     <mixed-citation publication-type="other">
      Deng, J.
      <italic>
       et al.
      </italic>
      Imagenet: A large-scale hierarchical image database. In
      <italic>
       2009 IEEE Conference on Computer Vision and Pattern Recognition
      </italic>
      248–255 (IEEE, 2009).
     </mixed-citation>
    </ref>
    <ref id="CR58">
     <label>
      58.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Rousseeuw
        </surname>
        <given-names>
         PJ
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Silhouettes: A graphical aid to the interpretation and validation of cluster analysis
      </article-title>
      <source>
       J. Comput. Appl. Math.
      </source>
      <year>
       1987
      </year>
      <volume>
       20
      </volume>
      <fpage>
       53
      </fpage>
      <lpage>
       65
      </lpage>
      <pub-id pub-id-type="doi">
       10.1016/0377-0427(87)90125-7
      </pub-id>
      <pub-id assigning-authority="Zentralblatt MATH" pub-id-type="other">
       0636.62059
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR59">
     <label>
      59.
     </label>
     <mixed-citation publication-type="other">
      von Humboldt, W. On language: On the diversity of human language construction and its influence on the mental development of the human species. (1836/1999).
     </mixed-citation>
    </ref>
    <ref id="CR60">
     <label>
      60.
     </label>
     <mixed-citation publication-type="other">
      Yu, H., Zhang, H. &amp; Xu, W. Interactive grounded language acquisition and generalization in a 2d world. In
      <italic>
       ICLR
      </italic>
      (2018).
     </mixed-citation>
    </ref>
    <ref id="CR61">
     <label>
      61.
     </label>
     <mixed-citation publication-type="other">
      Chevalier-Boisvert, M.
      <italic>
       et al
      </italic>
      . BabyAI: A platform to study the sample efficiency of grounded language learning. In
      <italic>
       ICLR
      </italic>
      (2019).
     </mixed-citation>
    </ref>
    <ref id="CR62">
     <label>
      62.
     </label>
     <mixed-citation publication-type="other">
      Bender, E. M. &amp; Koller, A. Climbing towards NLU: On meaning, form, and understanding in the age of data. In
      <italic>
       Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
      </italic>
      5185–5198  (Association for Computational Linguistics, Online, 2020).
      <ext-link ext-link-type="doi" xlink:href="10.18653/v1/2020.acl-main.463">
       https://doi.org/10.18653/v1/2020.acl-main.463
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR63">
     <label>
      63.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Merrill
        </surname>
        <given-names>
         WC
        </given-names>
       </name>
       <name>
        <surname>
         Goldberg
        </surname>
        <given-names>
         Y
        </given-names>
       </name>
       <name>
        <surname>
         Schwartz
        </surname>
        <given-names>
         R
        </given-names>
       </name>
       <name>
        <surname>
         Smith
        </surname>
        <given-names>
         NA
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Provable limitations of acquiring meaning from ungrounded form: What will future language models understand?
      </article-title>
      <source>
       Trans. Assoc. Comput. Linguist.
      </source>
      <year>
       2021
      </year>
      <volume>
       9
      </volume>
      <fpage>
       1047
      </fpage>
      <lpage>
       1060
      </lpage>
      <pub-id pub-id-type="doi">
       10.1162/tacl_a_00412
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR64">
     <label>
      64.
     </label>
     <mixed-citation publication-type="other">
      Chollet, F. On the measure of intelligence. ArXiv
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1911.01547">
       arXiv:1911.01547
      </ext-link>
      (2019).
     </mixed-citation>
    </ref>
    <ref id="CR65">
     <label>
      65.
     </label>
     <mixed-citation publication-type="other">
      Gandhi, K., Stojnić, G., Lake, B. M. &amp; Dillon, M. R. Baby intuitions benchmark (bib): Discerning the goals, preferences, and actions of others.
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2102.11938">
       arXiv:cs.LG/2102.11938
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR66">
     <label>
      66.
     </label>
     <mixed-citation publication-type="other">
      Fong, R. C. &amp; Vedaldi, A. Interpretable explanations of black boxes by meaningful perturbation. In
      <italic>
       Proceedings of the IEEE International Conference on Computer Vision (ICCV)
      </italic>
      (2017).
     </mixed-citation>
    </ref>
    <ref id="CR67">
     <label>
      67.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Covert
        </surname>
        <given-names>
         I
        </given-names>
       </name>
       <name>
        <surname>
         Lundberg
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <name>
        <surname>
         Lee
        </surname>
        <given-names>
         S-I
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Explaining by removing: A unified framework for model explanation
      </article-title>
      <source>
       J. Mach. Learn. Res.
      </source>
      <year>
       2021
      </year>
      <volume>
       22
      </volume>
      <fpage>
       1
      </fpage>
      <lpage>
       90
      </lpage>
      <pub-id assigning-authority="Zentralblatt MATH" pub-id-type="other">
       07626724
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR68">
     <label>
      68.
     </label>
     <mixed-citation publication-type="other">
      Bianchi, F.
      <italic>
       et al
      </italic>
      . Contrastive language-image pre-training for the italian language. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2108.08688">
       arXiv:2108.08688
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR69">
     <label>
      69.
     </label>
     <mixed-citation publication-type="book">
      <person-group person-group-type="author">
       <name>
        <surname>
         Marconi
        </surname>
        <given-names>
         D
        </given-names>
       </name>
      </person-group>
      <source>
       Lexical Competence
      </source>
      <year>
       1997
      </year>
      <publisher-loc>
       Cambridge, MA
      </publisher-loc>
      <publisher-name>
       MIT Press
      </publisher-name>
     </mixed-citation>
    </ref>
    <ref id="CR70">
     <label>
      70.
     </label>
     <mixed-citation publication-type="other">
      Bianchi, F., Greco, C. &amp; Tagliabue, J. Language in a (search) box: Grounding language learning in real-world human-machine interaction. In
      <italic>
       Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
      </italic>
      4409–4415  (Association for Computational Linguistics, Online, 2021).
      <ext-link ext-link-type="doi" xlink:href="10.18653/v1/2021.naacl-main.348">
       https://doi.org/10.18653/v1/2021.naacl-main.348
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR71">
     <label>
      71.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Cheng
        </surname>
        <given-names>
         W-H
        </given-names>
       </name>
       <name>
        <surname>
         Song
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <name>
        <surname>
         Chen
        </surname>
        <given-names>
         C-Y
        </given-names>
       </name>
       <name>
        <surname>
         Hidayati
        </surname>
        <given-names>
         SC
        </given-names>
       </name>
       <name>
        <surname>
         Liu
        </surname>
        <given-names>
         J
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Fashion meets computer vision: A survey
      </article-title>
      <source>
       ACM Comput. Surv.
      </source>
      <year>
       2021
      </year>
      <pub-id pub-id-type="doi">
       10.1145/3447239
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR72">
     <label>
      72.
     </label>
     <mixed-citation publication-type="other">
      Zheng, S., Yang, F., Kiapour, M. H. &amp; Piramuthu, R. Modanet: A large-scale street fashion dataset with polygon annotations. In
      <italic>
       Proceedings of the 26th ACM international conference on Multimedia
      </italic>
      1670–1678 (2018).
     </mixed-citation>
    </ref>
    <ref id="CR73">
     <label>
      73.
     </label>
     <mixed-citation publication-type="book">
      <person-group person-group-type="author">
       <name>
        <surname>
         Chierchia
        </surname>
        <given-names>
         G
        </given-names>
       </name>
       <name>
        <surname>
         McConnell-Ginet
        </surname>
        <given-names>
         S
        </given-names>
       </name>
      </person-group>
      <source>
       Meaning and Grammar: An Introduction to Semantics
      </source>
      <year>
       2000
      </year>
      <edition>
       2
      </edition>
      <publisher-loc>
       Cambridge, MA, USA
      </publisher-loc>
      <publisher-name>
       MIT Press
      </publisher-name>
     </mixed-citation>
    </ref>
    <ref id="CR74">
     <label>
      74.
     </label>
     <mixed-citation publication-type="other">
      Pham, T. M., Bui, T., Mai, L. &amp; Nguyen, A. M. Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks?
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2012.15180">
       arXiv:cs.LG/2012.15180
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR75">
     <label>
      75.
     </label>
     <mixed-citation publication-type="other">
      Thrush, T.
      <italic>
       et al
      </italic>
      . Winoground: Probing vision and language models for visio-linguistic compositionality. In
      <italic>
       CVPR
      </italic>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR76">
     <label>
      76.
     </label>
     <mixed-citation publication-type="other">
      Chia, P. J., Tagliabue, J., Bianchi, F., Greco, C. &amp; Goncalves, D. “does it come in black?” clip-like models are zero-shot recommenders. In
      <italic>
       Proceedings of The 5th Workshop on e-Commerce and NLP
      </italic>
      (Association for Computational Linguistics, 2022).
     </mixed-citation>
    </ref>
    <ref id="CR77">
     <label>
      77.
     </label>
     <mixed-citation publication-type="other">
      Ratner, A. J.
      <italic>
       et al
      </italic>
      . Snorkel: Rapid training data creation with weak supervision. In
      <italic>
       Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases
      </italic>
      Vol. 11 3, 269–282 (2017).
     </mixed-citation>
    </ref>
    <ref id="CR78">
     <label>
      78.
     </label>
     <mixed-citation publication-type="other">
      Yu, B., Tagliabue, J., Greco, C. &amp; Bianchi, F. “an image is worth a thousand features”: Scalable product representations for in-session type-ahead personalization. In
      <italic>
       Companion Proceedings of the Web Conference 2020, WWW ’20
      </italic>
      461–470  (Association for Computing Machinery, New York, NY, USA, 2020).
      <ext-link ext-link-type="doi" xlink:href="10.1145/3366424.3386198">
       https://doi.org/10.1145/3366424.3386198
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR79">
     <label>
      79.
     </label>
     <mixed-citation publication-type="other">
      Vincent, J. OpenAI’s state-of-the-art machine vision AI is fooled by handwritten notes. (2021).
      <ext-link ext-link-type="uri" xlink:href="https://www.theverge.com/2021/3/8/22319173/openai-machine-visionadversarial-typographic-attacka-clip-multimodal-neuron">
       https://www.theverge.com/2021/3/8/22319173/openai-machine-visionadversarial-typographic-attacka-clip-multimodal-neuron
      </ext-link>
      . Accessed 04-Nov-2022.
     </mixed-citation>
    </ref>
    <ref id="CR80">
     <label>
      80.
     </label>
     <mixed-citation publication-type="other">
      Noever, D. A. &amp; Noever, S. E. M. Reading isn’t believing: Adversarial attacks on multi-modal neurons. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2103.10480">
       arXiv:2103.10480
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR81">
     <label>
      81.
     </label>
     <mixed-citation publication-type="other">
      Yu, Y., Lee, H. J., Kim, B. C., Kim, J. U. &amp; Ro, Y. M. Investigating vulnerability to adversarial examples on multimodal data fusion in deep learning.
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2005.10987">
       arXiv:2005.10987
      </ext-link>
      (2020).
     </mixed-citation>
    </ref>
    <ref id="CR82">
     <label>
      82.
     </label>
     <mixed-citation publication-type="other">
      Berg, D.
      <italic>
       et al
      </italic>
      . Open-Sourcing Metaflow, a Human-Centric Framework for Data Science (2019).
      <ext-link ext-link-type="uri" xlink:href="https://netflixtechblog.com/open-sourcing-metaflow-a-human-centricframework-for-data-science-fa72e04a5d9">
       https://netflixtechblog.com/open-sourcing-metaflow-a-human-centricframework-for-data-science-fa72e04a5d9
      </ext-link>
      . Accessed 04-Nov-2022.
     </mixed-citation>
    </ref>
    <ref id="CR83">
     <label>
      83.
     </label>
     <mixed-citation publication-type="other">
      Comet.ML. Comet.ML home page (2021).
     </mixed-citation>
    </ref>
    <ref id="CR84">
     <label>
      84.
     </label>
     <mixed-citation publication-type="other">
      Lacoste, A., Luccioni, A., Schmidt, V. &amp; Dandres, T. Quantifying the carbon emissions of machine learning. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1910.09700">
       arXiv:1910.09700
      </ext-link>
      (2019).
     </mixed-citation>
    </ref>
   </ref-list>
  </ref-list>
  <notes notes-type="Misc">
   <title>
    Nota dell'editore
   </title>
   <p>
    Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
   </p>
  </notes>
 </back>
</article>
