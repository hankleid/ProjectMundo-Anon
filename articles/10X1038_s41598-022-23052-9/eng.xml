<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type='text/xsl' href='/ProjectMundo-Anon/style/jats-html.xsl'?>
<!DOCTYPE response>
<article article-type="research-article" dtd-version="1.2" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
 <front>
  <journal-meta>
   <journal-id journal-id-type="publisher-id">
    41598
   </journal-id>
   <journal-id journal-id-type="doi">
    10.1038/41598.2045-2322
   </journal-id>
   <journal-title-group>
    <journal-title>
     Scientific Reports
    </journal-title>
    <abbrev-journal-title abbrev-type="publisher">
     Sci Rep
    </abbrev-journal-title>
   </journal-title-group>
   <issn pub-type="epub">
    2045-2322
   </issn>
   <publisher>
    <publisher-name>
     Nature Publishing Group UK
    </publisher-name>
    <publisher-loc>
     London
    </publisher-loc>
   </publisher>
  </journal-meta>
  <article-meta>
   <article-id pub-id-type="publisher-id">
    s41598-022-23052-9
   </article-id>
   <article-id pub-id-type="manuscript">
    23052
   </article-id>
   <article-id pub-id-type="doi">
    10.1038/s41598-022-23052-9
   </article-id>
   <article-categories>
    <subj-group subj-group-type="heading">
     <subject>
      Article
     </subject>
    </subj-group>
    <subj-group subj-group-type="SubjectPath">
     <subject>
      /639/166
     </subject>
    </subj-group>
    <subj-group subj-group-type="SubjectPath">
     <subject>
      /639/166/987
     </subject>
    </subj-group>
    <subj-group subj-group-type="NatureArticleTypeID">
     <subject>
      article
     </subject>
    </subj-group>
   </article-categories>
   <title-group>
    <article-title xml:lang="en">
     Contrastive language and vision learning of general fashion concepts
    </article-title>
   </title-group>
   <contrib-group>
    <contrib contrib-type="author" corresp="yes" id="Au1">
     <name name-style="western">
      <surname>
       Chia
      </surname>
      <given-names>
       Patrick John
      </given-names>
     </name>
     <address>
      <email>
       pchia@coveo.com
      </email>
     </address>
     <xref ref-type="aff" rid="Aff1">
      1
     </xref>
     <xref ref-type="corresp" rid="IDs41598022230529_cor1">
      a
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au2">
     <name name-style="western">
      <surname>
       Attanasio
      </surname>
      <given-names>
       Giuseppe
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff2">
      2
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au3">
     <name name-style="western">
      <surname>
       Bianchi
      </surname>
      <given-names>
       Federico
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff3">
      3
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au4">
     <name name-style="western">
      <surname>
       Terragni
      </surname>
      <given-names>
       Silvia
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff4">
      4
     </xref>
     <xref ref-type="aff" rid="Aff7">
      7
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au5">
     <name name-style="western">
      <surname>
       Magalhães
      </surname>
      <given-names>
       Ana Rita
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff5">
      5
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au6">
     <name name-style="western">
      <surname>
       Goncalves
      </surname>
      <given-names>
       Diogo
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff5">
      5
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au7">
     <name name-style="western">
      <surname>
       Greco
      </surname>
      <given-names>
       Ciro
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff6">
      6
     </xref>
    </contrib>
    <contrib contrib-type="author" id="Au8">
     <name name-style="western">
      <surname>
       Tagliabue
      </surname>
      <given-names>
       Jacopo
      </given-names>
     </name>
     <xref ref-type="aff" rid="Aff6">
      6
     </xref>
    </contrib>
    <aff id="Aff1">
     <label>
      1
     </label>
     <institution-wrap>
      <institution content-type="org-name">
       Coveo
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      Montreal
     </addr-line>
     <country country="CA">
      Canada
     </country>
    </aff>
    <aff id="Aff2">
     <label>
      2
     </label>
     <institution-wrap>
      <institution-id institution-id-type="GRID">
       grid.7945.f
      </institution-id>
      <institution-id institution-id-type="ISNI">
       0000 0001 2165 6939
      </institution-id>
      <institution content-type="org-name">
       Bocconi University
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      Milan
     </addr-line>
     <country country="IT">
      Italy
     </country>
    </aff>
    <aff id="Aff3">
     <label>
      3
     </label>
     <institution-wrap>
      <institution-id institution-id-type="GRID">
       grid.168010.e
      </institution-id>
      <institution-id institution-id-type="ISNI">
       0000000419368956
      </institution-id>
      <institution content-type="org-name">
       Stanford University
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      Stanford
     </addr-line>
     <addr-line content-type="state">
      CA
     </addr-line>
     <country country="US">
      USA
     </country>
    </aff>
    <aff id="Aff4">
     <label>
      4
     </label>
     <institution-wrap>
      <institution content-type="org-name">
       Telepathy Labs
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      Zurich
     </addr-line>
     <country country="CH">
      Switzerland
     </country>
    </aff>
    <aff id="Aff5">
     <label>
      5
     </label>
     <institution-wrap>
      <institution content-type="org-name">
       Farfetch
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      Porto
     </addr-line>
     <country country="PT">
      Portugal
     </country>
    </aff>
    <aff id="Aff6">
     <label>
      6
     </label>
     <institution-wrap>
      <institution content-type="org-name">
       South Park Commons
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      New York
     </addr-line>
     <country country="US">
      USA
     </country>
    </aff>
    <aff id="Aff7">
     <label>
      7
     </label>
     <institution-wrap>
      <institution-id institution-id-type="GRID">
       grid.7563.7
      </institution-id>
      <institution-id institution-id-type="ISNI">
       0000 0001 2174 1754
      </institution-id>
      <institution content-type="org-name">
       University of Milano-Bicocca
      </institution>
     </institution-wrap>
     <addr-line content-type="city">
      Milan
     </addr-line>
     <country country="IT">
      Italy
     </country>
    </aff>
   </contrib-group>
   <author-notes>
    <corresp id="IDs41598022230529_cor1">
     <label>
      a
     </label>
     <email>
      pchia@coveo.com
     </email>
    </corresp>
   </author-notes>
   <pub-date date-type="pub" publication-format="electronic">
    <day>
     8
    </day>
    <month>
     11
    </month>
    <year>
     2022
    </year>
   </pub-date>
   <pub-date date-type="collection" publication-format="electronic">
    <month>
     12
    </month>
    <year>
     2022
    </year>
   </pub-date>
   <volume>
    12
   </volume>
   <issue seq="18958">
    1
   </issue>
   <elocation-id>
    18958
   </elocation-id>
   <history>
    <date date-type="registration">
     <day>
      25
     </day>
     <month>
      10
     </month>
     <year>
      2022
     </year>
    </date>
    <date date-type="received">
     <day>
      26
     </day>
     <month>
      5
     </month>
     <year>
      2022
     </year>
    </date>
    <date date-type="accepted">
     <day>
      25
     </day>
     <month>
      10
     </month>
     <year>
      2022
     </year>
    </date>
    <date date-type="online">
     <day>
      8
     </day>
     <month>
      11
     </month>
     <year>
      2022
     </year>
    </date>
   </history>
   <pub-history>
    <event event-type="Correction">
     <event-desc>
      A Correction to this paper has been published: https://doi.org/10.1038/s41598-022-26364-y
     </event-desc>
     <date>
      <day>
       23
      </day>
      <month>
       1
      </month>
      <year>
       2023
      </year>
     </date>
    </event>
   </pub-history>
   <permissions>
    <copyright-statement content-type="compact">
     © The Author(s) 2022
    </copyright-statement>
    <copyright-year>
     2022
    </copyright-year>
    <copyright-holder>
     The Author(s)
    </copyright-holder>
    <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/">
     <license-p>
      <bold>
       Open Access
      </bold>
      This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit
      <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">
       http://creativecommons.org/licenses/by/4.0/
      </ext-link>
      .
     </license-p>
    </license>
   </permissions>
   <related-article ext-link-type="doi" related-article-type="correction-forward" xlink:href="10.1038/s41598-022-26364-y"/>
   <abstract id="Abs1" xml:lang="en">
    <title>
     Abstract
    </title>
    <p id="Par1">
     The steady rise of online shopping goes hand in hand with the development of increasingly complex ML and NLP models. While most use cases are cast as specialized supervised learning problems, we argue that practitioners would greatly benefit from general and transferable representations of products. In
     <italic>
      this
     </italic>
     work, we build on recent developments in contrastive learning to train
     <italic>
      FashionCLIP
     </italic>
     , a
     <italic>
      CLIP
     </italic>
     -like model adapted for the fashion industry. We demonstrate the effectiveness of the representations learned by
     <italic>
      FashionCLIP
     </italic>
     with extensive tests across a variety of tasks, datasets and generalization probes. We argue that adaptations of large pre-trained models such as CLIP offer new perspectives in terms of scalability and sustainability for certain types of players in the industry. Finally, we detail the costs and environmental impact of training, and release the model weights and code as open source contribution to the community.
    </p>
   </abstract>
   <custom-meta-group>
    <custom-meta>
     <meta-name>
      publisher-imprint-name
     </meta-name>
     <meta-value>
      Nature Portfolio
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      volume-issue-count
     </meta-name>
     <meta-value>
      1
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-article-count
     </meta-name>
     <meta-value>
      22654
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-toc-levels
     </meta-name>
     <meta-value>
      0
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-pricelist-year
     </meta-name>
     <meta-value>
      2022
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-copyright-holder
     </meta-name>
     <meta-value>
      The Author(s)
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-copyright-year
     </meta-name>
     <meta-value>
      2022
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-contains-esm
     </meta-name>
     <meta-value>
      No
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-numbering-style
     </meta-name>
     <meta-value>
      Unnumbered
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-registration-date-year
     </meta-name>
     <meta-value>
      2022
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-registration-date-month
     </meta-name>
     <meta-value>
      10
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-registration-date-day
     </meta-name>
     <meta-value>
      25
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-toc-levels
     </meta-name>
     <meta-value>
      0
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      toc-levels
     </meta-name>
     <meta-value>
      0
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      volume-type
     </meta-name>
     <meta-value>
      Regular
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-product
     </meta-name>
     <meta-value>
      NonStandardArchiveJournal
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      numbering-style
     </meta-name>
     <meta-value>
      Unnumbered
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-grants-type
     </meta-name>
     <meta-value>
      OpenChoice
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      metadata-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      abstract-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      bodypdf-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      bodyhtml-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      bibliography-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      esm-grant
     </meta-name>
     <meta-value>
      OpenAccess
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      online-first
     </meta-name>
     <meta-value>
      false
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      pdf-file-reference
     </meta-name>
     <meta-value>
      BodyRef/PDF/41598_2022_Article_23052.pdf
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      pdf-type
     </meta-name>
     <meta-value>
      Typeset
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      target-type
     </meta-name>
     <meta-value>
      OnlinePDF
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-online-date-year
     </meta-name>
     <meta-value>
      2023
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-online-date-month
     </meta-name>
     <meta-value>
      1
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-online-date-day
     </meta-name>
     <meta-value>
      1
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      issue-type
     </meta-name>
     <meta-value>
      Regular
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      article-type
     </meta-name>
     <meta-value>
      OriginalPaper
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-primary
     </meta-name>
     <meta-value>
      Science, Humanities and Social Sciences, multidisciplinary
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-secondary
     </meta-name>
     <meta-value>
      Science, Humanities and Social Sciences, multidisciplinary
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-secondary
     </meta-name>
     <meta-value>
      Science, multidisciplinary
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      journal-subject-collection
     </meta-name>
     <meta-value>
      Science (multidisciplinary)
     </meta-value>
    </custom-meta>
    <custom-meta>
     <meta-name>
      open-access
     </meta-name>
     <meta-value>
      true
     </meta-value>
    </custom-meta>
   </custom-meta-group>
  </article-meta>
  <notes notes-type="CrossLinking">
   <sec>
    <p>
     A correction to this article is available online at
     <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-022-26364-y">
      https://doi.org/10.1038/s41598-022-26364-y
     </ext-link>
     .
    </p>
   </sec>
  </notes>
 </front>
 <body>
  <sec id="Sec1">
   <title>
    Introduction
   </title>
   <sec id="Sec2">
    <title>
     Generalization and scalability in machine learning
    </title>
    <p id="Par2">
     The extraordinary growth of online retail—as of 2020, 4 trillion dollars per year
     <sup>
      <xref ref-type="bibr" rid="CR1">
       1
      </xref>
     </sup>
     —has profoundly impacted the fashion industry, with 1 out of 4 transactions now happening online
     <sup>
      <xref ref-type="bibr" rid="CR2">
       2
      </xref>
     </sup>
     . The combination of large amounts of data and a variety of use cases has made e-commerce fertile for cutting-edge machine learning (ML) models, with Natural Language Processing (NLP) involved in recommendations
     <sup>
      <xref ref-type="bibr" rid="CR3">
       3
      </xref>
      ,
      <xref ref-type="bibr" rid="CR4">
       4
      </xref>
      –
      <xref ref-type="bibr" rid="CR5">
       5
      </xref>
     </sup>
     , information retrieval (IR)
     <sup>
      <xref ref-type="bibr" rid="CR6">
       6
      </xref>
     </sup>
     , product classification
     <sup>
      <xref ref-type="bibr" rid="CR7">
       7
      </xref>
     </sup>
     and many other use cases
     <sup>
      <xref ref-type="bibr" rid="CR8">
       8
      </xref>
      ,
      <xref ref-type="bibr" rid="CR9">
       9
      </xref>
      –
      <xref ref-type="bibr" rid="CR10">
       10
      </xref>
     </sup>
     .
    </p>
    <p id="Par3">
     However, as the community starts to address the huge operational costs of training and developing models
     <sup>
      <xref ref-type="bibr" rid="CR11">
       11
      </xref>
     </sup>
     , it is becoming clear that the value of ML innovations has been mostly captured by a few players
     <sup>
      <xref ref-type="bibr" rid="CR12">
       12
      </xref>
     </sup>
     . Where the rest of the retail industry is making concrete efforts to adapt promptly, companies offering ML products as a service recently gained traction, creating a new multi-billion dollar market
     <sup>
      <xref ref-type="bibr" rid="CR13">
       13
      </xref>
      ,
      <xref ref-type="bibr" rid="CR14">
       14
      </xref>
      ,
      <xref ref-type="bibr" rid="CR15">
       15
      </xref>
      –
      <xref ref-type="bibr" rid="CR16">
       16
      </xref>
     </sup>
     . The need for ML capabilities that can be applied across entire industries and verticals raises the stakes for an age-old question in ML:
     <italic>
      can we build models that can be reused on different tasks and datasets
     </italic>
     ?
    </p>
    <p id="Par4">
     While generalization is a theoretical virtue, real-world models often succeed by (over)fitting to a specific dataset and task
     <sup>
      <xref ref-type="bibr" rid="CR17">
       17
      </xref>
      ,
      <xref ref-type="bibr" rid="CR18">
       18
      </xref>
     </sup>
     . In practice, generalization has been considered both hard to achieve
     <italic>
      and
     </italic>
     economically undesirable for large-scale use cases. In this context, the advent of large-scale, self-supervised models such as
     <italic>
      Contrastive Language-Image Pre-training
     </italic>
     (CLIP)
     <sup>
      <xref ref-type="bibr" rid="CR17">
       17
      </xref>
     </sup>
     is particularly interesting both from a theoretical and a practical point of view. Building upon large pre-trained models to learn
     <italic>
      general
     </italic>
     concepts in specific verticals/industries (e.g., Fashion, Electronics, DIY, etc.) may provide a new and sustainable way to bring the benefits of ML capabilities to a broader set of practitioners, especially outside of large tech companies. The idea would be to fine-tune general foundational models
     <sup>
      <xref ref-type="bibr" rid="CR19">
       19
      </xref>
     </sup>
     to learn concepts that are specific to a domain (e.g., fashion), but general enough to be applicable to all the use cases within that domain.
    </p>
    <p id="Par5">
     In
     <italic>
      this
     </italic>
     work, we show through extensive testing and open-source code that multi-modal training can be successfully used to learn general concepts in a specific domain, namely fashion. In fact, we will argue that it is not only technically possible, but also economically viable, and practically advantageous, since moving away from the traditional setting where single supervised models are trained specifically per use case reduces annotation and maintenance costs while providing solutions transferable across tasks.
    </p>
   </sec>
   <sec id="Sec3">
    <title>
     Self-supervised contrastive learning of fashion concepts
    </title>
    <p id="Par6">
     Contrastive learning has recently become a predominant approach to learn meaningful representations of concepts in ML. The learning framework builds on the idea that semantically related
     <italic>
      concepts
     </italic>
     (e.g., two pictures of the same object from different viewpoints) should have
     <italic>
      similar
     </italic>
     representations, while unrelated ones should be
     <italic>
      dissimilar
     </italic>
     . Initially devised for self-supervised image representation learning
     <sup>
      <xref ref-type="bibr" rid="CR20">
       20
      </xref>
      ,
      <xref ref-type="bibr" rid="CR21">
       21
      </xref>
     </sup>
     , contrastive learning has recently been applied to language as well
     <sup>
      <xref ref-type="bibr" rid="CR22">
       22
      </xref>
      ,
      <xref ref-type="bibr" rid="CR23">
       23
      </xref>
     </sup>
     . Recent work has used contrastive training to bridge different modalities, e.g., vision and language
     <sup>
      <xref ref-type="bibr" rid="CR24">
       24
      </xref>
      ,
      <xref ref-type="bibr" rid="CR25">
       25
      </xref>
     </sup>
     , audio and language
     <sup>
      <xref ref-type="bibr" rid="CR26">
       26
      </xref>
      ,
      <xref ref-type="bibr" rid="CR27">
       27
      </xref>
     </sup>
     , or a combination of the three
     <sup>
      <xref ref-type="bibr" rid="CR28">
       28
      </xref>
      ,
      <xref ref-type="bibr" rid="CR29">
       29
      </xref>
     </sup>
     . These models learn concept representations from different modalities (e.g., a textual excerpt such as “a dog running on a field” and a picture depicting the scene) and optimize them to be close in a shared latent space. Crucially, the typical pipeline is self-supervised: since no manual annotation is involved (e.g., in the previous example, one can gather image-text pairs from the web), human intervention is limited to deciding which pre-training task shall be used.
    </p>
    <p id="Par7">
     <monospace>
      CLIP
     </monospace>
     <sup>
      <xref ref-type="bibr" rid="CR17">
       17
      </xref>
     </sup>
     is a vision-language multi-modal neural network trained via CL to associate vision concepts with text. The model comprises a vision and text encoder, each followed by a linear layer to project the image and text representations to the same latent space.
     <monospace>
      CLIP
     </monospace>
     is trained to
     <italic>
      position
     </italic>
     images and matching descriptions (e.g. an image of a red shirt and its description “a red shirt”) close together in the vector space (see Fig.
     <xref ref-type="fig" rid="Fig1">
      1
     </xref>
     for an example). When trained on 400 million&lt;image, text&gt; pairs collected from the internet,
     <monospace>
      CLIP
     </monospace>
     has demonstrated competitive zero-shot or few-shot transfer to downstream tasks such as OCR and fine-grained object classification
     <sup>
      <xref ref-type="bibr" rid="CR17">
       17
      </xref>
     </sup>
     .
    </p>
    <p id="Par8">
     More formally,
     <monospace>
      CLIP
     </monospace>
     is a multi-modal model that makes use of an image (
     <inline-formula id="IEq1">
      <alternatives>
       <math id="IEq1_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi>
          I
         </mi>
         <msup>
          <mi>
           θ
          </mi>
          <mi>
           I
          </mi>
         </msup>
        </msub>
       </math>
       <tex-math id="IEq1_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_{\theta ^I}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq1.gif"/>
      </alternatives>
     </inline-formula>
     ) and a text (
     <inline-formula id="IEq2">
      <alternatives>
       <math id="IEq2_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi>
          T
         </mi>
         <msup>
          <mi>
           θ
          </mi>
          <mi>
           T
          </mi>
         </msup>
        </msub>
       </math>
       <tex-math id="IEq2_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T_{\theta ^T}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq2.gif"/>
      </alternatives>
     </inline-formula>
     ) encoder. Both encoders are deep neural networks that map raw representations (i.e., an image and a text) to a 512-dimensional dense vector (e.g, given an image
     <italic>
      i
     </italic>
     ,
     <inline-formula id="IEq3">
      <alternatives>
       <math id="IEq3_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <msub>
          <mi>
           I
          </mi>
          <msup>
           <mi>
            θ
           </mi>
           <mi>
            I
           </mi>
          </msup>
         </msub>
         <mrow>
          <mo stretchy="false">
           (
          </mo>
          <mi>
           i
          </mi>
          <mo stretchy="false">
           )
          </mo>
         </mrow>
         <mo>
          ∈
         </mo>
         <msup>
          <mrow>
           <mi mathvariant="double-struck">
            R
           </mi>
          </mrow>
          <mn>
           512
          </mn>
         </msup>
        </mrow>
       </math>
       <tex-math id="IEq3_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_{\theta ^I}(i) \in \mathbb {R}^{512}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq3.gif"/>
      </alternatives>
     </inline-formula>
     ). During training,
     <italic>
      N
     </italic>
     pairs of matching images and texts
     <inline-formula id="IEq4">
      <alternatives>
       <math id="IEq4_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mo>
          &lt;
         </mo>
         <mi>
          i
         </mi>
         <mo>
          ,
         </mo>
         <mi>
          t
         </mi>
         <mo>
          &gt;
         </mo>
        </mrow>
       </math>
       <tex-math id="IEq4_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&lt;i, t&gt;$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq4.gif"/>
      </alternatives>
     </inline-formula>
     are selected (e.g., as in Fig.
     <xref ref-type="fig" rid="Fig1">
      1
     </xref>
     , the image of a red shirt and the description “a red shirt”), encoded using
     <inline-formula id="IEq5">
      <alternatives>
       <math id="IEq5_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi>
          I
         </mi>
         <msup>
          <mi>
           θ
          </mi>
          <mi>
           I
          </mi>
         </msup>
        </msub>
       </math>
       <tex-math id="IEq5_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_{\theta ^I}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq5.gif"/>
      </alternatives>
     </inline-formula>
     and
     <inline-formula id="IEq6">
      <alternatives>
       <math id="IEq6_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi>
          T
         </mi>
         <msup>
          <mi>
           θ
          </mi>
          <mi>
           T
          </mi>
         </msup>
        </msub>
       </math>
       <tex-math id="IEq6_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T_{\theta ^T}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq6.gif"/>
      </alternatives>
     </inline-formula>
     ,
     <inline-formula id="IEq7">
      <alternatives>
       <math id="IEq7_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi>
          L
         </mi>
         <mn>
          2
         </mn>
        </msub>
       </math>
       <tex-math id="IEq7_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq7.gif"/>
      </alternatives>
     </inline-formula>
     -normalized, and compared pairwise.
     <monospace>
      CLIP
     </monospace>
     minimizes cross-entropy loss such that
     <inline-formula id="IEq8">
      <alternatives>
       <math id="IEq8_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <msub>
          <mover accent="true">
           <mrow>
            <mi>
             I
            </mi>
           </mrow>
           <mrow>
            <mo stretchy="false">
             ¯
            </mo>
           </mrow>
          </mover>
          <msup>
           <mi>
            θ
           </mi>
           <mi>
            I
           </mi>
          </msup>
         </msub>
         <mrow>
          <mo stretchy="false">
           (
          </mo>
          <msub>
           <mi>
            i
           </mi>
           <mi>
            j
           </mi>
          </msub>
          <mo stretchy="false">
           )
          </mo>
         </mrow>
         <mo>
          ·
         </mo>
         <msub>
          <mover accent="true">
           <mrow>
            <mi>
             T
            </mi>
           </mrow>
           <mrow>
            <mo stretchy="false">
             ¯
            </mo>
           </mrow>
          </mover>
          <msup>
           <mi>
            θ
           </mi>
           <mi>
            T
           </mi>
          </msup>
         </msub>
         <mrow>
          <mo stretchy="false">
           (
          </mo>
          <msub>
           <mi>
            t
           </mi>
           <mi>
            k
           </mi>
          </msub>
          <mo stretchy="false">
           )
          </mo>
         </mrow>
        </mrow>
       </math>
       <tex-math id="IEq8_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar{I}_{\theta ^I}(i_j) \cdot \bar{T}_{\theta ^T}(t_k)$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq8.gif"/>
      </alternatives>
     </inline-formula>
     for
     <inline-formula id="IEq9">
      <alternatives>
       <math id="IEq9_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mi>
          j
         </mi>
         <mo>
          ,
         </mo>
         <mi>
          k
         </mi>
         <mo>
          =
         </mo>
         <mn>
          1
         </mn>
         <mo>
          ,
         </mo>
         <mo>
          .
         </mo>
         <mo>
          .
         </mo>
         <mo>
          ,
         </mo>
         <mi>
          N
         </mi>
        </mrow>
       </math>
       <tex-math id="IEq9_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$j,k = 1,..,N$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq9.gif"/>
      </alternatives>
     </inline-formula>
     is highest when the caption is paired with the correct image (
     <inline-formula id="IEq10">
      <alternatives>
       <math id="IEq10_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mi>
          j
         </mi>
         <mo>
          =
         </mo>
         <mi>
          k
         </mi>
        </mrow>
       </math>
       <tex-math id="IEq10_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$j=k$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq10.gif"/>
      </alternatives>
     </inline-formula>
     ), and low otherwise (
     <inline-formula id="IEq11">
      <alternatives>
       <math id="IEq11_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mi>
          j
         </mi>
         <mo>
          ≠
         </mo>
         <mi>
          k
         </mi>
        </mrow>
       </math>
       <tex-math id="IEq11_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$j \ne k$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq11.gif"/>
      </alternatives>
     </inline-formula>
     ), where
     <inline-formula id="IEq12">
      <alternatives>
       <math id="IEq12_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <msub>
          <mover accent="true">
           <mrow>
            <mi>
             I
            </mi>
           </mrow>
           <mrow>
            <mo stretchy="false">
             ¯
            </mo>
           </mrow>
          </mover>
          <msup>
           <mi>
            θ
           </mi>
           <mi>
            I
           </mi>
          </msup>
         </msub>
         <mrow>
          <mo stretchy="false">
           (
          </mo>
          <mo>
           ·
          </mo>
          <mo stretchy="false">
           )
          </mo>
         </mrow>
        </mrow>
       </math>
       <tex-math id="IEq12_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar{I}_{\theta ^I}(\cdot )$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq12.gif"/>
      </alternatives>
     </inline-formula>
     /
     <inline-formula id="IEq13">
      <alternatives>
       <math id="IEq13_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <msub>
          <mover accent="true">
           <mrow>
            <mi>
             T
            </mi>
           </mrow>
           <mrow>
            <mo stretchy="false">
             ¯
            </mo>
           </mrow>
          </mover>
          <msup>
           <mi>
            θ
           </mi>
           <mi>
            I
           </mi>
          </msup>
         </msub>
         <mrow>
          <mo stretchy="false">
           (
          </mo>
          <mo>
           ·
          </mo>
          <mo stretchy="false">
           )
          </mo>
         </mrow>
        </mrow>
       </math>
       <tex-math id="IEq13_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar{T}_{\theta ^I}(\cdot )$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq13.gif"/>
      </alternatives>
     </inline-formula>
     are the
     <inline-formula id="IEq14">
      <alternatives>
       <math id="IEq14_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi>
          L
         </mi>
         <mn>
          2
         </mn>
        </msub>
       </math>
       <tex-math id="IEq14_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq14.gif"/>
      </alternatives>
     </inline-formula>
     -normalized outputs of the image and text encoders. We summarize the optimization objective for
     <monospace>
      CLIP
     </monospace>
     in Eq. (
     <xref ref-type="disp-formula" rid="Equ1">
      1
     </xref>
     ) and (
     <xref ref-type="disp-formula" rid="Equ2">
      2
     </xref>
     ).
     <disp-formula id="Equ1">
      <label>
       1
      </label>
      <alternatives>
       <math display="block" id="Equ1_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mtable>
          <mtr>
           <mtd/>
           <mtd columnalign="left">
            <mrow>
             <mi mathvariant="script">
              L
             </mi>
             <mrow>
              <mo stretchy="false">
               (
              </mo>
              <msup>
               <mi>
                θ
               </mi>
               <mi>
                I
               </mi>
              </msup>
              <mo>
               ,
              </mo>
              <msup>
               <mi>
                θ
               </mi>
               <mi>
                T
               </mi>
              </msup>
              <mo stretchy="false">
               )
              </mo>
             </mrow>
             <mo>
              =
             </mo>
             <mo>
              -
             </mo>
             <mfrac>
              <mn>
               1
              </mn>
              <mrow>
               <mn>
                2
               </mn>
               <mi>
                N
               </mi>
              </mrow>
             </mfrac>
             <mfenced close=")" open="(">
              <munder>
               <mo>
                ∑
               </mo>
               <mi>
                j
               </mi>
              </munder>
              <mo>
               log
              </mo>
              <mfrac>
               <msup>
                <mi>
                 e
                </mi>
                <mrow>
                 <msub>
                  <mover accent="true">
                   <mrow>
                    <mi>
                     I
                    </mi>
                   </mrow>
                   <mrow>
                    <mo stretchy="false">
                     ¯
                    </mo>
                   </mrow>
                  </mover>
                  <msup>
                   <mi>
                    θ
                   </mi>
                   <mi>
                    I
                   </mi>
                  </msup>
                 </msub>
                 <mrow>
                  <mo stretchy="false">
                   (
                  </mo>
                  <msub>
                   <mi>
                    i
                   </mi>
                   <mi>
                    j
                   </mi>
                  </msub>
                  <mo stretchy="false">
                   )
                  </mo>
                 </mrow>
                 <mo>
                  ·
                 </mo>
                 <msub>
                  <mover accent="true">
                   <mrow>
                    <mi>
                     T
                    </mi>
                   </mrow>
                   <mrow>
                    <mo stretchy="false">
                     ¯
                    </mo>
                   </mrow>
                  </mover>
                  <msup>
                   <mi>
                    θ
                   </mi>
                   <mi>
                    T
                   </mi>
                  </msup>
                 </msub>
                 <mrow>
                  <mo stretchy="false">
                   (
                  </mo>
                  <msub>
                   <mi>
                    t
                   </mi>
                   <mi>
                    j
                   </mi>
                  </msub>
                  <mo stretchy="false">
                   )
                  </mo>
                 </mrow>
                </mrow>
               </msup>
               <mrow>
                <msub>
                 <mo>
                  ∑
                 </mo>
                 <msup>
                  <mi>
                   k
                  </mi>
                  <msup>
                   <mrow/>
                   <mo>
                    ′
                   </mo>
                  </msup>
                 </msup>
                </msub>
                <msup>
                 <mi>
                  e
                 </mi>
                 <mrow>
                  <msub>
                   <mover accent="true">
                    <mrow>
                     <mi>
                      I
                     </mi>
                    </mrow>
                    <mrow>
                     <mo stretchy="false">
                      ¯
                     </mo>
                    </mrow>
                   </mover>
                   <msup>
                    <mi>
                     θ
                    </mi>
                    <mi>
                     I
                    </mi>
                   </msup>
                  </msub>
                  <mrow>
                   <mo stretchy="false">
                    (
                   </mo>
                   <msub>
                    <mi>
                     i
                    </mi>
                    <mi>
                     j
                    </mi>
                   </msub>
                   <mo stretchy="false">
                    )
                   </mo>
                  </mrow>
                  <mo>
                   ·
                  </mo>
                  <msub>
                   <mover accent="true">
                    <mrow>
                     <mi>
                      T
                     </mi>
                    </mrow>
                    <mrow>
                     <mo stretchy="false">
                      ¯
                     </mo>
                    </mrow>
                   </mover>
                   <msup>
                    <mi>
                     θ
                    </mi>
                    <mi>
                     T
                    </mi>
                   </msup>
                  </msub>
                  <mrow>
                   <mo stretchy="false">
                    (
                   </mo>
                   <msub>
                    <mi>
                     t
                    </mi>
                    <msup>
                     <mi>
                      k
                     </mi>
                     <msup>
                      <mrow/>
                      <mo>
                       ′
                      </mo>
                     </msup>
                    </msup>
                   </msub>
                   <mo stretchy="false">
                    )
                   </mo>
                  </mrow>
                 </mrow>
                </msup>
               </mrow>
              </mfrac>
              <mo>
               +
              </mo>
              <munder>
               <mo>
                ∑
               </mo>
               <mi>
                k
               </mi>
              </munder>
              <mo>
               log
              </mo>
              <mfrac>
               <msup>
                <mi>
                 e
                </mi>
                <mrow>
                 <msub>
                  <mover accent="true">
                   <mrow>
                    <mi>
                     I
                    </mi>
                   </mrow>
                   <mrow>
                    <mo stretchy="false">
                     ¯
                    </mo>
                   </mrow>
                  </mover>
                  <msup>
                   <mi>
                    θ
                   </mi>
                   <mi>
                    I
                   </mi>
                  </msup>
                 </msub>
                 <mrow>
                  <mo stretchy="false">
                   (
                  </mo>
                  <msub>
                   <mi>
                    i
                   </mi>
                   <mi>
                    k
                   </mi>
                  </msub>
                  <mo stretchy="false">
                   )
                  </mo>
                 </mrow>
                 <mo>
                  ·
                 </mo>
                 <msub>
                  <mover accent="true">
                   <mrow>
                    <mi>
                     T
                    </mi>
                   </mrow>
                   <mrow>
                    <mo stretchy="false">
                     ¯
                    </mo>
                   </mrow>
                  </mover>
                  <msup>
                   <mi>
                    θ
                   </mi>
                   <mi>
                    T
                   </mi>
                  </msup>
                 </msub>
                 <mrow>
                  <mo stretchy="false">
                   (
                  </mo>
                  <msub>
                   <mi>
                    t
                   </mi>
                   <mi>
                    k
                   </mi>
                  </msub>
                  <mo stretchy="false">
                   )
                  </mo>
                 </mrow>
                </mrow>
               </msup>
               <mrow>
                <msub>
                 <mo>
                  ∑
                 </mo>
                 <msup>
                  <mi>
                   j
                  </mi>
                  <msup>
                   <mrow/>
                   <mo>
                    ′
                   </mo>
                  </msup>
                 </msup>
                </msub>
                <msup>
                 <mi>
                  e
                 </mi>
                 <mrow>
                  <msub>
                   <mover accent="true">
                    <mrow>
                     <mi>
                      I
                     </mi>
                    </mrow>
                    <mrow>
                     <mo stretchy="false">
                      ¯
                     </mo>
                    </mrow>
                   </mover>
                   <msup>
                    <mi>
                     θ
                    </mi>
                    <mi>
                     I
                    </mi>
                   </msup>
                  </msub>
                  <mrow>
                   <mo stretchy="false">
                    (
                   </mo>
                   <msub>
                    <mi>
                     i
                    </mi>
                    <msup>
                     <mi>
                      j
                     </mi>
                     <msup>
                      <mrow/>
                      <mo>
                       ′
                      </mo>
                     </msup>
                    </msup>
                   </msub>
                   <mo stretchy="false">
                    )
                   </mo>
                  </mrow>
                  <mo>
                   ·
                  </mo>
                  <msub>
                   <mover accent="true">
                    <mrow>
                     <mi>
                      T
                     </mi>
                    </mrow>
                    <mrow>
                     <mo stretchy="false">
                      ¯
                     </mo>
                    </mrow>
                   </mover>
                   <msup>
                    <mi>
                     θ
                    </mi>
                    <mi>
                     T
                    </mi>
                   </msup>
                  </msub>
                  <mrow>
                   <mo stretchy="false">
                    (
                   </mo>
                   <msub>
                    <mi>
                     t
                    </mi>
                    <mi>
                     k
                    </mi>
                   </msub>
                   <mo stretchy="false">
                    )
                   </mo>
                  </mrow>
                 </mrow>
                </msup>
               </mrow>
              </mfrac>
             </mfenced>
            </mrow>
           </mtd>
          </mtr>
         </mtable>
        </mrow>
       </math>
       <tex-math id="Equ1_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned}&amp;{\mathscr {L}}(\theta ^{I}, \theta ^{T}) = - \frac{1}{2N} \left( \sum _{j} \log \frac{e^{\bar{I}_{\theta ^I}(i_{j}) \cdot \bar{T}_{\theta ^T}(t_{j})}}{\sum _{k^{'}} e^{\bar{I}_{\theta ^{I}}(i_{j}) \cdot \bar{T}_{\theta ^{T}}(t_{k^{'}})}} + \sum _{k} \log \frac{e^{\bar{I}_{\theta ^I}(i_k) \cdot \bar{T}_{\theta ^{T}}(t_{k})}}{\sum _{j^{'}} e^{\bar{I}_{\theta ^{I}}(i_{j^{'}}) \cdot \bar{T}_{\theta ^{T}}(t_{k})}} \right) \end{aligned}$$\end{document}
       </tex-math>
       <graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_Equ1.gif"/>
      </alternatives>
     </disp-formula>
     <disp-formula id="Equ2">
      <label>
       2
      </label>
      <alternatives>
       <math display="block" id="Equ2_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mtable>
          <mtr>
           <mtd/>
           <mtd columnalign="left">
            <mrow>
             <msup>
              <mi>
               θ
              </mi>
              <mrow>
               <mi>
                I
               </mi>
               <mrow/>
               <mo>
                ∗
               </mo>
              </mrow>
             </msup>
             <mo>
              ,
             </mo>
             <msup>
              <mi>
               θ
              </mi>
              <mrow>
               <mi>
                T
               </mi>
               <mrow/>
               <mo>
                ∗
               </mo>
              </mrow>
             </msup>
             <mo>
              =
             </mo>
             <munder>
              <mrow>
               <mi mathvariant="normal">
                arg
               </mi>
               <mspace width="0.166667em"/>
               <mi mathvariant="normal">
                min
               </mi>
              </mrow>
              <mrow>
               <msup>
                <mi>
                 θ
                </mi>
                <mi>
                 I
                </mi>
               </msup>
               <mo>
                ,
               </mo>
               <msup>
                <mi>
                 θ
                </mi>
                <mi>
                 T
                </mi>
               </msup>
              </mrow>
             </munder>
             <mspace width="0.166667em"/>
             <mi mathvariant="script">
              L
             </mi>
             <mrow>
              <mo stretchy="false">
               (
              </mo>
              <msup>
               <mi>
                θ
               </mi>
               <mi>
                I
               </mi>
              </msup>
              <mo>
               ,
              </mo>
              <msup>
               <mi>
                θ
               </mi>
               <mi>
                T
               </mi>
              </msup>
              <mo stretchy="false">
               )
              </mo>
             </mrow>
            </mrow>
           </mtd>
          </mtr>
         </mtable>
        </mrow>
       </math>
       <tex-math id="Equ2_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned}&amp;\theta ^{I*}, \theta ^{T*} = {\mathop {\mathrm{arg\,min}}\limits _{\theta ^{I}, \theta ^{T}}}\, {\mathscr {L}}(\theta ^{I},\theta ^{T}) \end{aligned}$$\end{document}
       </tex-math>
       <graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_Equ2.gif"/>
      </alternatives>
     </disp-formula>
     Here,
     <inline-formula id="IEq15">
      <alternatives>
       <math id="IEq15_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mi>
          θ
         </mi>
         <mi>
          I
         </mi>
        </msup>
       </math>
       <tex-math id="IEq15_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta ^I$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq15.gif"/>
      </alternatives>
     </inline-formula>
     and
     <inline-formula id="IEq16">
      <alternatives>
       <math id="IEq16_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mi>
          θ
         </mi>
         <mi>
          T
         </mi>
        </msup>
       </math>
       <tex-math id="IEq16_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta ^T$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq16.gif"/>
      </alternatives>
     </inline-formula>
     are the learnable parameters of the image and text encoder neural networks, and the
     <inline-formula id="IEq17">
      <alternatives>
       <math id="IEq17_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mo stretchy="false">
          (
         </mo>
         <mo>
          ·
         </mo>
         <mo stretchy="false">
          )
         </mo>
        </mrow>
       </math>
       <tex-math id="IEq17_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\cdot )$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq17.gif"/>
      </alternatives>
     </inline-formula>
     operator represents the dot product. The first addition operand of Equation
     <xref ref-type="disp-formula" rid="Equ1">
      1
     </xref>
     is the cross-entropy on the image axis, while the second addition operand is on the text axis.
     <fig id="Fig1" position="float">
      <label>
       Figure 1
      </label>
      <caption xml:lang="en">
       <p>
        Two-dimensional representation of images and text in FashionCLIP vector space
        <bold>
         before
        </bold>
        and
        <bold>
         after
        </bold>
        training. Images and their corresponding textual descriptions are embedded closer to each other in the latent vector space after training.
       </p>
      </caption>
      <graphic id="MO3" mime-subtype="PNG" specific-use="web" xlink:href="/ProjectMundo-Anon/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig1_HTML.png"/>
     </fig>
    </p>
    <p id="Par9">
     Recently, industry practitioners have begun to recognize the importance and utility of contrastive pre-training for their target domain, with several works presenting successful downstream applications starting from the
     <monospace>
      CLIP
     </monospace>
     model
     <sup>
      <xref ref-type="bibr" rid="CR30">
       30
      </xref>
     </sup>
     . In fashion, the multi-modal nature of CLIP has been found helpful in recent discriminative
     <sup>
      <xref ref-type="bibr" rid="CR31">
       31
      </xref>
      ,
      <xref ref-type="bibr" rid="CR32">
       32
      </xref>
     </sup>
     models, which have been developed under the standard paradigm of task-specific, supervised models. In the generative setup, CLIP often complements a larger framework: for example, CLIP is used to learn linguistically grounded codebooks in Variational Auto Encoders
     <sup>
      <xref ref-type="bibr" rid="CR33">
       33
      </xref>
     </sup>
     or to guide image synthesis and manipulation in diffusion generative models.
     <sup>
      <xref ref-type="bibr" rid="CR34">
       34
      </xref>
      ,
      <xref ref-type="bibr" rid="CR35">
       35
      </xref>
     </sup>
     While interesting for grounding (see below Fig.
     <xref ref-type="fig" rid="Fig8">
      8
     </xref>
     ), the target use case (image generation) and more narrow focus (single task, single dataset) are not readily comparable to
     <monospace>
      FashionCLIP
     </monospace>
     , but instead suggest a possible complementary application in generative use cases. However, no recent CLIP application has been developed to produce industry-wide representations across multiple use cases and datasets. In other words, CLIP has been used only as a pre-trained model, with no attempt to overcome single-task supervised models’ operational and conceptual problems.
    </p>
    <p id="Par10">
     In
     <italic>
      this
     </italic>
     work, we introduce
     <monospace>
      FashionCLIP
     </monospace>
     , a CLIP-based model explicitly trained and tested to produce general product representations for fashion concepts. We train
     <monospace>
      FashionCLIP
     </monospace>
     on a large, high-quality novel fashion dataset: as discussed in the next section, our goal is to establish whether such fine-tuning is sufficient to produce product representations that are transferable in a zero-shot fashion to entirely new datasets.
    </p>
   </sec>
   <sec id="Sec4">
    <title>
     Research question and methodology
    </title>
    <p id="Par11">
     Standard supervised models for vertical-specific applications such as fashion are costly to train and operate, providing a large barrier to entry for SaaS providers and smaller players
     <sup>
      <xref ref-type="bibr" rid="CR12">
       12
      </xref>
     </sup>
     . For example, a product classification model might be trained on
     <inline-formula id="IEq18">
      <alternatives>
       <math id="IEq18_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mo>
          &lt;
         </mo>
         <mi>
          p
         </mi>
         <mi>
          r
         </mi>
         <mi>
          o
         </mi>
         <mi>
          d
         </mi>
         <mi>
          u
         </mi>
         <mi>
          c
         </mi>
         <mi>
          t
         </mi>
         <mspace width="0.166667em"/>
         <mi>
          d
         </mi>
         <mi>
          e
         </mi>
         <mi>
          s
         </mi>
         <mi>
          c
         </mi>
         <mi>
          r
         </mi>
         <mi>
          i
         </mi>
         <mi>
          p
         </mi>
         <mi>
          t
         </mi>
         <mi>
          i
         </mi>
         <mi>
          o
         </mi>
         <mi>
          n
         </mi>
         <mo>
          ,
         </mo>
         <mspace width="0.166667em"/>
         <mi>
          c
         </mi>
         <mi>
          a
         </mi>
         <mi>
          t
         </mi>
         <mi>
          e
         </mi>
         <mi>
          g
         </mi>
         <mi>
          o
         </mi>
         <mi>
          r
         </mi>
         <mi>
          y
         </mi>
         <mo>
          &gt;
         </mo>
        </mrow>
       </math>
       <tex-math id="IEq18_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&lt;product\,description,\,category&gt;$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq18.gif"/>
      </alternatives>
     </inline-formula>
     pairs derived from catalog data
     <sup>
      <xref ref-type="bibr" rid="CR36">
       36
      </xref>
     </sup>
     while optimizing for classification accuracy: if the labels change, or the model is deployed on a different catalog, accuracy would drop. It is important to note that moving to CLIP-based architectures, such as
     <italic>
      CMA-CLIP
     </italic>
     <sup>
      <xref ref-type="bibr" rid="CR37">
       37
      </xref>
     </sup>
     , does not
     <italic>
      ipso facto
     </italic>
     solve the problem: if CLIP is used as a per-task model, it will raise the same scalability issues as traditional supervised methods.
    </p>
    <p id="Par12">
     After training
     <monospace>
      FashionCLIP
     </monospace>
     , we set out to answer a broader, and potentially more impactful question: given the right dataset and fine-tuning procedure, can we learn multi-modal concepts that are general enough for the entire fashion domain? We proceed with a mixture of quantitative benchmarks – inspired both by existing literature and problems known to be important in the industry – and qualitative probes to answer it: since obtaining general concepts is our goal, it is important to verify that
     <monospace>
      FashionCLIP
     </monospace>
     does not only learn a dataset (e.g., an “Armani collection”), but genuine transferable concepts, such as “skirt”, “sleeves”, etc. Taking inspiration from CLIP, our two initial benchmarks will test how
     <monospace>
      FashionCLIP
     </monospace>
     goes from text to image, and vice versa (see Fig.
     <xref ref-type="fig" rid="Fig2">
      2
     </xref>
     ):
     <list list-type="order">
      <list-item>
       <p id="Par13">
        <italic>
         Text to image
        </italic>
        Product search is one of the main channels of interactions and revenues between a shop and its users, accounting on average for 30% to 60% of the total online revenues
        <sup>
         <xref ref-type="bibr" rid="CR38">
          38
         </xref>
         ,
         <xref ref-type="bibr" rid="CR39">
          39
         </xref>
        </sup>
        . Historically, product search has been performed chiefly with textual features by first matching queries and product descriptions in an index
        <sup>
         <xref ref-type="bibr" rid="CR40">
          40
         </xref>
         ,
         <xref ref-type="bibr" rid="CR41">
          41
         </xref>
         –
         <xref ref-type="bibr" rid="CR42">
          42
         </xref>
        </sup>
        and then re-ranking the candidate results
        <sup>
         <xref ref-type="bibr" rid="CR43">
          43
         </xref>
        </sup>
        . However, there are good reasons to believe that including visual features can bring significant improvements since images are often the most curated aspect of the catalog. In contrast, text quality varies throughout verticals, languages, and specific product feeds. Our extensive tests show that
        <monospace>
         FashionCLIP
        </monospace>
        learns fashion concepts, and successfully applies them to unseen products and incomplete or ambiguous descriptions.
       </p>
      </list-item>
      <list-item>
       <p id="Par14">
        <italic>
         Image to text
        </italic>
        Product classification is the task of predicting a product category given its meta-data. Classification (even for CLIP-based models, such as CMA-CLIP) is cast as a supervised learning problem where one extracts golden labels from the catalog itself or collects them through crowd-sourcing
        <sup>
         <xref ref-type="bibr" rid="CR7">
          7
         </xref>
         ,
         <xref ref-type="bibr" rid="CR44">
          44
         </xref>
        </sup>
        . Generalizing classification to arbitrary labels without constant retraining is again crucial for making ML feasible across numerous players in the fashion industry: transferable concepts help with the interoperability of overlapping, and yet different, fashion taxonomies
        <sup>
         <xref ref-type="bibr" rid="CR45">
          45
         </xref>
        </sup>
        , a challenge increasingly recognized as central by both practitioners and commentators
        <sup>
         <xref ref-type="bibr" rid="CR46">
          46
         </xref>
        </sup>
        (this includes the case of catalogs in less represented languages, for which an English classification is still desirable). Our extensive tests show that
        <monospace>
         FashionCLIP
        </monospace>
        zero-shot capabilities, based on learned associations between vision and textual concepts, allow for quick classification of products in target classes of interest,
        <italic>
         irrespective of the specific labeling schemes of individual suppliers
        </italic>
        .
       </p>
      </list-item>
     </list>
     We also perform specific probing to understand whether the concepts learned by the model are robust and (somehow) aligned with human semantic intuitions, as opposed to picking up spurious correlations in the dataset
     <sup>
      <xref ref-type="bibr" rid="CR47">
       47
      </xref>
     </sup>
     :
     <list list-type="order">
      <list-item>
       <p id="Par15">
        <italic>
         Grounding
        </italic>
        . We probe FashionCLIP for grounding capabilities through localization maps and apply them to the task of zero-shot semantic segmentation for fashion concepts (e.g., sleeve length, texture patterns) and attributes (e.g.,
        <italic>
         laced
        </italic>
        shoes,
        <italic>
         glittered
        </italic>
        shirts).
       </p>
      </list-item>
      <list-item>
       <p id="Par16">
        <italic>
         Compositionality
        </italic>
        . We propose a novel way to probe whether the model can go from semantic segmentation to inferential abilities by composing such concepts to generate new linguistic expressions. We do that through the device of “improbable object”, where a linguistic expression is meant to describe an odd combination of concepts that have never been observed before (e.g., a pair of shoes with handles).
       </p>
      </list-item>
     </list>
     We summarize our contributions as follows:
     <list list-type="order">
      <list-item>
       <p id="Par17">
        while other researchers have independently developed CLIP-based solutions for individual fashion problems,
        <monospace>
         FashionCLIP
        </monospace>
        is the first explicit attempt to produce general multi-modal concepts for industry: the breadth and nature of our testing methodology make
        <monospace>
         FashionCLIP
        </monospace>
        appealing as a general fashion model, applicable to situations where supervised systems are not practical or viable. Our model is trained on over 700k
        <inline-formula id="IEq19">
         <alternatives>
          <math id="IEq19_Math" xmlns="http://www.w3.org/1998/Math/MathML">
           <mrow>
            <mo>
             &lt;
            </mo>
            <mi>
             i
            </mi>
            <mi>
             m
            </mi>
            <mi>
             a
            </mi>
            <mi>
             g
            </mi>
            <mi>
             e
            </mi>
            <mo>
             ,
            </mo>
            <mspace width="0.166667em"/>
            <mi>
             t
            </mi>
            <mi>
             e
            </mi>
            <mi>
             x
            </mi>
            <mi>
             t
            </mi>
            <mo>
             &gt;
            </mo>
           </mrow>
          </math>
          <tex-math id="IEq19_TeX">
           \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${&lt;image,\,text&gt;}$$\end{document}
          </tex-math>
          <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq19.gif"/>
         </alternatives>
        </inline-formula>
        pairs from the inventory of
        <italic>
         Farfetch
        </italic>
        , one of the largest fashion luxury retailers in the world, and is shown to be useful in important use cases in a vast global market;
       </p>
      </list-item>
      <list-item>
       <p id="Par18">
        we evaluate
        <monospace>
         FashionCLIP
        </monospace>
        in various tasks, showing that fine-tuning helps capture domain-specific concepts and generalizes them in zero-shot scenarios; we supplement quantitative tests with qualitative analyses and offer insights into how concepts grounded in a visual space unlock linguistic generalization. These results would not be possible without the flexibility provided by natural language as a supervision signal and the domain-specific accuracy achieved through fine-tuning;
       </p>
      </list-item>
      <list-item>
       <p id="Par19">
        we transparently report training time, costs, and emissions. We additionally release to the community, under an open-source license, training code, a demo app, and plug-and-play checkpoints to help leverage our findings while facilitating ROI considerations
        <sup>
         <xref ref-type="bibr" rid="CR48">
          48
         </xref>
         ,
         <xref ref-type="bibr" rid="CR49">
          49
         </xref>
        </sup>
        ; the large and unique dataset is also scheduled to be released directly by
        <italic>
         Farfetch
        </italic>
        . Taken together,
        <monospace>
         FashionCLIP
        </monospace>
        artifacts (model, demo, data) are a foundational toolkit for practitioners in the space and a template for other verticalized CLIP models (https://github.com/patrickjohncyh/fashion-clip).
       </p>
      </list-item>
     </list>
     We believe that our methods and results are interesting not just for the fashion industry but broadly speaking for the ever-expanding industry of online retail, as our artifacts, use cases and benchmarks might serve as a blueprint for other vertical-specific applications of large multi-modal models. Finally, adding to the industry significance of the work, the evaluation in  “
     <xref ref-type="sec" rid="Sec8">
      Grounding and Compositionality
     </xref>
     ” section is new in the context of
     <monospace>
      CLIP
     </monospace>
     -like models, and we believe it may be of independent interest for future work in NLP. As a matter of fact, showcasing a practical thread connecting generalization and latent space interpretation to industrial scalability may be the most interesting contribution of
     <monospace>
      FashionCLIP
     </monospace>
     .
     <fig id="Fig2" position="float">
      <label>
       Figure 2
      </label>
      <caption xml:lang="en">
       <p>
        Schematic overview of multi-modal retrieval (left) and zero-shot classification tasks (right).
       </p>
      </caption>
      <graphic id="MO4" mime-subtype="PNG" specific-use="web" xlink:href="/ProjectMundo-Anon/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig2_HTML.png"/>
     </fig>
    </p>
   </sec>
  </sec>
  <sec id="Sec5" sec-type="results">
   <title>
    Results
   </title>
   <p id="Par20">
    In this section, we detail the performance of
    <monospace>
     FashionCLIP
    </monospace>
    over a range of tasks, demonstrating the efficacy of domain adaptation and the applicability of
    <monospace>
     CLIP
    </monospace>
    -like models to fashion. Details on the training and on the evaluation are available in the “
    <xref ref-type="sec" rid="Sec12">
     Methods
    </xref>
    ” Section. We leverage a variety of in-domain and out of domain datasets, with varying degrees of similarity:
    <bold>
     TEST
    </bold>
    is the test set from
    <italic>
     Farfetch
    </italic>
    containing 20k products;
    <bold>
     HOUT-C
    </bold>
    is the dataset containing a category which we excluded from training;
    <bold>
     HOUT-B
    </bold>
    is the dataset containing two brands which were excluded from training;
    <bold>
     STLE
    </bold>
    is a merchandising dataset from
    <italic>
     Farfetch
    </italic>
    ;
    <bold>
     KAGL
    </bold>
    is a subset of
    <sup>
     <xref ref-type="bibr" rid="CR50">
      50
     </xref>
    </sup>
    , where each product has a white background image, a caption, and a category;
    <bold>
     F-MNIST
    </bold>
    <sup>
     <xref ref-type="bibr" rid="CR51">
      51
     </xref>
    </sup>
    contains 10, 000 gray-scale images from 10 product classes;
    <bold>
     DEEP
    </bold>
    <sup>
     <xref ref-type="bibr" rid="CR52">
      52
     </xref>
    </sup>
    contains 4000 product images that are non-standardized (i.e., contain humans) from 50 categories. An overview of image and textual data offered by Farfetch (
    <bold>
     TEST, HOUT-C, HOUT-B, STLE
    </bold>
    ),
    <bold>
     KAGL
    </bold>
    ,
    <bold>
     F-MNIST
    </bold>
    and
    <bold>
     DEEP
    </bold>
    can be found in Fig.
    <xref ref-type="fig" rid="Fig3">
     3
    </xref>
    . Our extensive benchmarks and evaluations answer two research questions quantitatively: can domain-specific knowledge improve CLIP understanding of an industry (Fig.
    <xref ref-type="fig" rid="Fig5">
     5
    </xref>
    ) and, if yes, does that knowledge translate across different use cases and datasets?
    <fig id="Fig3" position="float">
     <label>
      Figure 3
     </label>
     <caption xml:lang="en">
      <p>
       Sample of data from various datasets used. We observe a range of distributions on both the image and textual modalities. For the image modality, we see a range from “Low resolution, B &amp;W” to “High resolution, In-the-Wild”. For the textual modality, Farfetch offers the best “textual resolution”, while DEEP also has very fashion specific terminology. The
       <bold>
        KAGL
       </bold>
       ,
       <bold>
        F-MNIST
       </bold>
       , and
       <bold>
        DEEP
       </bold>
       datasets are publicly available. For more details regarding the data, see the Data Availability Section.
      </p>
     </caption>
     <graphic id="MO5" mime-subtype="PNG" specific-use="web" xlink:href="/ProjectMundo-Anon/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig3_HTML.png"/>
    </fig>
   </p>
   <sec id="Sec6">
    <title>
     Multi-modal retrieval
    </title>
    <p id="Par21">
     The Multi-modal Retrieval task is described as follows: given a textual description and a set of images, we ask the model to find the image related to that description. For example, a product retrieval task entails matching a product description (e.g., “a red polo for men”) and a photo of it in a catalog.
    </p>
    <p id="Par22">
     Multi-modal retrieval is possible due to the optimization objective of
     <monospace>
      FashionCLIP
     </monospace>
     which aligns the language and image latent spaces (see Fig.
     <xref ref-type="fig" rid="Fig1">
      1
     </xref>
     ). We test
     <monospace>
      FashionCLIP
     </monospace>
     on multi-modal retrieval to assess the benefits of domain-specific fine-tuning on real-world product search.
    </p>
    <p id="Par23">
     Our benchmark takes as input a product description from the catalog’s
     <italic>
      test set
     </italic>
     and asks models to rank product images corresponding to the caption—the gold standard is the image associated with the product. We extract the ranking using embedding similarities:
     <monospace>
      FashionCLIP
     </monospace>
     performs the dot product between the input caption embedding and each image vector embedding obtained via
     <inline-formula id="IEq20">
      <alternatives>
       <math id="IEq20_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <msub>
          <mover accent="true">
           <mrow>
            <mi>
             T
            </mi>
           </mrow>
           <mrow>
            <mo stretchy="false">
             ¯
            </mo>
           </mrow>
          </mover>
          <msup>
           <mi>
            θ
           </mi>
           <mi>
            T
           </mi>
          </msup>
         </msub>
         <mrow>
          <mo stretchy="false">
           (
          </mo>
          <mo>
           ·
          </mo>
          <mo stretchy="false">
           )
          </mo>
         </mrow>
        </mrow>
       </math>
       <tex-math id="IEq20_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar{T}_{\theta ^T}(\cdot )$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq20.gif"/>
      </alternatives>
     </inline-formula>
     and
     <inline-formula id="IEq21">
      <alternatives>
       <math id="IEq21_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <msub>
          <mover accent="true">
           <mrow>
            <mi>
             I
            </mi>
           </mrow>
           <mrow>
            <mo stretchy="false">
             ¯
            </mo>
           </mrow>
          </mover>
          <msup>
           <mi>
            θ
           </mi>
           <mi>
            I
           </mi>
          </msup>
         </msub>
         <mrow>
          <mo stretchy="false">
           (
          </mo>
          <mo>
           ·
          </mo>
          <mo stretchy="false">
           )
          </mo>
         </mrow>
        </mrow>
       </math>
       <tex-math id="IEq21_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar{I}_{\theta ^I}(\cdot )$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq21.gif"/>
      </alternatives>
     </inline-formula>
     respectively and returns a rank based on descending order. We use
     <italic>
      HITS@5
     </italic>
     <sup>
      <xref ref-type="bibr" rid="CR53">
       53
      </xref>
     </sup>
     (Hit Rate @
     <inline-formula id="IEq22">
      <alternatives>
       <math id="IEq22_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <mi>
          k
         </mi>
         <mo>
          =
         </mo>
         <mn>
          5
         </mn>
        </mrow>
       </math>
       <tex-math id="IEq22_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k=5$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq22.gif"/>
      </alternatives>
     </inline-formula>
     ) and
     <italic>
      MRR
     </italic>
     <sup>
      <xref ref-type="bibr" rid="CR54">
       54
      </xref>
     </sup>
     (Mean Recirpocal Rank) as our metrics. Table
     <xref ref-type="table" rid="Tab1">
      1
     </xref>
     compares
     <monospace>
      FashionCLIP
     </monospace>
     against non-domain specific
     <monospace>
      CLIP
     </monospace>
     on different heldout test sets and shows how fine-tuning significantly improves the understanding of our target domain.
    </p>
    <p id="Par24">
     We also perform extensive qualitative tests comparing
     <monospace>
      FashionCLIP
     </monospace>
     with the production search engine presently employed in the catalog. Fig.
     <xref ref-type="fig" rid="Fig4">
      4
     </xref>
     shows a case of particular interest for product search: in this example, visual concepts do not belong to the fashion domain and are not available in the caption. The first comparison (
     <italic>
      left
     </italic>
     ) shows that
     <monospace>
      FashionCLIP
     </monospace>
     can recover the concept of
     <italic>
      tiger
     </italic>
     when prompted with “t-shirt with tiger”; for the same query, the search engine retrieves items matching the category, unable to interpret
     <italic>
      tiger
     </italic>
     based solely on text. The second comparison (
     <italic>
      right
     </italic>
     ) shows that
     <monospace>
      FashionCLIP
     </monospace>
     can interpret
     <italic>
      a cat
     </italic>
     from a stylized, partially occluded drawing. In contrast, the search engine fails to generalize beyond the captions explicitly containing the string “cat”. Finally, visualizing the learnt embeddings (Fig.
     <xref ref-type="fig" rid="Fig5">
      5
     </xref>
     ) also helps to build an intuition of
     <monospace>
      FashionCLIP
     </monospace>
     ’s better conceptual resolution when it comes to the target domain.
     <fig id="Fig4" position="float">
      <label>
       Figure 4
      </label>
      <caption xml:lang="en">
       <p>
        Retrieval with non-fashion concepts. Sample results for “t-shirt with tiger” and “t-shirt with cat” from
        <monospace>
         FashionCLIP
        </monospace>
        (
        <italic>
         green
        </italic>
        ) vs
        <italic>
         Farfetch
        </italic>
        production search engine (
        <italic>
         red
        </italic>
        ).
       </p>
      </caption>
      <graphic id="MO6" mime-subtype="PNG" specific-use="web" xlink:href="/ProjectMundo-Anon/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig4_HTML.png"/>
     </fig>
     <table-wrap id="Tab1">
      <label>
       Table 1
      </label>
      <caption xml:lang="en">
       <p>
        Comparing
        <monospace>
         FashionCLIP
        </monospace>
        (
        <monospace>
         F-CLIP
        </monospace>
        ) vs
        <monospace>
         CLIP
        </monospace>
        on the multi-modal retrieval task.
       </p>
      </caption>
      <table frame="hsides" rules="groups">
       <thead>
        <tr>
         <th align="left">
          <p>
           Model
          </p>
         </th>
         <th align="left">
          <p>
           Dataset
          </p>
         </th>
         <th align="left">
          <p>
           HITS@5
          </p>
         </th>
         <th align="left">
          <p>
           MRR
          </p>
         </th>
        </tr>
       </thead>
       <tbody>
        <tr>
         <td align="left">
          <p>
           <monospace>
            F-CLIP
           </monospace>
          </p>
         </td>
         <td align="left" rowspan="2">
          <p>
           TEST
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.66
           </bold>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.50
           </bold>
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.28
          </p>
         </td>
         <td align="left">
          <p>
           0.21
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            F-CLIP
           </monospace>
          </p>
         </td>
         <td align="left" rowspan="2">
          <p>
           HOUT-C
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.62
           </bold>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.47
           </bold>
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.33
          </p>
         </td>
         <td align="left">
          <p>
           0.23
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            F-CLIP
           </monospace>
          </p>
         </td>
         <td align="left" rowspan="2">
          <p>
           HOUT-B
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.58
           </bold>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.41
           </bold>
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.31
          </p>
         </td>
         <td align="left">
          <p>
           0.22
          </p>
         </td>
        </tr>
       </tbody>
      </table>
      <table-wrap-foot>
       <p>
        Best performing models are in bold.
       </p>
      </table-wrap-foot>
     </table-wrap>
     <table-wrap id="Tab2">
      <label>
       Table 2
      </label>
      <caption xml:lang="en">
       <p>
        Comparing the performance of
        <monospace>
         FashionCLIP
        </monospace>
        (
        <monospace>
         F-CLIP
        </monospace>
        ) on product classification task over several datasets (
        <bold>
         F1
        </bold>
        is
        <italic>
         weighted macro F1
        </italic>
        ).
       </p>
      </caption>
      <table frame="hsides" rules="groups">
       <thead>
        <tr>
         <th align="left">
          <p>
           Model
          </p>
         </th>
         <th align="left">
          <p>
           Dataset
          </p>
         </th>
         <th align="left">
          <p>
           F1
          </p>
         </th>
        </tr>
       </thead>
       <tbody>
        <tr>
         <td align="left">
          <p>
           <monospace>
            F-CLIP
           </monospace>
          </p>
         </td>
         <td align="left" rowspan="2">
          <p>
           TEST
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.39
           </bold>
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.31
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            F-CLIP
           </monospace>
          </p>
         </td>
         <td align="left" rowspan="2">
          <p>
           KAGL
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.67
           </bold>
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.63
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            F-CLIP
           </monospace>
          </p>
         </td>
         <td align="left" rowspan="2">
          <p>
           F-MNIST
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.71
           </bold>
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.66
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            F-CLIP
           </monospace>
          </p>
         </td>
         <td align="left" rowspan="2">
          <p>
           DEEP
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.47
           </bold>
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.45
          </p>
         </td>
        </tr>
       </tbody>
      </table>
      <table-wrap-foot>
       <p>
        Best performing models are in bold.
       </p>
      </table-wrap-foot>
     </table-wrap>
    </p>
   </sec>
   <sec id="Sec7">
    <title>
     Zero-shot classification
    </title>
    <p id="Par25">
     We replicate
     <monospace>
      CLIP
     </monospace>
     ’s original zero-shot classification setup
     <sup>
      <xref ref-type="bibr" rid="CR17">
       17
      </xref>
     </sup>
     , which allows us to quantitatively assess the transferability of
     <monospace>
      FashionCLIP
     </monospace>
     ’s fine-tuned representations to different data distributions from the same vertical (i.e. Fashion). The model generates
     <italic>
      one
     </italic>
     image embedding for the product image, and
     <italic>
      k
     </italic>
     text embeddings, one for each of the labels in the classification scheme (e.g., “shoes”, “shirt”). The predicted label is the one that is closer (measured via dot product) to the image in the model’s vector space. We use
     <italic>
      weighted macro F1
     </italic>
     <sup>
      <xref ref-type="bibr" rid="CR55">
       55
      </xref>
     </sup>
     as the performance metric. Table
     <xref ref-type="table" rid="Tab2">
      2
     </xref>
     summarizes the results of different SOTA benchmarks. On all the tested benchmarks,
     <monospace>
      FashionCLIP
     </monospace>
     is superior to
     <monospace>
      CLIP
     </monospace>
     , a result which suggests that domain-specific fine-tuning is indeed useful in-domain and that it generalizes to other, completely unseen datasets.
    </p>
    <p id="Par26">
     Furthermore, we set out to investigate the “cheating hypothesis” on our domain-specific model, i.e., the hypothesis that supervised models do not generalize as well as
     <monospace>
      CLIP
     </monospace>
     because they fit spurious features unique to each dataset. We freeze the image encoder from
     <monospace>
      FashionCLIP
     </monospace>
     and fine-tune a linear classifier,
     <monospace>
      LINEAR
     </monospace>
     , over the embeddings generated on a subset of categories (47) from the validation set from
     <italic>
      Farfetch
     </italic>
     . We run benchmarks on
     <inline-formula id="IEq23">
      <alternatives>
       <math id="IEq23_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi mathvariant="bold">
          TEST
         </mi>
         <mi mathvariant="bold">
          S
         </mi>
        </msub>
       </math>
       <tex-math id="IEq23_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {TEST_S}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq23.gif"/>
      </alternatives>
     </inline-formula>
     ,
     <inline-formula id="IEq24">
      <alternatives>
       <math id="IEq24_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi mathvariant="bold">
          KAGL
         </mi>
         <mi mathvariant="bold">
          S
         </mi>
        </msub>
       </math>
       <tex-math id="IEq24_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {KAGL_S}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq24.gif"/>
      </alternatives>
     </inline-formula>
     ,
     <inline-formula id="IEq25">
      <alternatives>
       <math id="IEq25_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi mathvariant="bold">
         F
        </mi>
       </math>
       <tex-math id="IEq25_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {F}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq25.gif"/>
      </alternatives>
     </inline-formula>
     <bold>
      -
     </bold>
     <inline-formula id="IEq26">
      <alternatives>
       <math id="IEq26_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi mathvariant="bold">
          MNIST
         </mi>
         <mi mathvariant="bold">
          S
         </mi>
        </msub>
       </math>
       <tex-math id="IEq26_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {MNIST_S}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq26.gif"/>
      </alternatives>
     </inline-formula>
     and
     <inline-formula id="IEq27">
      <alternatives>
       <math id="IEq27_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mi mathvariant="bold">
          DEEP
         </mi>
         <mi mathvariant="bold">
          S
         </mi>
        </msub>
       </math>
       <tex-math id="IEq27_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {DEEP_S}$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq27.gif"/>
      </alternatives>
     </inline-formula>
     , sub-sampled versions of the respective datasets. Where labels are different, we adapt
     <monospace>
      LINEAR
     </monospace>
     to the labels by pooling the scores from relevant classes. We compare this to zero-shot performance, using the original labels to generate the text embeddings.
    </p>
    <p id="Par27">
     Table
     <xref ref-type="table" rid="Tab3">
      3
     </xref>
     reports our findings, which are partially similar to those from
     <monospace>
      CLIP
     </monospace>
     <sup>
      <xref ref-type="bibr" rid="CR17">
       17
      </xref>
     </sup>
     . Given that
     <bold>
      F-MNIST
     </bold>
     is very different from
     <bold>
      TEST
     </bold>
     —comparable, for example, to CIFAR-100
     <sup>
      <xref ref-type="bibr" rid="CR56">
       56
      </xref>
     </sup>
     vs. ImageNet
     <sup>
      <xref ref-type="bibr" rid="CR57">
       57
      </xref>
     </sup>
     —the decrease in performance may be an indication of cheating. However,
     <monospace>
      LINEAR
     </monospace>
     performs well on the other datasets, with the biggest gain for
     <bold>
      KAGL
     </bold>
     , whose product image most resembles those in
     <bold>
      TEST
     </bold>
     (i.e., high-resolution items on a white background). Compared to the original setting
     <sup>
      <xref ref-type="bibr" rid="CR17">
       17
      </xref>
     </sup>
     , one may argue that the supervised model has an easier job in our case: much fewer categories (
     <inline-formula id="IEq28">
      <alternatives>
       <math id="IEq28_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mn>
          10
         </mn>
         <mn>
          1
         </mn>
        </msup>
       </math>
       <tex-math id="IEq28_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^1$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq28.gif"/>
      </alternatives>
     </inline-formula>
     vs.
     <inline-formula id="IEq29">
      <alternatives>
       <math id="IEq29_Math" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
         <msup>
          <mn>
           10
          </mn>
          <mn>
           3
          </mn>
         </msup>
         <mrow>
          <mo stretchy="false">
           )
          </mo>
         </mrow>
        </mrow>
       </math>
       <tex-math id="IEq29_TeX">
        \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^3)$$\end{document}
       </tex-math>
       <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq29.gif"/>
      </alternatives>
     </inline-formula>
     and relatively homogeneous items,
     <bold>
      F-MNIST
     </bold>
     aside.
     <table-wrap id="Tab3">
      <label>
       Table 3
      </label>
      <caption xml:lang="en">
       <p>
        <monospace>
         LINEAR
        </monospace>
        classification performance relative to zero-shot on
        <monospace>
         F-CLIP
        </monospace>
        (
        <bold>
         F1
        </bold>
        is
        <italic>
         weighted macro F1
        </italic>
        ).
       </p>
      </caption>
      <table frame="hsides" rules="groups">
       <thead>
        <tr>
         <th align="left">
          <p>
           Dataset
          </p>
         </th>
         <th align="left">
          <p>
           F-CLIP
          </p>
         </th>
         <th align="left">
          <p>
           LINEAR
          </p>
         </th>
         <th align="left">
          <p>
           <inline-formula id="IEq30">
            <alternatives>
             <math id="IEq30_Math" xmlns="http://www.w3.org/1998/Math/MathML">
              <mi mathvariant="normal">
               Δ
              </mi>
             </math>
             <tex-math id="IEq30_TeX">
              \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta $$\end{document}
             </tex-math>
             <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq30.gif"/>
            </alternatives>
           </inline-formula>
           F1
          </p>
         </th>
        </tr>
       </thead>
       <tbody>
        <tr>
         <td align="left">
          <p>
           <inline-formula id="IEq31">
            <alternatives>
             <math id="IEq31_Math" xmlns="http://www.w3.org/1998/Math/MathML">
              <msub>
               <mi mathvariant="monospace">
                TEST
               </mi>
               <mi mathvariant="monospace">
                S
               </mi>
              </msub>
             </math>
             <tex-math id="IEq31_TeX">
              \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\texttt {TEST}_\texttt {S}$$\end{document}
             </tex-math>
             <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq31.gif"/>
            </alternatives>
           </inline-formula>
          </p>
         </td>
         <td align="left">
          <p>
           0.746
          </p>
         </td>
         <td align="left">
          <p>
           0.900
          </p>
         </td>
         <td align="left">
          <p>
           + 0.154
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <inline-formula id="IEq32">
            <alternatives>
             <math id="IEq32_Math" xmlns="http://www.w3.org/1998/Math/MathML">
              <msub>
               <mi mathvariant="monospace">
                KAGL
               </mi>
               <mi mathvariant="monospace">
                S
               </mi>
              </msub>
             </math>
             <tex-math id="IEq32_TeX">
              \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\texttt {KAGL}_\texttt {S}$$\end{document}
             </tex-math>
             <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq32.gif"/>
            </alternatives>
           </inline-formula>
          </p>
         </td>
         <td align="left">
          <p>
           0.764
          </p>
         </td>
         <td align="left">
          <p>
           0.881
          </p>
         </td>
         <td align="left">
          <p>
           + 0.117
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <inline-formula id="IEq33">
            <alternatives>
             <math id="IEq33_Math" xmlns="http://www.w3.org/1998/Math/MathML">
              <msub>
               <mi mathvariant="monospace">
                DEEP
               </mi>
               <mi mathvariant="monospace">
                S
               </mi>
              </msub>
             </math>
             <tex-math id="IEq33_TeX">
              \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\texttt {DEEP}_\texttt {S}$$\end{document}
             </tex-math>
             <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq33.gif"/>
            </alternatives>
           </inline-formula>
          </p>
         </td>
         <td align="left">
          <p>
           0.411
          </p>
         </td>
         <td align="left">
          <p>
           0.444
          </p>
         </td>
         <td align="left">
          <p>
           + 0.033
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <inline-formula id="IEq34">
            <alternatives>
             <math id="IEq34_Math" xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
               <mi mathvariant="monospace">
                F
               </mi>
               <mo>
                -
               </mo>
               <msub>
                <mi mathvariant="monospace">
                 MNIST
                </mi>
                <mi mathvariant="monospace">
                 S
                </mi>
               </msub>
              </mrow>
             </math>
             <tex-math id="IEq34_TeX">
              \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\texttt {F}-\texttt {MNIST}_\texttt {S}$$\end{document}
             </tex-math>
             <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq34.gif"/>
            </alternatives>
           </inline-formula>
          </p>
         </td>
         <td align="left">
          <p>
           0.781
          </p>
         </td>
         <td align="left">
          <p>
           0.602
          </p>
         </td>
         <td align="left">
          <p>
           − 0.179
          </p>
         </td>
        </tr>
       </tbody>
      </table>
     </table-wrap>
     <table-wrap id="Tab4">
      <label>
       Table 4
      </label>
      <caption xml:lang="en">
       <p>
        F1 macro on
        <bold>
         STLE
        </bold>
        ;
        <monospace>
         Prior
        </monospace>
        classifies using empirical class probabilities.
       </p>
      </caption>
      <table frame="hsides" rules="groups">
       <thead>
        <tr>
         <th align="left">
          <p>
           Model
          </p>
         </th>
         <th align="left">
          <p>
           Man
          </p>
         </th>
         <th align="left">
          <p>
           Woman
          </p>
         </th>
        </tr>
       </thead>
       <tbody>
        <tr>
         <td align="left">
          <p>
           <monospace>
            Prior
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.24
          </p>
         </td>
         <td align="left">
          <p>
           0.20
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            F-CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.36
           </bold>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.27
           </bold>
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <monospace>
            CLIP
           </monospace>
          </p>
         </td>
         <td align="left">
          <p>
           0.33
          </p>
         </td>
         <td align="left">
          <p>
           0.17
          </p>
         </td>
        </tr>
       </tbody>
      </table>
      <table-wrap-foot>
       <p>
        Best performing models are in bold.
       </p>
      </table-wrap-foot>
     </table-wrap>
    </p>
    <p id="Par28">
     While we leave the investigation of fashion classification in more ecological settings as future work, our results contain actionable insights for real-world deployments. In particular, supervised classifiers still require a good deal of manual intervention even for similar datasets, and they are utterly unusable on neighboring yet different problems. Table
     <xref ref-type="table" rid="Tab4">
      4
     </xref>
     reports performance on
     <bold>
      STLE
     </bold>
     divided by Man- and Woman-related items. Products in the dataset still come from
     <italic>
      Farfetch
     </italic>
     , but labels are manually assigned by merchandisers and are orthogonal to the taxonomy (
     <italic>
      classic, streetwear, edgy
     </italic>
     vs.
     <italic>
      shoes, hats, bags
     </italic>
     ). The versatility afforded by language supervision allows zero-shot models to tackle the challenge by simple prompt engineering (“an item in
     <italic>
      classic
     </italic>
     style”); in contrast, supervised models would require a new training and evaluation pipeline. As emphasized above, learning general fashion concepts is the main motivation behind
     <italic>
      this
     </italic>
     work: while specific, supervised pipelines may still be the best choice for specific problems, they are no longer the only viable option in multi-tasks scenarios thanks to the advent of large-scale models such as
     <monospace>
      FashionCLIP
     </monospace>
     . Although no single answer can fit all the use cases, we wish to encourage data-driven decision-making by charting all the options and providing cost and performance assessments.
     <fig id="Fig5" position="float">
      <label>
       Figure 5
      </label>
      <caption xml:lang="en">
       <p>
        Comparison of
        <monospace>
         F-CLIP
        </monospace>
        and
        <monospace>
         CLIP
        </monospace>
        Image Space T-SNE projection. We observe better clustering (0.115 vs 0.0745 silhouette score
        <sup>
         <xref ref-type="bibr" rid="CR58">
          58
         </xref>
        </sup>
        ) in
        <monospace>
         F-CLIP
        </monospace>
        for categories such as Shirts, Skirts and Dresses, where the products form a denser cluster with less overlap between categories, suggesting that the
        <monospace>
         F-CLIP
        </monospace>
        latent space is better tuned for fashion concepts.
       </p>
      </caption>
      <graphic id="MO7" mime-subtype="PNG" specific-use="web" xlink:href="/ProjectMundo-Anon/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig5_HTML.png"/>
     </fig>
    </p>
   </sec>
   <sec id="Sec8">
    <title>
     Grounding and compositionality
    </title>
    <p id="Par29">
     As argued in the
     <italic>
      Introduction
     </italic>
     , given that we are interested in establishing a connection between generality and scalability through large multi-modal models, it is important to further evaluate the quality of the learned representations. While the question of whether
     <monospace>
      FashionCLIP
     </monospace>
     <italic>
      learns
     </italic>
     fashion has been addressed quantitatively above, we are also interested in evaluating the model from a broader theoretical perspective of language understanding, offering a glimpse of the extent of
     <monospace>
      FashionCLIP
     </monospace>
     ’s “true” generalization capabilities,
     <italic>
      ala
     </italic>
     “infinite use of finite means”
     <sup>
      <xref ref-type="bibr" rid="CR59">
       59
      </xref>
     </sup>
     .
    </p>
    <p id="Par30">
     The literature on language compositionality spans centuries: limiting ourselves only to recent work,
     <italic>
      grounding
     </italic>
     has been explored in connection with efficient learning
     <sup>
      <xref ref-type="bibr" rid="CR60">
       60
      </xref>
      ,
      <xref ref-type="bibr" rid="CR61">
       61
      </xref>
     </sup>
     , and “true understanding”
     <sup>
      <xref ref-type="bibr" rid="CR62">
       62
      </xref>
      ,
      <xref ref-type="bibr" rid="CR63">
       63
      </xref>
     </sup>
     . Using combinatorial principles to test generalization abilities is a known strategy in toy world
     <sup>
      <xref ref-type="bibr" rid="CR64">
       64
      </xref>
      ,
      <xref ref-type="bibr" rid="CR65">
       65
      </xref>
     </sup>
     : we exploit insights from our target domain to operationalize similar principles on
     <italic>
      real-world
     </italic>
     objects.
    </p>
    <p id="Par31">
     In this section, we provide evidence of semantic grounding in
     <monospace>
      FashionCLIP
     </monospace>
     and build on that to offer a preliminary investigation of its compositional abilities. Our analysis starts from two lessons from previous research. First,
     <italic>
      localization maps
     </italic>
     <sup>
      <xref ref-type="bibr" rid="CR66">
       66
      </xref>
      ,
      <xref ref-type="bibr" rid="CR67">
       67
      </xref>
     </sup>
     are an effective way to probe the model for
     <italic>
      referential
     </italic>
     knowledge
     <sup>
      <xref ref-type="bibr" rid="CR68">
       68
      </xref>
     </sup>
     (we borrow here the referential/inferential distinction from the classic work by Marconi
     <sup>
      <xref ref-type="bibr" rid="CR69">
       69
      </xref>
     </sup>
     ) and visually grounded lexical knowledge. Second, from a linguistic point of view most search queries in fashion have the form of Noun Phrases (NPs)—e.g. “armani dress”. Therefore, the semantics of NP can be considered a good real-world generalization
     <sup>
      <xref ref-type="bibr" rid="CR9">
       9
      </xref>
      ,
      <xref ref-type="bibr" rid="CR70">
       70
      </xref>
     </sup>
     for studying
     <monospace>
      FashionCLIP
     </monospace>
     <italic>
      compositional
     </italic>
     and
     <italic>
      inferential
     </italic>
     abilities.
     <fig id="Fig6" position="float">
      <label>
       Figure 6
      </label>
      <caption xml:lang="en">
       <p>
        Grounded lexical knowledge. Maps are easy-to-use probes into the model fashion knowledge.
        <italic>
         Left to right
        </italic>
        : localization map for “long sleeves” on a red polo; sneakers and the map for “Nike”, a phone cover and the map for “Palm Angels”; the same phone cover and map, when the logo is written with an out-of-distribution font in a new spot.
       </p>
      </caption>
      <graphic id="MO8" mime-subtype="PNG" specific-use="web" xlink:href="/ProjectMundo-Anon/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig6_HTML.png"/>
     </fig>
    </p>
    <sec id="Sec9">
     <title>
      Grounding
     </title>
     <p id="Par32">
      We probe
      <monospace>
       FashionCLIP
      </monospace>
      for evidence of referential knowledge and investigate its grounding capabilities by utilizing localization maps. We further apply localization maps to the task of zero-shot fashion parsing—a crucial open problem in the industry
      <sup>
       <xref ref-type="bibr" rid="CR71">
        71
       </xref>
      </sup>
      .
     </p>
     <p id="Par33">
      Localization maps are obtained by repeatedly occluding different parts of the image. We then encode each occluded version and measure its distance from the target text in the contrastive space. Intuitively, the farther the image is pushed away by the occlusion, the stronger the linkage was between the removed visual concept and the text and, in turn, the higher its score on the map. Fashion parsing is a specific case of semantic segmentation where bounding box annotations contain clothing items. We extract bounding box annotations (as an approximation of fine-grained segmentation) from localization maps by finding the minimum bounding rectangle of highly activated areas.
     </p>
     <p id="Par34">
      As shown in Figs.
      <xref ref-type="fig" rid="Fig6">
       6
      </xref>
      and
      <xref ref-type="fig" rid="Fig8">
       8
      </xref>
      , features such as “high heels”, “ankle strap”, “long sleeves” are well represented in
      <monospace>
       FashionCLIP
      </monospace>
      ; the model also seems to be very aware of brands, in more or less explicit form.
      <monospace>
       FashionCLIP
      </monospace>
      picks up the abstract logo on
      <italic>
       sneakers
      </italic>
      (Fig.
      <xref ref-type="fig" rid="Fig6">
       6
      </xref>
      ), as well as showing (similar to
      <monospace>
       CLIP
      </monospace>
      ) good OCR capabilities, when recognizing a logo as an explicit text string. Fig.
      <xref ref-type="fig" rid="Fig7">
       7
      </xref>
      shows zero-shot bounding box annotations of some samples in the previously unseen ModaNet
      <sup>
       <xref ref-type="bibr" rid="CR72">
        72
       </xref>
      </sup>
      dataset. While it is unlikely that zero-shot models could replace specialized segmentation training, we believe that models such as
      <monospace>
       FashionCLIP
      </monospace>
      could provide a cheap way to generate probabilistic labels for weak supervision pipelines.
      <fig id="Fig7" position="float">
       <label>
        Figure 7
       </label>
       <caption xml:lang="en">
        <p>
         Item bounding-box detection. Localization maps can be easily extended to provide zero-shot bounding boxes for items of interest.Green bounding boxes show the predicted locations for fashion concepts “Backpack” (left) and “Straw hat” (right). Images above are taken from the publicly available Unsplash Lite Dataset 1.2.0: FashionCLIP was tested extensively on ModaNet - please reach out to authors for links to those images.
        </p>
       </caption>
       <graphic id="MO9" mime-subtype="PNG" specific-use="web" xlink:href="/ProjectMundo-Anon/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig7_HTML.png"/>
      </fig>
     </p>
    </sec>
    <sec id="Sec10">
     <title>
      Compositionality
     </title>
     <p id="Par35">
      Given the preliminary evidence that isolated concepts reliably map onto visual regions, our working hypothesis is that
      <monospace>
       FashionCLIP
      </monospace>
      should exhibit true
      <italic>
       inferential
      </italic>
      abilities by
      <italic>
       composing
      </italic>
      such concepts to generate new NPs.
     </p>
     <p id="Par36">
      We build on domain knowledge, previous literature
      <sup>
       <xref ref-type="bibr" rid="CR52">
        52
       </xref>
      </sup>
      and
      <italic>
       Farfetch
      </italic>
      ’s inventory to probe the model for knowledge of
      <italic>
       brands
      </italic>
      (e.g. “nike”),
      <italic>
       features
      </italic>
      (“high heels”), and
      <italic>
       drawings
      </italic>
      (“keyboard”), manually verifying the text-to-region mapping for each of these concepts via localization maps. Given that these single concepts are grounded in regions (Fig.
      <xref ref-type="fig" rid="Fig8">
       8
      </xref>
      ), we could leverage this knowledge to generate new images and NPs
      <italic>
       systematically
      </italic>
      . Crucially, we can assign a defined semantics to a new
      <italic>
       brand + object
      </italic>
      NP that describes an “improbable object” that has never been seen before (Fig.
      <xref ref-type="fig" rid="Fig9">
       9
      </xref>
      ). Improbable objects vary: they may portray odd combinations of concepts, such as a
      <italic>
       Nike long dress
      </italic>
      , a surreal item,
      <italic>
       sneakers with handles
      </italic>
      , or an unlikely extension of existing fashion items, such as the
      <italic>
       keyboard pochette
      </italic>
      (which generalizes the theme first found in J. Mugatu’s
      <italic>
       keyboard tie
      </italic>
      ). A new NP such as “nike dress” would require the visual region corresponding to the word
      <italic>
       dress
      </italic>
      to contain the visual region of the logo corresponding to the word
      <italic>
       nike
      </italic>
      .
      <fig id="Fig8" position="float">
       <label>
        Figure 8
       </label>
       <caption xml:lang="en">
        <p>
         Grounding and compositionality. Localization maps for a product retrieved with the query “ankle strap sandals with high heels”: left-to-right, the product, “ankle strap”, “sandals”, “high heels”).
        </p>
       </caption>
       <graphic id="MO10" mime-subtype="JPEG" specific-use="web" xlink:href="/ProjectMundo-Anon/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig8_HTML.jpg"/>
      </fig>
     </p>
     <p id="Par37">
      We supplement our analysis by re-purposing our classification and retrieval pipeline: in the classification task,
      <monospace>
       FashionCLIP
      </monospace>
      achieves an accuracy of 0.74 when asked to pick the improbable label out of a set of credible distractors. The following are examples of test cases:
      <list list-type="bullet">
       <list-item>
        <p id="Par38">
         <italic>
          target
         </italic>
         :
         <italic>
          NIKE DRESS
         </italic>
         (as seen in Fig.
         <xref ref-type="fig" rid="Fig9">
          9
         </xref>
         ),
         <italic>
          labels
         </italic>
         : Nike dress, an Armani dress, a shirt, the flag of Italy, a Gucci dress, a Nike t-shirt;
        </p>
       </list-item>
       <list-item>
        <p id="Par39">
         <italic>
          target
         </italic>
         :
         <italic>
          BLACK SHOES WITH RED HEEL
         </italic>
         ,
         <italic>
          labels
         </italic>
         : black shoes with red heel, black shoes, red shoes with red heel, red shoes with black heel, red shoes, fuchsia shoes, the flag of Italy, sneakers, black sneakers, a bag.
        </p>
       </list-item>
       <list-item>
        <p id="Par40">
         <italic>
          target
         </italic>
         :
         <italic>
          RED SHOES WITH BLACK HEEL
         </italic>
         (as seen in Fig.
         <xref ref-type="fig" rid="Fig9">
          9
         </xref>
         ),
         <italic>
          labels
         </italic>
         : black shoes with red heel, black shoes, red shoes with red heel, red shoes with black heel, red shoes, fuchsia shoes, the flag of Italy, sneakers, black sneakers, a bag.
        </p>
       </list-item>
      </list>
      For the retrieval task, we add the new images to
      <bold>
       TEST
      </bold>
      , and use the NPs as queries: out of 20k products, the model’s top choice is correct half the time (
      <italic>
       HITS@1
      </italic>
      <inline-formula id="IEq35">
       <alternatives>
        <math id="IEq35_Math" xmlns="http://www.w3.org/1998/Math/MathML">
         <mrow>
          <mo>
           =
          </mo>
          <mn>
           0.53
          </mn>
         </mrow>
        </math>
        <tex-math id="IEq35_TeX">
         \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ =0.53$$\end{document}
        </tex-math>
        <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq35.gif"/>
       </alternatives>
      </inline-formula>
      ), a percentage that quickly rises to 0.82 with
      <inline-formula id="IEq36">
       <alternatives>
        <math id="IEq36_Math" xmlns="http://www.w3.org/1998/Math/MathML">
         <mrow>
          <mi>
           k
          </mi>
          <mo>
           =
          </mo>
          <mn>
           5
          </mn>
         </mrow>
        </math>
        <tex-math id="IEq36_TeX">
         \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k=5$$\end{document}
        </tex-math>
        <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq36.gif"/>
       </alternatives>
      </inline-formula>
      (as a comparison,
      <monospace>
       CLIP
      </monospace>
      scored
      <italic>
       HITS@1
      </italic>
      <inline-formula id="IEq37">
       <alternatives>
        <math id="IEq37_Math" xmlns="http://www.w3.org/1998/Math/MathML">
         <mrow>
          <mo>
           =
          </mo>
          <mn>
           0.51
          </mn>
         </mrow>
        </math>
        <tex-math id="IEq37_TeX">
         \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$=0.51$$\end{document}
        </tex-math>
        <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq37.gif"/>
       </alternatives>
      </inline-formula>
      and
      <italic>
       HITS@5
      </italic>
      <inline-formula id="IEq38">
       <alternatives>
        <math id="IEq38_Math" xmlns="http://www.w3.org/1998/Math/MathML">
         <mrow>
          <mo>
           =
          </mo>
          <mn>
           0.73
          </mn>
         </mrow>
        </math>
        <tex-math id="IEq38_TeX">
         \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$=0.73$$\end{document}
        </tex-math>
        <inline-graphic mime-subtype="GIF" specific-use="web" xlink:href="41598_2022_23052_Article_IEq38.gif"/>
       </alternatives>
      </inline-formula>
      ).
      <fig id="Fig9" position="float">
       <label>
        Figure 9
       </label>
       <caption xml:lang="en">
        <p>
         Improbable products. By combining fashion features, brands, and items in new ways, we obtain visually realistic products with clear, zero-shot compositional semantics. From left to right: “Nike long dress”, “converse with handles”, “red shoes with black high heel”, “keyboard pochette”.
        </p>
       </caption>
       <graphic id="MO11" mime-subtype="PNG" specific-use="web" xlink:href="/ProjectMundo-Anon/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig9_HTML.png"/>
      </fig>
     </p>
     <p id="Par41">
      While a full-fledged investigation of compositional abilities is beyond the scope of
      <italic>
       this
      </italic>
      contribution,
      <monospace>
       FashionCLIP
      </monospace>
      inferences on improbable products suggest the presence of
      <italic>
       some
      </italic>
      degree of compositionality: important fashion concepts are “identifiable” in the latent space and can be singled out and re-combined into unseen concepts, exhibiting on a small scale the creative generalization we usually associate with symbolic systems
      <sup>
       <xref ref-type="bibr" rid="CR73">
        73
       </xref>
      </sup>
      . In addition, the ability to distinguish “red shoes with black heel” from “black shoes with red heel” implies knowledge beyond a bag-of-words semantics
      <sup>
       <xref ref-type="bibr" rid="CR74">
        74
       </xref>
      </sup>
      .
     </p>
     <p id="Par42">
      Recent research suggests that CLIP’s compositional capabilities are limited.
      <sup>
       <xref ref-type="bibr" rid="CR75">
        75
       </xref>
      </sup>
      . As shown by our results, restricted domains allow for direct manipulation, without the risk of confounding; indeed, restricted domains may be easier to explore but further investigation is needed to confirm compositional capabilities. Furthermore, as suggested by the use of the MASKClip objective introduced in the
      <italic>
       ARMANI
      </italic>
      model
      <sup>
       <xref ref-type="bibr" rid="CR33">
        33
       </xref>
      </sup>
      , adding explicit visual segmentation may induce better discrimination for certain fashion concepts. While more costly losses are an interesting area at the intersection of grounding and compositionality, given both the narrow generative focus and the magnitude of the improvements in the original paper
      <sup>
       <xref ref-type="bibr" rid="CR33">
        33
       </xref>
      </sup>
      , their conclusions cannot be readily applied to
      <monospace>
       FashionCLIP
      </monospace>
      . We look forward to performing future research combining insights from generative and discriminative use cases.
     </p>
    </sec>
   </sec>
  </sec>
  <sec id="Sec11" sec-type="discussion">
   <title>
    Discussion
   </title>
   <p id="Par43">
    <monospace>
     FashionCLIP
    </monospace>
    is a domain-adaptation of
    <monospace>
     CLIP
    </monospace>
    , motivated by central use cases in fashion
    <sup>
     <xref ref-type="bibr" rid="CR71">
      71
     </xref>
    </sup>
    : differently from
    <italic>
     task-specific supervised
    </italic>
    methods,
    <monospace>
     FashionCLIP
    </monospace>
    does not need a specialized architecture, labeling, and tuning. We extensively verified the flexibility afforded by language supervision, and investigated
    <monospace>
     FashionCLIP
    </monospace>
    ’s semantic capabilities on new tasks. Our focus on a specific industry allows not just practical gains but also opens up theoretical possibilities by constraining the domain, which is still large, but also easy to manipulate. By providing quantitative and qualitative evidence that contrastive learning, coupled with a large and diverse dataset, can indeed produce general multi-model industry concepts, we connect theoretical virtues with significant practical gains, and open new possibilities for scaling the horizontal deployment of machine learning systems in an effective way.
   </p>
   <p id="Par44">
    As a truly general system,
    <monospace>
     FashionCLIP
    </monospace>
    concepts could be used for many more tasks: for example, multi-modal representations can be features in downstream systems, or directly used for zero-shot recommendations in item-to-item scenarios
    <sup>
     <xref ref-type="bibr" rid="CR76">
      76
     </xref>
    </sup>
    ; classification over arbitrary labels could be used as a fast and scalable labeling mechanism, supporting probabilistic labeling
    <sup>
     <xref ref-type="bibr" rid="CR77">
      77
     </xref>
    </sup>
    or data generation for multi-modal IR models
    <sup>
     <xref ref-type="bibr" rid="CR78">
      78
     </xref>
    </sup>
    . While leaving this (and many other themes) to future iterations, we do believe
    <italic>
     this
    </italic>
    work—with its artifacts and methodology—to be a first, rounded assessment of the great potential of general, transferable, multi-modal concepts for digital commerce.
    <fig id="Fig10" position="float">
     <label>
      Figure 10
     </label>
     <caption xml:lang="en">
      <p>
       Typographical attack.
       <monospace>
        FashionCLIP
       </monospace>
       correctly identifies the object to the left as an “apple”, but misclassifies the one to the right as “nike air”, as the text acts as a confounder
       <sup>
        <xref ref-type="bibr" rid="CR79">
         79
        </xref>
       </sup>
       .
      </p>
     </caption>
     <graphic id="MO12" mime-subtype="JPEG" specific-use="web" xlink:href="/ProjectMundo-Anon/MediaObjects/10X1038_s41598-022-23052-9/41598_2022_23052_Fig10_HTML.jpg"/>
    </fig>
   </p>
   <p id="Par45">
    The authors are aware of the risks of multi-modal
    <italic>
     CLIP
    </italic>
    -like models in production associated with their limited robustness, as well as general issues with biases in large language models pre-trained at scale. In particular, we acknowledge that the risk of adversarial attacks on multi-modal models is an area of active research
    <sup>
     <xref ref-type="bibr" rid="CR80">
      80
     </xref>
     ,
     <xref ref-type="bibr" rid="CR81">
      81
     </xref>
    </sup>
    . To the limits of our knowledge, we have no reason to believe that
    <monospace>
     FashionCLIP
    </monospace>
    introduces any
    <italic>
     additional
    </italic>
    risk when compared to the original CLIP. As with the original model, it should be noted that
    <monospace>
     FashionCLIP
    </monospace>
    appears to be susceptible to “typographical attacks” (Fig.
    <xref ref-type="fig" rid="Fig10">
     10
    </xref>
    ). No datasets used for training or testing contain PII and/or other sensitive user data.
   </p>
  </sec>
  <sec id="Sec12" sec-type="methods">
   <title>
    Methods
   </title>
   <sec id="Sec13">
    <title>
     Training dataset
    </title>
    <p id="Par46">
     <italic>
      Farfetch
     </italic>
     made available for the first time an English dataset comprising over 800 k fashion products, with more than 3k brands across dozens of object types. Compared to other large fashion datasets, our dataset is significantly more complete than DeepFashion
     <sup>
      <xref ref-type="bibr" rid="CR52">
       52
      </xref>
     </sup>
     , which lacks detailed text descriptions, and even larger than CM-Fashion
     <sup>
      <xref ref-type="bibr" rid="CR33">
       33
      </xref>
     </sup>
     , which has been collected without any direct involvement by
     <italic>
      Farfetch
     </italic>
     . Items are organized in hierarchical trees, producing a three-layer taxonomy: for example,
     <italic>
      trees
     </italic>
     could be something like
     <italic>
      Clothing&gt; Dresses&gt; Day Dresses
     </italic>
     or
     <italic>
      Clothing&gt; Coats&gt; Parkas
     </italic>
     , for a total of 800+ trees. As input for the image encoder, we use the standard product image, which is a picture of the item over a white background, with no humans (images follow a specific set of rules regarding the placement of the item, lights of the photo, etc., designed to highlight the item’s features); as for the text,
     <italic>
      Farfetch
     </italic>
     has two types of text,
     <italic>
      highlight
     </italic>
     (e.g., “stripes”, “long sleeves”, “Armani”) and a
     <italic>
      short description
     </italic>
     (“80s styled t-shirt”). See Fig.
     <xref ref-type="fig" rid="Fig3">
      3
     </xref>
     for an example.
    </p>
    <p id="Par47">
     We create a training, validation, and test set from the catalog by random sampling products. Our final training and validation sets comprise 700 k and 50 k products respectively from 188 categories.
    </p>
   </sec>
   <sec id="Sec14">
    <title>
     Training pipeline
    </title>
    <p id="Par48">
     We apply fine-tuning starting from the pre-trained
     <monospace>
      CLIP
     </monospace>
     with the following parameters: we use Adam Optimizer with betas in (0.9, 0.98), epsilon of 1e−6 and weight decay equal to 0.2 and three different learning rates [1e−4, 1e−5, 1e−6]. We train the models for 4 epochs, evaluate every 500 steps and select the model with the lowest validation loss for each configuration (Table
     <xref ref-type="table" rid="Tab5">
      5
     </xref>
     , model selected in bold). In our preliminary tests, the model with the lowest validation loss overall did not generalize the best in the zero-shot setting. This poses an interesting question, left for future work, of how to fine-tune these large pre-trained models without losing in generalization. The pipeline has been implemented with Metaflow
     <sup>
      <xref ref-type="bibr" rid="CR82">
       82
      </xref>
     </sup>
     , with training executed remotely on cloud GPUs; experiment tracking was provided by Comet
     <sup>
      <xref ref-type="bibr" rid="CR83">
       83
      </xref>
     </sup>
     .
    </p>
   </sec>
   <sec id="Sec15">
    <title>
     Testing datasets
    </title>
    <p id="Par49">
     We prepare the following datasets for testing purposes and to further gauge the potential impact of the model in production at scale.
     <bold>
      TEST
     </bold>
     is the test set from
     <italic>
      Farfetch
     </italic>
     containing 20k products;
     <bold>
      HOUT-C
     </bold>
     is the dataset containing a category which we excluded from training (
     <italic>
      Performance Tops
     </italic>
     ), for a total of 1.5k items;
     <bold>
      HOUT-B
     </bold>
     is the dataset containing two brands which were excluded from training, for a total of 1.7k items;
     <bold>
      STLE
     </bold>
     is a merchandising dataset from
     <italic>
      Farfetch
     </italic>
     , completely independent from the catalog, that classifies 7749 items across 6 styles for gender women and 4 styles for gender men; example of styles are
     <italic>
      Classic
     </italic>
     and
     <italic>
      Streetwear
     </italic>
     and each item may belong to more than one style;
     <bold>
      KAGL
     </bold>
     is a subset of
     <sup>
      <xref ref-type="bibr" rid="CR50">
       50
      </xref>
     </sup>
     , where each product has a white background image, a caption, and a category, for a total of 9990 items over 62 categories;
     <bold>
      F-MNIST
     </bold>
     <sup>
      <xref ref-type="bibr" rid="CR51">
       51
      </xref>
     </sup>
     contains 10, 000 gray-scale images from 10 product classes, with pixel intensity inverted to obtain images with white background (note that these images have a size of 24 × 24 thus showing much less details than the images on which the models have been trained on).
     <bold>
      DEEP
     </bold>
     <sup>
      <xref ref-type="bibr" rid="CR52">
       52
      </xref>
     </sup>
     contains 4000 product images that are non-standardized (i.e contains humans) from 50 categories.
    </p>
   </sec>
   <sec id="Sec16">
    <title>
     Training
     <monospace>
      FashionCLIP
     </monospace>
    </title>
    <p id="Par50">
     We re-purpose the
     <monospace>
      CLIP
     </monospace>
     main architecture
     <sup>
      <xref ref-type="bibr" rid="CR17">
       17
      </xref>
     </sup>
     , which we describe briefly in the
     <italic>
      Introduction
     </italic>
     for the sake of completeness. In the end, we obtain a multi-modal space where images and texts are jointly projected and learned: if training has been successful, we expect that, for example, the textual embedding for the string “red long dress” is actually similar (as measured by the dot product) to the image embeddings of red dresses. Table
     <xref ref-type="table" rid="Tab5">
      5
     </xref>
     shows training time, performance, and costs.
     <table-wrap id="Tab5">
      <label>
       Table 5
      </label>
      <caption xml:lang="en">
       <p>
        Comparing training time, performance, costs, and carbon emission on variants of the
        <monospace>
         FashionCLIP
        </monospace>
        architecture on the
        <italic>
         Farfetch
        </italic>
        catalog.
       </p>
      </caption>
      <table frame="hsides" rules="groups">
       <thead>
        <tr>
         <th align="left">
          <p>
           LR
          </p>
         </th>
         <th align="left">
          <p>
           Loss
          </p>
         </th>
         <th align="left">
          <p>
           Time(m)
          </p>
         </th>
         <th align="left">
          <p>
           USD
          </p>
         </th>
         <th align="left">
          <p>
           kgCO
           <sub>
            2
           </sub>
           eq
          </p>
         </th>
        </tr>
       </thead>
       <tbody>
        <tr>
         <td align="left">
          <p>
           1e−4
          </p>
         </td>
         <td align="left">
          <p>
           16.0
          </p>
         </td>
         <td align="left">
          <p>
           618
          </p>
         </td>
         <td align="left">
          <p>
           31$
          </p>
         </td>
         <td align="left">
          <p>
           0.77
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           1e−5
          </p>
         </td>
         <td align="left">
          <p>
           1.73
          </p>
         </td>
         <td align="left">
          <p>
           617
          </p>
         </td>
         <td align="left">
          <p>
           31$
          </p>
         </td>
         <td align="left">
          <p>
           0.77
          </p>
         </td>
        </tr>
        <tr>
         <td align="left">
          <p>
           <bold>
            1e−6
           </bold>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            2.83
           </bold>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            621
           </bold>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            31$
           </bold>
          </p>
         </td>
         <td align="left">
          <p>
           <bold>
            0.78
           </bold>
          </p>
         </td>
        </tr>
       </tbody>
      </table>
      <table-wrap-foot>
       <p>
        Cost is calculated with the AWS pricing for a
        <italic>
         p3.2xlarge
        </italic>
        ; estimations were conducted using the Machine Learning Impact calculator
        <sup>
         <xref ref-type="bibr" rid="CR84">
          84
         </xref>
        </sup>
        . Model used for testing in bold.
       </p>
      </table-wrap-foot>
     </table-wrap>
    </p>
   </sec>
  </sec>
 </body>
 <back>
  <sec sec-type="author-contribution">
   <title>
    Author contributions
   </title>
   <p>
    FashionCLIP was started by JT and FB, PC and GA led implementation and experiments; DG and ARM prepared the dataset, performed EDA, and provided domain knowledge; ST and CG helped with fine-tuning, model evaluation, and research background. Everybody contributed to the final draft. JT and FB acted as senior PIs for the project.
   </p>
  </sec>
  <sec sec-type="data-availability">
   <title>
    Data availability
   </title>
   <p>
    The
    <bold>
     KAGL
    </bold>
    ,
    <bold>
     F-MNIST
    </bold>
    , and
    <bold>
     DEEP
    </bold>
    datasets are publicly available. The Farfetch dataset is scheduled to be released in the near future. As part of the ongoing mission to help the retail space leverage the latest A.I. techniques and to promote multidisciplinary research in data science across industries, Farfetch is working to finalize the release of the dataset used in this study under a research-friendly license. Please check
    <ext-link ext-link-type="uri" xlink:href="https://github.com/Farfetch">
     https://github.com/Farfetch
    </ext-link>
    for updates on the data release, and reach out to the authors for preliminary inquiries.
   </p>
  </sec>
  <sec sec-type="ethics-statement">
   <sec id="FPar1" sec-type="COI-statement">
    <title>
     Competing interests
    </title>
    <p id="Par54">
     GA is a member of the Bocconi Institute of Data Science and Analytics (BIDSA) and the Data and Marketing Insights (DMI) unit. FB was a BIDSA and DMI member during the project. JT and CG were at Coveo Labs for the initial phase of the project. The authors declare no competing interests.
    </p>
   </sec>
  </sec>
  <ref-list id="Bib1">
   <title>
    References
   </title>
   <ref-list>
    <ref id="CR1">
     <label>
      1.
     </label>
     <mixed-citation publication-type="other">
      Cramer-Flood, E. Global Ecommerce 2020. Ecommerce Decelerates amid Global Retail Contraction but Remains a Bright Spot. (2020).
     </mixed-citation>
    </ref>
    <ref id="CR2">
     <label>
      2.
     </label>
     <mixed-citation publication-type="other">
      McKinsey. The state of Fashion 2019. (2019).
     </mixed-citation>
    </ref>
    <ref id="CR3">
     <label>
      3.
     </label>
     <mixed-citation publication-type="other">
      de Souza Pereira Moreira, G., Jannach, D. &amp; da Cunha, A. M. On the importance of news content representation in hybrid neural session-based recommender systems. In
      <italic>
       INRA@RecSys
      </italic>
      (2019).
     </mixed-citation>
    </ref>
    <ref id="CR4">
     <label>
      4.
     </label>
     <mixed-citation publication-type="other">
      Guo, M.
      <italic>
       et al
      </italic>
      . Deep learning-based online alternative product recommendations at scale. In
      <italic>
       Proceedings of The 3rd Workshop on e-Commerce and NLP
      </italic>
      19–23 (Association for Computational Linguistics, Seattle, WA, USA, 2020).
      <ext-link ext-link-type="doi" xlink:href="10.18653/v1/2020.ecnlp-1.3">
       https://doi.org/10.18653/v1/2020.ecnlp-1.3
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR5">
     <label>
      5.
     </label>
     <mixed-citation publication-type="book">
      <person-group person-group-type="author">
       <name>
        <surname>
         Goncalves
        </surname>
        <given-names>
         D
        </given-names>
       </name>
       <etal/>
      </person-group>
      <person-group person-group-type="editor">
       <name>
        <surname>
         Dokoohaki
        </surname>
        <given-names>
         N
        </given-names>
       </name>
       <name>
        <surname>
         Jaradat
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <name>
        <surname>
         Corona Pampín
        </surname>
        <given-names>
         HJ
        </given-names>
       </name>
       <name>
        <surname>
         Shirvany
        </surname>
        <given-names>
         R
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       The importance of brand affinity in luxury fashion recommendations
      </article-title>
      <source>
       Recommender Systems in Fashion and Retail
      </source>
      <year>
       2021
      </year>
      <publisher-loc>
       Cham
      </publisher-loc>
      <publisher-name>
       Springer International Publishing
      </publisher-name>
      <fpage>
       3
      </fpage>
      <lpage>
       19
      </lpage>
      <pub-id pub-id-type="doi">
       10.1007/978-3-030-66103-8_1
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR6">
     <label>
      6.
     </label>
     <mixed-citation publication-type="other">
      Ai, Q. &amp; Narayanan, R. L. Model-agnostic vs. model-intrinsic interpretability for explainable product search. In
      <italic>
       Proceedings of the 30th ACM International Conference on Information and Knowledge Management, CIKM ’21
      </italic>
      5–15,
      <ext-link ext-link-type="doi" xlink:href="10.1145/3459637.3482276">
       https://doi.org/10.1145/3459637.3482276
      </ext-link>
      (Association for Computing Machinery, New York, NY, USA, 2021).
     </mixed-citation>
    </ref>
    <ref id="CR7">
     <label>
      7.
     </label>
     <mixed-citation publication-type="other">
      Chen, L., Chou, H., Xia, Y. &amp; Miyake, H. Multimodal item categorization fully based on transformer. In
      <italic>
       Proceedings of The 4th Workshop on e-Commerce and NLP
      </italic>
      111–115 (Association for Computational Linguistics, Online, 2021).
      <ext-link ext-link-type="doi" xlink:href="10.18653/v1/2021.ecnlp-1.13">
       https://doi.org/10.18653/v1/2021.ecnlp-1.13
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR8">
     <label>
      8.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Tsagkias
        </surname>
        <given-names>
         M
        </given-names>
       </name>
       <name>
        <surname>
         King
        </surname>
        <given-names>
         TH
        </given-names>
       </name>
       <name>
        <surname>
         Kallumadi
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <name>
        <surname>
         Murdock
        </surname>
        <given-names>
         V
        </given-names>
       </name>
       <name>
        <surname>
         de Rijke
        </surname>
        <given-names>
         M
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Challenges and research opportunities in ecommerce search and recommendations
      </article-title>
      <source>
       SIGIR Forum
      </source>
      <year>
       2021
      </year>
      <volume>
       54
      </volume>
      <fpage>
       1
      </fpage>
      <lpage>
       23
      </lpage>
      <pub-id pub-id-type="doi">
       10.1145/3451964.3451966
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR9">
     <label>
      9.
     </label>
     <mixed-citation publication-type="other">
      Bianchi, F., Tagliabue, J. &amp; Yu, B. Query2Prod2Vec: Grounded word embeddings for eCommerce. In
      <italic>
       Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers
      </italic>
      154–162 (Association for Computational Linguistics, Online, 2021).
      <ext-link ext-link-type="doi" xlink:href="10.18653/v1/2021.naacl-industry.20">
       https://doi.org/10.18653/v1/2021.naacl-industry.20
      </ext-link>
     </mixed-citation>
    </ref>
    <ref id="CR10">
     <label>
      10.
     </label>
     <mixed-citation publication-type="other">
      Tagliabue, J. &amp; Yu, B. Shopping in the multiverse: A counterfactual approach to in-session attribution. In
      <italic>
       Proceedings of the SIGIR 2020 Workshop on eCommerce
      </italic>
      (ECOM 20) (2020).
     </mixed-citation>
    </ref>
    <ref id="CR11">
     <label>
      11.
     </label>
     <mixed-citation publication-type="other">
      Sculley, D.
      <italic>
       et al
      </italic>
      . Hidden technical debt in machine learning systems. In
      <italic>
       NIPS
      </italic>
      (2015).
     </mixed-citation>
    </ref>
    <ref id="CR12">
     <label>
      12.
     </label>
     <mixed-citation publication-type="other">
      Tagliabue, J. You do not need a bigger boat: Recommendations at reasonable scale in a (mostly) serverless and open stack. In
      <italic>
       Fifteenth ACM Conference on Recommender Systems,
      </italic>
      RecSys ’21  598–600  (Association for Computing Machinery, New York, NY, USA, 2021).
      <ext-link ext-link-type="doi" xlink:href="10.1145/3460231.3474604">
       https://doi.org/10.1145/3460231.3474604
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR13">
     <label>
      13.
     </label>
     <mixed-citation publication-type="other">
      Techcrunch. Algolia finds $110M from Accel and Salesforce (2019).
     </mixed-citation>
    </ref>
    <ref id="CR14">
     <label>
      14.
     </label>
     <mixed-citation publication-type="other">
      Techcrunch. Bloomreach raises $150M on $900M valuation and acquires Exponea (2021).
     </mixed-citation>
    </ref>
    <ref id="CR15">
     <label>
      15.
     </label>
     <mixed-citation publication-type="other">
      Techcrunch. Lucidworks raises $100M to expand in AI finds (2019).
     </mixed-citation>
    </ref>
    <ref id="CR16">
     <label>
      16.
     </label>
     <mixed-citation publication-type="other">
      Marotta, S. Canada’s Latest Tech Public Debut Swings Amid Soft IPOs (2021).
      <ext-link ext-link-type="uri" xlink:href="https://www.bloomberg.com/news/articles/2021-11-25/canada-slatest-tech-public-debut-swings-amid-slew-of-soft-ipos">
       https://www.bloomberg.com/news/articles/2021-11-25/canada-slatest-tech-public-debut-swings-amid-slew-of-soft-ipos
      </ext-link>
      . Accessed 04-Nov-2022.
     </mixed-citation>
    </ref>
    <ref id="CR17">
     <label>
      17.
     </label>
     <mixed-citation publication-type="other">
      Radford, A.
      <italic>
       et al
      </italic>
      . Learning transferable visual models from natural language supervision. In
      <italic>
       ICML
      </italic>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR18">
     <label>
      18.
     </label>
     <mixed-citation publication-type="other">
      Shankar, S. Thoughts on ML
Engineering After a Year of my PhD—shreya-shankar.com. (2022).
      <ext-link ext-link-type="uri" xlink:href="https://www.shreyashankar.com/phd-year-one/">
       https://www.shreyashankar.com/phd-year-one/
      </ext-link>
      . Accessed 04-Nov-2022.
     </mixed-citation>
    </ref>
    <ref id="CR19">
     <label>
      19.
     </label>
     <mixed-citation publication-type="other">
      Bommasani, R. et al. On the opportunities and risks of foundation models. CoRR
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2108.07258">
       arXiv:2108.07258
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR20">
     <label>
      20.
     </label>
     <mixed-citation publication-type="other">
      Chen, T., Kornblith, S., Norouzi, M. &amp; Hinton, G. A simple framework for contrastive learning of visual representations. In
      <italic>
       International Conference on Machine Learning
      </italic>
      1597–1607 (PMLR, 2020).
     </mixed-citation>
    </ref>
    <ref id="CR21">
     <label>
      21.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Grill
        </surname>
        <given-names>
         J-B
        </given-names>
       </name>
       <etal/>
      </person-group>
      <article-title xml:lang="en">
       Bootstrap your own latent-a new approach to self-supervised learning
      </article-title>
      <source>
       Adv. Neural Inf. Process. Syst.
      </source>
      <year>
       2020
      </year>
      <volume>
       33
      </volume>
      <fpage>
       21271
      </fpage>
      <lpage>
       21284
      </lpage>
     </mixed-citation>
    </ref>
    <ref id="CR22">
     <label>
      22.
     </label>
     <mixed-citation publication-type="other">
      Iter, D., Guu, K., Lansing, L. &amp; Jurafsky, D. Pretraining with contrastive sentence objectives improves discourse performance of language models. In
      <italic>
       Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
      </italic>
      4859–4870 (Association for Computational Linguistics, Online, 2020).
      <ext-link ext-link-type="doi" xlink:href="10.18653/v1/2020.acl-main.439">
       https://doi.org/10.18653/v1/2020.acl-main.439
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR23">
     <label>
      23.
     </label>
     <mixed-citation publication-type="other">
      Su, Y.
      <italic>
       et al
      </italic>
      . A contrastive framework for neural text generation. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2202.06417">
       arXiv:2202.06417
      </ext-link>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR24">
     <label>
      24.
     </label>
     <mixed-citation publication-type="other">
      Jia, C.
      <italic>
       et al
      </italic>
      . Scaling up visual and vision-language representation learning with noisy text supervision. In
      <italic>
       International Conference on Machine Learning
      </italic>
      4904–4916 (PMLR, 2021).
     </mixed-citation>
    </ref>
    <ref id="CR25">
     <label>
      25.
     </label>
     <mixed-citation publication-type="other">
      Mu, N., Kirillov, A., Wagner, D. &amp; Xie, S. Slip: Self-supervision meets language-image pre-training. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2112.12750">
       arXiv:2112.12750
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR26">
     <label>
      26.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Schneider
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <name>
        <surname>
         Baevski
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <name>
        <surname>
         Collobert
        </surname>
        <given-names>
         R
        </given-names>
       </name>
       <name>
        <surname>
         Auli
        </surname>
        <given-names>
         M
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       wav2vec: Unsupervised pre-training for speech recognition
      </article-title>
      <source>
       Proc. Interspeech
      </source>
      <year>
       2019
      </year>
      <volume>
       2019
      </volume>
      <fpage>
       3465
      </fpage>
      <lpage>
       3469
      </lpage>
     </mixed-citation>
    </ref>
    <ref id="CR27">
     <label>
      27.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Baevski
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <name>
        <surname>
         Zhou
        </surname>
        <given-names>
         Y
        </given-names>
       </name>
       <name>
        <surname>
         Mohamed
        </surname>
        <given-names>
         A
        </given-names>
       </name>
       <name>
        <surname>
         Auli
        </surname>
        <given-names>
         M
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       wav2vec 2.0: A framework for self-supervised learning of speech representations
      </article-title>
      <source>
       Adv. Neural Inf. Process. Syst.
      </source>
      <year>
       2020
      </year>
      <volume>
       33
      </volume>
      <fpage>
       12449
      </fpage>
      <lpage>
       12460
      </lpage>
     </mixed-citation>
    </ref>
    <ref id="CR28">
     <label>
      28.
     </label>
     <mixed-citation publication-type="other">
      Nagrani, A.
      <italic>
       et al
      </italic>
      . Learning audio-video modalities from image captions. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2204.00679">
       arXiv:2204.00679
      </ext-link>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR29">
     <label>
      29.
     </label>
     <mixed-citation publication-type="other">
      Shvetsova, N.
      <italic>
       et al
      </italic>
      . Everything at once–multi-modal fusion transformer for video retrieval. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2112.04446">
       arXiv:2112.04446
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR30">
     <label>
      30.
     </label>
     <mixed-citation publication-type="other">
      Minderer, M.
      <italic>
       et al
      </italic>
      . Simple open-vocabulary object detection with vision transformers. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2205.06230">
       arXiv:2205.06230
      </ext-link>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR31">
     <label>
      31.
     </label>
     <mixed-citation publication-type="other">
      Liu, H.
      <italic>
       et al
      </italic>
      . Cma-clip: Cross-modality attention clip for image-text classification. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2112.03562">
       arXiv:2112.03562
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR32">
     <label>
      32.
     </label>
     <mixed-citation publication-type="other">
      Sevegnani, K.
      <italic>
       et al
      </italic>
      . Contrastive learning for interactive recommendation in fashion. CoRR
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2207.12033">
       arXiv:2207.12033
      </ext-link>
      ,
      <ext-link ext-link-type="doi" xlink:href="10.48550/arXiv.2207.12033">
       https://doi.org/10.48550/arXiv.2207.12033
      </ext-link>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR33">
     <label>
      33.
     </label>
     <mixed-citation publication-type="other">
      Zhang, X.
      <italic>
       et al
      </italic>
      . Armani: Part-level garment-text alignment for unified cross-modal fashion design. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2208.05621">
       arXiv:2208.05621
      </ext-link>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR34">
     <label>
      34.
     </label>
     <mixed-citation publication-type="other">
      Ramesh, A., Dhariwal, P., Nichol, A., Chu, C. &amp; Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2204.06125">
       arXiv:2204.06125
      </ext-link>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR35">
     <label>
      35.
     </label>
     <mixed-citation publication-type="other">
      Kong, C., Jeon, D., Kwon, O. &amp; Kwak, N. Leveraging off-the-shelf diffusion model for multi-attribute fashion image manipulation. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2210.05872">
       arXiv:2210.05872
      </ext-link>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR36">
     <label>
      36.
     </label>
     <mixed-citation publication-type="other">
      Gupta, V., Karnick, H., Bansal, A. &amp; Jhala, P. Product classification in e-commerce using distributional semantics. In
      <italic>
       COLING
      </italic>
      (2016).
     </mixed-citation>
    </ref>
    <ref id="CR37">
     <label>
      37.
     </label>
     <mixed-citation publication-type="other">
      Fu, J.
      <italic>
       et al
      </italic>
      . Cma-clip: Cross-modality attention clip for text-image classification. In
      <italic>
       IEEE ICIP 2022
      </italic>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR38">
     <label>
      38.
     </label>
     <mixed-citation publication-type="other">
      Commerce, B. How Ecommerce Site Search Can Create a Competitive Advantage. (2021).
      <ext-link ext-link-type="uri" xlink:href="https://www.bigcommerce.com/articles/ecommerce/site-search/#the-effectiveness-of-ecommerce-site-search-">
       https://www.bigcommerce.com/articles/ecommerce/site-search/#the-effectiveness-of-ecommerce-site-search-
      </ext-link>
      . Accessed 04-Nov-2022.
     </mixed-citation>
    </ref>
    <ref id="CR39">
     <label>
      39.
     </label>
     <mixed-citation publication-type="other">
      Alaimo, D. 87% of shoppers now begin product searches online. (2018).
     </mixed-citation>
    </ref>
    <ref id="CR40">
     <label>
      40.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Robertson
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <name>
        <surname>
         Zaragoza
        </surname>
        <given-names>
         H
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       The probabilistic relevance framework: Bm25 and beyond
      </article-title>
      <source>
       Found. Trends Inf. Retr.
      </source>
      <year>
       2009
      </year>
      <volume>
       3
      </volume>
      <fpage>
       333
      </fpage>
      <lpage>
       389
      </lpage>
      <pub-id pub-id-type="doi">
       10.1561/1500000019
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR41">
     <label>
      41.
     </label>
     <mixed-citation publication-type="other">
      Gillick, D., Presta, A. &amp; Tomar, G. S. End-to-end retrieval in continuous space. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1811.08008">
       arXiv:1811.08008
      </ext-link>
      (2018).
     </mixed-citation>
    </ref>
    <ref id="CR42">
     <label>
      42.
     </label>
     <mixed-citation publication-type="other">
      Izacard, G.
      <italic>
       et al.
      </italic>
      Towards unsupervised dense information retrieval with contrastive learning. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2112.09118">
       arXiv:2112.09118
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR43">
     <label>
      43.
     </label>
     <mixed-citation publication-type="other">
      Hu, Y., Da, Q., Zeng, A., Yu, Y. &amp; Xu, Y. Reinforcement learning to rank in e-commerce search engine: Formalization, analysis, and application. In
      <italic>
       Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’18
      </italic>
      , 368–377  (Association for Computing Machinery, New York, NY, USA, 2018).
      <ext-link ext-link-type="doi" xlink:href="10.1145/3219819.3219846">
       https://doi.org/10.1145/3219819.3219846
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR44">
     <label>
      44.
     </label>
     <mixed-citation publication-type="other">
      Chen, L. &amp; Miyake, H. Label-guided learning for item categorization in e-commerce. In
      <italic>
       Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers
      </italic>
      296–303 (Association for Computational Linguistics, Online, 2021).
      <ext-link ext-link-type="doi" xlink:href="10.18653/v1/2021.naacl-industry.37">
       https://doi.org/10.18653/v1/2021.naacl-industry.37
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR45">
     <label>
      45.
     </label>
     <mixed-citation publication-type="other">
      Costin, A. M., Eastman, C. &amp; Issa, R. R. A. The Need for Taxonomies in the Ontological Approach for Interoperability of Heterogeneous Information Models 9–17 (2017).
      <ext-link ext-link-type="uri" xlink:href="https://ascelibrary.org/doi/pdf/10.1061/9780784480830.002">
       https://ascelibrary.org/doi/pdf/10.1061/9780784480830.002
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR46">
     <label>
      46.
     </label>
     <mixed-citation publication-type="other">
      McDowell, M. Taxonomy is the new fashion-tech essential. (2020).
      <ext-link ext-link-type="uri" xlink:href="https://www.voguebusiness.com/technology/taxonomy-is-the-new-fashion-tech-essential-theyes">
       https://www.voguebusiness.com/technology/taxonomy-is-the-new-fashion-tech-essential-theyes
      </ext-link>
      .
Accessed 04-Nov-2022.
     </mixed-citation>
    </ref>
    <ref id="CR47">
     <label>
      47.
     </label>
     <mixed-citation publication-type="other">
      Feizi, S., Singla, S. Salient imagenet: how to discover spurious features in deep learning? In
      <italic>
       International Conference on Learning Representations
      </italic>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR48">
     <label>
      48.
     </label>
     <mixed-citation publication-type="other">
      Chia, P. J. Fashion CLIP.
      <ext-link ext-link-type="uri" xlink:href="https://github.com/patrickjohncyh/fashion-clip/">
       https://github.com/patrickjohncyh/fashion-clip/
      </ext-link>
      (2022). [Online; accessed 15-September-2022].
     </mixed-citation>
    </ref>
    <ref id="CR49">
     <label>
      49.
     </label>
     <mixed-citation publication-type="other">
      Chia, P. J. GradREC.
      <ext-link ext-link-type="uri" xlink:href="https://github.com/patrickjohncyh/gradient-recs/">
       https://github.com/patrickjohncyh/gradient-recs/
      </ext-link>
      (2022). [Online; accessed 15-September-2022].
     </mixed-citation>
    </ref>
    <ref id="CR50">
     <label>
      50.
     </label>
     <mixed-citation publication-type="other">
      Aggarwal, P. Fashion Product Images Dataset. (2020).
     </mixed-citation>
    </ref>
    <ref id="CR51">
     <label>
      51.
     </label>
     <mixed-citation publication-type="other">
      Xiao, H., Rasul, K. &amp; Vollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/cs.LG1708.07747">
       arXiv:cs.LG/1708.07747
      </ext-link>
      (2017).
     </mixed-citation>
    </ref>
    <ref id="CR52">
     <label>
      52.
     </label>
     <mixed-citation publication-type="other">
      Liu, Z., Luo, P., Qiu, S., Wang, X. &amp; Tang, X. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In
      <italic>
       Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
      </italic>
      (2016).
     </mixed-citation>
    </ref>
    <ref id="CR53">
     <label>
      53.
     </label>
     <mixed-citation publication-type="other">
      Cremonesi, P., Koren, Y. &amp; Turrin, R. Performance of recommender algorithms on top-n recommendation tasks. In
      <italic>
       Proceedings of the Fourth ACM Conference on Recommender Systems, RecSys ’10
      </italic>
      , 39–46  (Association for Computing Machinery, New York, NY, USA, 2010).
      <ext-link ext-link-type="doi" xlink:href="10.1145/1864708.1864721">
       https://doi.org/10.1145/1864708.1864721
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR54">
     <label>
      54.
     </label>
     <mixed-citation publication-type="book">
      <person-group person-group-type="author">
       <name>
        <surname>
         Craswell
        </surname>
        <given-names>
         N
        </given-names>
       </name>
      </person-group>
      <source>
       Mean Reciprocal Rank
      </source>
      <year>
       2009
      </year>
      <publisher-loc>
       US, Boston, MA
      </publisher-loc>
      <publisher-name>
       Springer
      </publisher-name>
      <fpage>
       1703
      </fpage>
      <lpage>
       1703
      </lpage>
     </mixed-citation>
    </ref>
    <ref id="CR55">
     <label>
      55.
     </label>
     <mixed-citation publication-type="other">
      Manning, C. D., Raghavan, P. &amp; Schütze, H.
      <italic>
       An Introduction to Information Retrieval
      </italic>
      Online. (Cambridge University Press, Cambridge, UK, 2009).
     </mixed-citation>
    </ref>
    <ref id="CR56">
     <label>
      56.
     </label>
     <mixed-citation publication-type="other">
      Krizhevsky, A.
      <italic>
       Learning Multiple Layers of Features from Tiny Images
      </italic>
      Tech Rep, (2009).
     </mixed-citation>
    </ref>
    <ref id="CR57">
     <label>
      57.
     </label>
     <mixed-citation publication-type="other">
      Deng, J.
      <italic>
       et al.
      </italic>
      Imagenet: A large-scale hierarchical image database. In
      <italic>
       2009 IEEE Conference on Computer Vision and Pattern Recognition
      </italic>
      248–255 (IEEE, 2009).
     </mixed-citation>
    </ref>
    <ref id="CR58">
     <label>
      58.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Rousseeuw
        </surname>
        <given-names>
         PJ
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Silhouettes: A graphical aid to the interpretation and validation of cluster analysis
      </article-title>
      <source>
       J. Comput. Appl. Math.
      </source>
      <year>
       1987
      </year>
      <volume>
       20
      </volume>
      <fpage>
       53
      </fpage>
      <lpage>
       65
      </lpage>
      <pub-id pub-id-type="doi">
       10.1016/0377-0427(87)90125-7
      </pub-id>
      <pub-id assigning-authority="Zentralblatt MATH" pub-id-type="other">
       0636.62059
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR59">
     <label>
      59.
     </label>
     <mixed-citation publication-type="other">
      von Humboldt, W. On language: On the diversity of human language construction and its influence on the mental development of the human species. (1836/1999).
     </mixed-citation>
    </ref>
    <ref id="CR60">
     <label>
      60.
     </label>
     <mixed-citation publication-type="other">
      Yu, H., Zhang, H. &amp; Xu, W. Interactive grounded language acquisition and generalization in a 2d world. In
      <italic>
       ICLR
      </italic>
      (2018).
     </mixed-citation>
    </ref>
    <ref id="CR61">
     <label>
      61.
     </label>
     <mixed-citation publication-type="other">
      Chevalier-Boisvert, M.
      <italic>
       et al
      </italic>
      . BabyAI: A platform to study the sample efficiency of grounded language learning. In
      <italic>
       ICLR
      </italic>
      (2019).
     </mixed-citation>
    </ref>
    <ref id="CR62">
     <label>
      62.
     </label>
     <mixed-citation publication-type="other">
      Bender, E. M. &amp; Koller, A. Climbing towards NLU: On meaning, form, and understanding in the age of data. In
      <italic>
       Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
      </italic>
      5185–5198  (Association for Computational Linguistics, Online, 2020).
      <ext-link ext-link-type="doi" xlink:href="10.18653/v1/2020.acl-main.463">
       https://doi.org/10.18653/v1/2020.acl-main.463
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR63">
     <label>
      63.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Merrill
        </surname>
        <given-names>
         WC
        </given-names>
       </name>
       <name>
        <surname>
         Goldberg
        </surname>
        <given-names>
         Y
        </given-names>
       </name>
       <name>
        <surname>
         Schwartz
        </surname>
        <given-names>
         R
        </given-names>
       </name>
       <name>
        <surname>
         Smith
        </surname>
        <given-names>
         NA
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Provable limitations of acquiring meaning from ungrounded form: What will future language models understand?
      </article-title>
      <source>
       Trans. Assoc. Comput. Linguist.
      </source>
      <year>
       2021
      </year>
      <volume>
       9
      </volume>
      <fpage>
       1047
      </fpage>
      <lpage>
       1060
      </lpage>
      <pub-id pub-id-type="doi">
       10.1162/tacl_a_00412
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR64">
     <label>
      64.
     </label>
     <mixed-citation publication-type="other">
      Chollet, F. On the measure of intelligence. ArXiv
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1911.01547">
       arXiv:1911.01547
      </ext-link>
      (2019).
     </mixed-citation>
    </ref>
    <ref id="CR65">
     <label>
      65.
     </label>
     <mixed-citation publication-type="other">
      Gandhi, K., Stojnić, G., Lake, B. M. &amp; Dillon, M. R. Baby intuitions benchmark (bib): Discerning the goals, preferences, and actions of others.
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2102.11938">
       arXiv:cs.LG/2102.11938
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR66">
     <label>
      66.
     </label>
     <mixed-citation publication-type="other">
      Fong, R. C. &amp; Vedaldi, A. Interpretable explanations of black boxes by meaningful perturbation. In
      <italic>
       Proceedings of the IEEE International Conference on Computer Vision (ICCV)
      </italic>
      (2017).
     </mixed-citation>
    </ref>
    <ref id="CR67">
     <label>
      67.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Covert
        </surname>
        <given-names>
         I
        </given-names>
       </name>
       <name>
        <surname>
         Lundberg
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <name>
        <surname>
         Lee
        </surname>
        <given-names>
         S-I
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Explaining by removing: A unified framework for model explanation
      </article-title>
      <source>
       J. Mach. Learn. Res.
      </source>
      <year>
       2021
      </year>
      <volume>
       22
      </volume>
      <fpage>
       1
      </fpage>
      <lpage>
       90
      </lpage>
      <pub-id assigning-authority="Zentralblatt MATH" pub-id-type="other">
       07626724
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR68">
     <label>
      68.
     </label>
     <mixed-citation publication-type="other">
      Bianchi, F.
      <italic>
       et al
      </italic>
      . Contrastive language-image pre-training for the italian language. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2108.08688">
       arXiv:2108.08688
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR69">
     <label>
      69.
     </label>
     <mixed-citation publication-type="book">
      <person-group person-group-type="author">
       <name>
        <surname>
         Marconi
        </surname>
        <given-names>
         D
        </given-names>
       </name>
      </person-group>
      <source>
       Lexical Competence
      </source>
      <year>
       1997
      </year>
      <publisher-loc>
       Cambridge, MA
      </publisher-loc>
      <publisher-name>
       MIT Press
      </publisher-name>
     </mixed-citation>
    </ref>
    <ref id="CR70">
     <label>
      70.
     </label>
     <mixed-citation publication-type="other">
      Bianchi, F., Greco, C. &amp; Tagliabue, J. Language in a (search) box: Grounding language learning in real-world human-machine interaction. In
      <italic>
       Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
      </italic>
      4409–4415  (Association for Computational Linguistics, Online, 2021).
      <ext-link ext-link-type="doi" xlink:href="10.18653/v1/2021.naacl-main.348">
       https://doi.org/10.18653/v1/2021.naacl-main.348
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR71">
     <label>
      71.
     </label>
     <mixed-citation publication-type="journal">
      <person-group person-group-type="author">
       <name>
        <surname>
         Cheng
        </surname>
        <given-names>
         W-H
        </given-names>
       </name>
       <name>
        <surname>
         Song
        </surname>
        <given-names>
         S
        </given-names>
       </name>
       <name>
        <surname>
         Chen
        </surname>
        <given-names>
         C-Y
        </given-names>
       </name>
       <name>
        <surname>
         Hidayati
        </surname>
        <given-names>
         SC
        </given-names>
       </name>
       <name>
        <surname>
         Liu
        </surname>
        <given-names>
         J
        </given-names>
       </name>
      </person-group>
      <article-title xml:lang="en">
       Fashion meets computer vision: A survey
      </article-title>
      <source>
       ACM Comput. Surv.
      </source>
      <year>
       2021
      </year>
      <pub-id pub-id-type="doi">
       10.1145/3447239
      </pub-id>
     </mixed-citation>
    </ref>
    <ref id="CR72">
     <label>
      72.
     </label>
     <mixed-citation publication-type="other">
      Zheng, S., Yang, F., Kiapour, M. H. &amp; Piramuthu, R. Modanet: A large-scale street fashion dataset with polygon annotations. In
      <italic>
       Proceedings of the 26th ACM international conference on Multimedia
      </italic>
      1670–1678 (2018).
     </mixed-citation>
    </ref>
    <ref id="CR73">
     <label>
      73.
     </label>
     <mixed-citation publication-type="book">
      <person-group person-group-type="author">
       <name>
        <surname>
         Chierchia
        </surname>
        <given-names>
         G
        </given-names>
       </name>
       <name>
        <surname>
         McConnell-Ginet
        </surname>
        <given-names>
         S
        </given-names>
       </name>
      </person-group>
      <source>
       Meaning and Grammar: An Introduction to Semantics
      </source>
      <year>
       2000
      </year>
      <edition>
       2
      </edition>
      <publisher-loc>
       Cambridge, MA, USA
      </publisher-loc>
      <publisher-name>
       MIT Press
      </publisher-name>
     </mixed-citation>
    </ref>
    <ref id="CR74">
     <label>
      74.
     </label>
     <mixed-citation publication-type="other">
      Pham, T. M., Bui, T., Mai, L. &amp; Nguyen, A. M. Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks?
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2012.15180">
       arXiv:cs.LG/2012.15180
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR75">
     <label>
      75.
     </label>
     <mixed-citation publication-type="other">
      Thrush, T.
      <italic>
       et al
      </italic>
      . Winoground: Probing vision and language models for visio-linguistic compositionality. In
      <italic>
       CVPR
      </italic>
      (2022).
     </mixed-citation>
    </ref>
    <ref id="CR76">
     <label>
      76.
     </label>
     <mixed-citation publication-type="other">
      Chia, P. J., Tagliabue, J., Bianchi, F., Greco, C. &amp; Goncalves, D. “does it come in black?” clip-like models are zero-shot recommenders. In
      <italic>
       Proceedings of The 5th Workshop on e-Commerce and NLP
      </italic>
      (Association for Computational Linguistics, 2022).
     </mixed-citation>
    </ref>
    <ref id="CR77">
     <label>
      77.
     </label>
     <mixed-citation publication-type="other">
      Ratner, A. J.
      <italic>
       et al
      </italic>
      . Snorkel: Rapid training data creation with weak supervision. In
      <italic>
       Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases
      </italic>
      Vol. 11 3, 269–282 (2017).
     </mixed-citation>
    </ref>
    <ref id="CR78">
     <label>
      78.
     </label>
     <mixed-citation publication-type="other">
      Yu, B., Tagliabue, J., Greco, C. &amp; Bianchi, F. “an image is worth a thousand features”: Scalable product representations for in-session type-ahead personalization. In
      <italic>
       Companion Proceedings of the Web Conference 2020, WWW ’20
      </italic>
      461–470  (Association for Computing Machinery, New York, NY, USA, 2020).
      <ext-link ext-link-type="doi" xlink:href="10.1145/3366424.3386198">
       https://doi.org/10.1145/3366424.3386198
      </ext-link>
      .
     </mixed-citation>
    </ref>
    <ref id="CR79">
     <label>
      79.
     </label>
     <mixed-citation publication-type="other">
      Vincent, J. OpenAI’s state-of-the-art machine vision AI is fooled by handwritten notes. (2021).
      <ext-link ext-link-type="uri" xlink:href="https://www.theverge.com/2021/3/8/22319173/openai-machine-visionadversarial-typographic-attacka-clip-multimodal-neuron">
       https://www.theverge.com/2021/3/8/22319173/openai-machine-visionadversarial-typographic-attacka-clip-multimodal-neuron
      </ext-link>
      . Accessed 04-Nov-2022.
     </mixed-citation>
    </ref>
    <ref id="CR80">
     <label>
      80.
     </label>
     <mixed-citation publication-type="other">
      Noever, D. A. &amp; Noever, S. E. M. Reading isn’t believing: Adversarial attacks on multi-modal neurons. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2103.10480">
       arXiv:2103.10480
      </ext-link>
      (2021).
     </mixed-citation>
    </ref>
    <ref id="CR81">
     <label>
      81.
     </label>
     <mixed-citation publication-type="other">
      Yu, Y., Lee, H. J., Kim, B. C., Kim, J. U. &amp; Ro, Y. M. Investigating vulnerability to adversarial examples on multimodal data fusion in deep learning.
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2005.10987">
       arXiv:2005.10987
      </ext-link>
      (2020).
     </mixed-citation>
    </ref>
    <ref id="CR82">
     <label>
      82.
     </label>
     <mixed-citation publication-type="other">
      Berg, D.
      <italic>
       et al
      </italic>
      . Open-Sourcing Metaflow, a Human-Centric Framework for Data Science (2019).
      <ext-link ext-link-type="uri" xlink:href="https://netflixtechblog.com/open-sourcing-metaflow-a-human-centricframework-for-data-science-fa72e04a5d9">
       https://netflixtechblog.com/open-sourcing-metaflow-a-human-centricframework-for-data-science-fa72e04a5d9
      </ext-link>
      . Accessed 04-Nov-2022.
     </mixed-citation>
    </ref>
    <ref id="CR83">
     <label>
      83.
     </label>
     <mixed-citation publication-type="other">
      Comet.ML. Comet.ML home page (2021).
     </mixed-citation>
    </ref>
    <ref id="CR84">
     <label>
      84.
     </label>
     <mixed-citation publication-type="other">
      Lacoste, A., Luccioni, A., Schmidt, V. &amp; Dandres, T. Quantifying the carbon emissions of machine learning. arXiv preprint
      <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1910.09700">
       arXiv:1910.09700
      </ext-link>
      (2019).
     </mixed-citation>
    </ref>
   </ref-list>
  </ref-list>
  <notes notes-type="Misc">
   <title>
    Publisher's note
   </title>
   <p>
    Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
   </p>
  </notes>
 </back>
</article>
